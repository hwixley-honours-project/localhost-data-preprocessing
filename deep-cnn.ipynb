{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07dbc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36307537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129bfdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = \"polar\"\n",
    "lag = \"0\"\n",
    "model_save_dir = os.getcwd() + f\"/ml-models/{features}-lag{lag}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05984650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNpy(filename):\n",
    "    with open(os.getcwd() + \"/train-val-test/\"+ filename, \"rb\") as f: return np.load(f)\n",
    "\n",
    "db = f\"db14/window-size-10/lag{lag}/{features}-features/\"\n",
    "data_type = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b58f23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = os.getcwd() + \"/train-val-test/\" + db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a28e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = loadNpy(db + f\"X_train.npy\"), loadNpy(db + f\"X_val.npy\"), loadNpy(db + f\"X_test.npy\")\n",
    "y_train, y_val, y_test = loadNpy(db + f\"y{data_type}_train.npy\"), loadNpy(db + f\"y{data_type}_val.npy\"), loadNpy(db + f\"y{data_type}_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2e258163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = loadNpy(db + f\"us-X_train.npy\"), loadNpy(db + f\"us-X_val.npy\"), loadNpy(db + f\"us-X_test.npy\")\n",
    "y_train, y_val, y_test = loadNpy(db + f\"y{data_type}_train.npy\"), loadNpy(db + f\"y{data_type}_val.npy\"), loadNpy(db + f\"y{data_type}_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a18c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_to_tensor(data, l=False):\n",
    "    if not l:\n",
    "        data = torch.from_numpy(data).float()\n",
    "    else:\n",
    "        data = torch.from_numpy(data).type(torch.LongTensor)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b0cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = npy_to_tensor(X_train), npy_to_tensor(X_val), npy_to_tensor(X_test)\n",
    "y_train, y_val, y_test = npy_to_tensor(y_train), npy_to_tensor(y_val), npy_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e9e6de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d8500f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Data loader\n",
    "train_iterator = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "val_iterator = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_iterator = DataLoader(dataset=test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e08964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, DIR + \"train-dataset\")\n",
    "torch.save(val_dataset, DIR + \"val-dataset\")\n",
    "torch.save(test_dataset, DIR + \"test-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b96b615b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17505, 784])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "27d00d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5835, 784])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96b32348",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = val_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "857a3976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d55761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rnn(mod, data):\n",
    "    preds = []\n",
    "    actual = []\n",
    "    \n",
    "    shape = data.dataset.tensors[0].shape[1:]\n",
    "    \n",
    "    mod.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in data:\n",
    "            images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = mod(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            preds.append(predicted)\n",
    "            actual.append(labels)\n",
    "\n",
    "        print(f'Accuracy of the model on the {total} samples: {100 * correct / total} %')\n",
    "    return preds, actual, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daa0e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import ROC\n",
    "\n",
    "def eval_cnn(data):\n",
    "    \n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pred_ys = []\n",
    "        labs = []\n",
    "        for images, labels in loaders[data]:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            test_output, last_layer = cnn(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            pred_ys = pred_ys + test_output.flatten().tolist()\n",
    "            labs = labs + labels.tolist()\n",
    "            total += len(labels)\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            pass\n",
    "        print(f'{data} Accuracy of the model on the {total} {data} images: %.3f' % (correct/total))\n",
    "        return pred_ys, labs, correct/total\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4193e0",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cca70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': train_iterator,\n",
    "    'val': val_iterator,\n",
    "    'test': test_iterator\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2dd062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        ).to(device)\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        ).to(device)\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(32, 64, 5, 1, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out = nn.Linear(64 * 7 * 7, 2).to(device)\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output, x    # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d01c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (out): Linear(in_features=3136, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48258a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()   \n",
    "loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e651794b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.001)   \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "008bc445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [100/137], Loss: 0.3298\n",
      "train Accuracy of the model on the 17505 train images: 0.877\n",
      "val Accuracy of the model on the 5835 val images: 0.869\n",
      "test Accuracy of the model on the 5835 test images: 0.874\n",
      "Epoch [2/1000], Step [100/137], Loss: 0.2916\n",
      "train Accuracy of the model on the 17505 train images: 0.886\n",
      "val Accuracy of the model on the 5835 val images: 0.876\n",
      "test Accuracy of the model on the 5835 test images: 0.880\n",
      "Epoch [3/1000], Step [100/137], Loss: 0.2737\n",
      "train Accuracy of the model on the 17505 train images: 0.889\n",
      "val Accuracy of the model on the 5835 val images: 0.877\n",
      "test Accuracy of the model on the 5835 test images: 0.883\n",
      "Epoch [4/1000], Step [100/137], Loss: 0.2682\n",
      "train Accuracy of the model on the 17505 train images: 0.876\n",
      "val Accuracy of the model on the 5835 val images: 0.864\n",
      "test Accuracy of the model on the 5835 test images: 0.874\n",
      "Epoch [5/1000], Step [100/137], Loss: 0.2582\n",
      "train Accuracy of the model on the 17505 train images: 0.890\n",
      "val Accuracy of the model on the 5835 val images: 0.879\n",
      "test Accuracy of the model on the 5835 test images: 0.883\n",
      "Epoch [6/1000], Step [100/137], Loss: 0.2452\n",
      "train Accuracy of the model on the 17505 train images: 0.894\n",
      "val Accuracy of the model on the 5835 val images: 0.885\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [7/1000], Step [100/137], Loss: 0.2344\n",
      "train Accuracy of the model on the 17505 train images: 0.894\n",
      "val Accuracy of the model on the 5835 val images: 0.880\n",
      "test Accuracy of the model on the 5835 test images: 0.888\n",
      "Epoch [8/1000], Step [100/137], Loss: 0.2239\n",
      "train Accuracy of the model on the 17505 train images: 0.789\n",
      "val Accuracy of the model on the 5835 val images: 0.775\n",
      "test Accuracy of the model on the 5835 test images: 0.790\n",
      "Epoch [9/1000], Step [100/137], Loss: 0.2201\n",
      "train Accuracy of the model on the 17505 train images: 0.600\n",
      "val Accuracy of the model on the 5835 val images: 0.592\n",
      "test Accuracy of the model on the 5835 test images: 0.595\n",
      "Epoch [10/1000], Step [100/137], Loss: 0.2105\n",
      "train Accuracy of the model on the 17505 train images: 0.756\n",
      "val Accuracy of the model on the 5835 val images: 0.742\n",
      "test Accuracy of the model on the 5835 test images: 0.748\n",
      "Epoch [11/1000], Step [100/137], Loss: 0.2049\n",
      "train Accuracy of the model on the 17505 train images: 0.174\n",
      "val Accuracy of the model on the 5835 val images: 0.175\n",
      "test Accuracy of the model on the 5835 test images: 0.170\n",
      "Epoch [12/1000], Step [100/137], Loss: 0.1970\n",
      "train Accuracy of the model on the 17505 train images: 0.161\n",
      "val Accuracy of the model on the 5835 val images: 0.164\n",
      "test Accuracy of the model on the 5835 test images: 0.163\n",
      "Epoch [13/1000], Step [100/137], Loss: 0.2035\n",
      "train Accuracy of the model on the 17505 train images: 0.173\n",
      "val Accuracy of the model on the 5835 val images: 0.175\n",
      "test Accuracy of the model on the 5835 test images: 0.169\n",
      "Epoch [14/1000], Step [100/137], Loss: 0.1949\n",
      "train Accuracy of the model on the 17505 train images: 0.904\n",
      "val Accuracy of the model on the 5835 val images: 0.890\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [15/1000], Step [100/137], Loss: 0.1944\n",
      "train Accuracy of the model on the 17505 train images: 0.215\n",
      "val Accuracy of the model on the 5835 val images: 0.211\n",
      "test Accuracy of the model on the 5835 test images: 0.210\n",
      "Epoch [16/1000], Step [100/137], Loss: 0.1928\n",
      "train Accuracy of the model on the 17505 train images: 0.900\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [17/1000], Step [100/137], Loss: 0.1940\n",
      "train Accuracy of the model on the 17505 train images: 0.893\n",
      "val Accuracy of the model on the 5835 val images: 0.879\n",
      "test Accuracy of the model on the 5835 test images: 0.882\n",
      "Epoch [18/1000], Step [100/137], Loss: 0.1902\n",
      "train Accuracy of the model on the 17505 train images: 0.909\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [19/1000], Step [100/137], Loss: 0.1837\n",
      "train Accuracy of the model on the 17505 train images: 0.203\n",
      "val Accuracy of the model on the 5835 val images: 0.203\n",
      "test Accuracy of the model on the 5835 test images: 0.202\n",
      "Epoch [20/1000], Step [100/137], Loss: 0.1727\n",
      "train Accuracy of the model on the 17505 train images: 0.906\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [21/1000], Step [100/137], Loss: 0.1855\n",
      "train Accuracy of the model on the 17505 train images: 0.819\n",
      "val Accuracy of the model on the 5835 val images: 0.800\n",
      "test Accuracy of the model on the 5835 test images: 0.797\n",
      "Epoch [22/1000], Step [100/137], Loss: 0.1840\n",
      "train Accuracy of the model on the 17505 train images: 0.611\n",
      "val Accuracy of the model on the 5835 val images: 0.601\n",
      "test Accuracy of the model on the 5835 test images: 0.592\n",
      "Epoch [23/1000], Step [100/137], Loss: 0.1731\n",
      "train Accuracy of the model on the 17505 train images: 0.905\n",
      "val Accuracy of the model on the 5835 val images: 0.878\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [24/1000], Step [100/137], Loss: 0.1929\n",
      "train Accuracy of the model on the 17505 train images: 0.651\n",
      "val Accuracy of the model on the 5835 val images: 0.634\n",
      "test Accuracy of the model on the 5835 test images: 0.624\n",
      "Epoch [25/1000], Step [100/137], Loss: 0.1855\n",
      "train Accuracy of the model on the 17505 train images: 0.907\n",
      "val Accuracy of the model on the 5835 val images: 0.874\n",
      "test Accuracy of the model on the 5835 test images: 0.888\n",
      "Epoch [26/1000], Step [100/137], Loss: 0.1756\n",
      "train Accuracy of the model on the 17505 train images: 0.910\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [27/1000], Step [100/137], Loss: 0.1704\n",
      "train Accuracy of the model on the 17505 train images: 0.919\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [28/1000], Step [100/137], Loss: 0.1714\n",
      "train Accuracy of the model on the 17505 train images: 0.363\n",
      "val Accuracy of the model on the 5835 val images: 0.341\n",
      "test Accuracy of the model on the 5835 test images: 0.346\n",
      "Epoch [29/1000], Step [100/137], Loss: 0.1643\n",
      "train Accuracy of the model on the 17505 train images: 0.221\n",
      "val Accuracy of the model on the 5835 val images: 0.213\n",
      "test Accuracy of the model on the 5835 test images: 0.207\n",
      "Epoch [30/1000], Step [100/137], Loss: 0.1683\n",
      "train Accuracy of the model on the 17505 train images: 0.278\n",
      "val Accuracy of the model on the 5835 val images: 0.270\n",
      "test Accuracy of the model on the 5835 test images: 0.266\n",
      "Epoch [31/1000], Step [100/137], Loss: 0.1714\n",
      "train Accuracy of the model on the 17505 train images: 0.795\n",
      "val Accuracy of the model on the 5835 val images: 0.778\n",
      "test Accuracy of the model on the 5835 test images: 0.777\n",
      "Epoch [32/1000], Step [100/137], Loss: 0.1605\n",
      "train Accuracy of the model on the 17505 train images: 0.920\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [33/1000], Step [100/137], Loss: 0.1551\n",
      "train Accuracy of the model on the 17505 train images: 0.918\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [34/1000], Step [100/137], Loss: 0.1748\n",
      "train Accuracy of the model on the 17505 train images: 0.912\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.885\n",
      "Epoch [35/1000], Step [100/137], Loss: 0.1854\n",
      "train Accuracy of the model on the 17505 train images: 0.913\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [36/1000], Step [100/137], Loss: 0.1552\n",
      "train Accuracy of the model on the 17505 train images: 0.917\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [37/1000], Step [100/137], Loss: 0.1455\n",
      "train Accuracy of the model on the 17505 train images: 0.907\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [38/1000], Step [100/137], Loss: 0.1584\n",
      "train Accuracy of the model on the 17505 train images: 0.901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.881\n",
      "test Accuracy of the model on the 5835 test images: 0.886\n",
      "Epoch [39/1000], Step [100/137], Loss: 0.1457\n",
      "train Accuracy of the model on the 17505 train images: 0.912\n",
      "val Accuracy of the model on the 5835 val images: 0.889\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [40/1000], Step [100/137], Loss: 0.1383\n",
      "train Accuracy of the model on the 17505 train images: 0.919\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [41/1000], Step [100/137], Loss: 0.1340\n",
      "train Accuracy of the model on the 17505 train images: 0.921\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [42/1000], Step [100/137], Loss: 0.1384\n",
      "train Accuracy of the model on the 17505 train images: 0.928\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [43/1000], Step [100/137], Loss: 0.1375\n",
      "train Accuracy of the model on the 17505 train images: 0.911\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [44/1000], Step [100/137], Loss: 0.1215\n",
      "train Accuracy of the model on the 17505 train images: 0.903\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.888\n",
      "Epoch [45/1000], Step [100/137], Loss: 0.1143\n",
      "train Accuracy of the model on the 17505 train images: 0.904\n",
      "val Accuracy of the model on the 5835 val images: 0.884\n",
      "test Accuracy of the model on the 5835 test images: 0.888\n",
      "Epoch [46/1000], Step [100/137], Loss: 0.1184\n",
      "train Accuracy of the model on the 17505 train images: 0.903\n",
      "val Accuracy of the model on the 5835 val images: 0.883\n",
      "test Accuracy of the model on the 5835 test images: 0.891\n",
      "Epoch [47/1000], Step [100/137], Loss: 0.0977\n",
      "train Accuracy of the model on the 17505 train images: 0.902\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [48/1000], Step [100/137], Loss: 0.0950\n",
      "train Accuracy of the model on the 17505 train images: 0.899\n",
      "val Accuracy of the model on the 5835 val images: 0.887\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [49/1000], Step [100/137], Loss: 0.1025\n",
      "train Accuracy of the model on the 17505 train images: 0.910\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [50/1000], Step [100/137], Loss: 0.1157\n",
      "train Accuracy of the model on the 17505 train images: 0.904\n",
      "val Accuracy of the model on the 5835 val images: 0.884\n",
      "test Accuracy of the model on the 5835 test images: 0.890\n",
      "Epoch [51/1000], Step [100/137], Loss: 0.1089\n",
      "train Accuracy of the model on the 17505 train images: 0.910\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [52/1000], Step [100/137], Loss: 0.1036\n",
      "train Accuracy of the model on the 17505 train images: 0.923\n",
      "val Accuracy of the model on the 5835 val images: 0.887\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [53/1000], Step [100/137], Loss: 0.0868\n",
      "train Accuracy of the model on the 17505 train images: 0.920\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [54/1000], Step [100/137], Loss: 0.1027\n",
      "train Accuracy of the model on the 17505 train images: 0.923\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [55/1000], Step [100/137], Loss: 0.0884\n",
      "train Accuracy of the model on the 17505 train images: 0.896\n",
      "val Accuracy of the model on the 5835 val images: 0.873\n",
      "test Accuracy of the model on the 5835 test images: 0.880\n",
      "Epoch [56/1000], Step [100/137], Loss: 0.0934\n",
      "train Accuracy of the model on the 17505 train images: 0.916\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [57/1000], Step [100/137], Loss: 0.0870\n",
      "train Accuracy of the model on the 17505 train images: 0.913\n",
      "val Accuracy of the model on the 5835 val images: 0.887\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [58/1000], Step [100/137], Loss: 0.1003\n",
      "train Accuracy of the model on the 17505 train images: 0.914\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [59/1000], Step [100/137], Loss: 0.0931\n",
      "train Accuracy of the model on the 17505 train images: 0.922\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [60/1000], Step [100/137], Loss: 0.1148\n",
      "train Accuracy of the model on the 17505 train images: 0.928\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [61/1000], Step [100/137], Loss: 0.1009\n",
      "train Accuracy of the model on the 17505 train images: 0.928\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [62/1000], Step [100/137], Loss: 0.0994\n",
      "train Accuracy of the model on the 17505 train images: 0.921\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [63/1000], Step [100/137], Loss: 0.1173\n",
      "train Accuracy of the model on the 17505 train images: 0.925\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [64/1000], Step [100/137], Loss: 0.0839\n",
      "train Accuracy of the model on the 17505 train images: 0.928\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [65/1000], Step [100/137], Loss: 0.0777\n",
      "train Accuracy of the model on the 17505 train images: 0.921\n",
      "val Accuracy of the model on the 5835 val images: 0.884\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [66/1000], Step [100/137], Loss: 0.0768\n",
      "train Accuracy of the model on the 17505 train images: 0.930\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [67/1000], Step [100/137], Loss: 0.0810\n",
      "train Accuracy of the model on the 17505 train images: 0.933\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [68/1000], Step [100/137], Loss: 0.0717\n",
      "train Accuracy of the model on the 17505 train images: 0.897\n",
      "val Accuracy of the model on the 5835 val images: 0.843\n",
      "test Accuracy of the model on the 5835 test images: 0.852\n",
      "Epoch [69/1000], Step [100/137], Loss: 0.0717\n",
      "train Accuracy of the model on the 17505 train images: 0.769\n",
      "val Accuracy of the model on the 5835 val images: 0.705\n",
      "test Accuracy of the model on the 5835 test images: 0.720\n",
      "Epoch [70/1000], Step [100/137], Loss: 0.0612\n",
      "train Accuracy of the model on the 17505 train images: 0.818\n",
      "val Accuracy of the model on the 5835 val images: 0.766\n",
      "test Accuracy of the model on the 5835 test images: 0.771\n",
      "Epoch [71/1000], Step [100/137], Loss: 0.0629\n",
      "train Accuracy of the model on the 17505 train images: 0.806\n",
      "val Accuracy of the model on the 5835 val images: 0.748\n",
      "test Accuracy of the model on the 5835 test images: 0.754\n",
      "Epoch [72/1000], Step [100/137], Loss: 0.0760\n",
      "train Accuracy of the model on the 17505 train images: 0.248\n",
      "val Accuracy of the model on the 5835 val images: 0.236\n",
      "test Accuracy of the model on the 5835 test images: 0.238\n",
      "Epoch [73/1000], Step [100/137], Loss: 0.0687\n",
      "train Accuracy of the model on the 17505 train images: 0.936\n",
      "val Accuracy of the model on the 5835 val images: 0.879\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [74/1000], Step [100/137], Loss: 0.0590\n",
      "train Accuracy of the model on the 17505 train images: 0.845\n",
      "val Accuracy of the model on the 5835 val images: 0.783\n",
      "test Accuracy of the model on the 5835 test images: 0.781\n",
      "Epoch [75/1000], Step [100/137], Loss: 0.0582\n",
      "train Accuracy of the model on the 17505 train images: 0.410\n",
      "val Accuracy of the model on the 5835 val images: 0.383\n",
      "test Accuracy of the model on the 5835 test images: 0.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/1000], Step [100/137], Loss: 0.0622\n",
      "train Accuracy of the model on the 17505 train images: 0.755\n",
      "val Accuracy of the model on the 5835 val images: 0.693\n",
      "test Accuracy of the model on the 5835 test images: 0.699\n",
      "Epoch [77/1000], Step [100/137], Loss: 0.0628\n",
      "train Accuracy of the model on the 17505 train images: 0.847\n",
      "val Accuracy of the model on the 5835 val images: 0.781\n",
      "test Accuracy of the model on the 5835 test images: 0.791\n",
      "Epoch [78/1000], Step [100/137], Loss: 0.0711\n",
      "train Accuracy of the model on the 17505 train images: 0.570\n",
      "val Accuracy of the model on the 5835 val images: 0.522\n",
      "test Accuracy of the model on the 5835 test images: 0.526\n",
      "Epoch [79/1000], Step [100/137], Loss: 0.0507\n",
      "train Accuracy of the model on the 17505 train images: 0.311\n",
      "val Accuracy of the model on the 5835 val images: 0.282\n",
      "test Accuracy of the model on the 5835 test images: 0.283\n",
      "Epoch [80/1000], Step [100/137], Loss: 0.0638\n",
      "train Accuracy of the model on the 17505 train images: 0.890\n",
      "val Accuracy of the model on the 5835 val images: 0.837\n",
      "test Accuracy of the model on the 5835 test images: 0.842\n",
      "Epoch [81/1000], Step [100/137], Loss: 0.0650\n",
      "train Accuracy of the model on the 17505 train images: 0.895\n",
      "val Accuracy of the model on the 5835 val images: 0.844\n",
      "test Accuracy of the model on the 5835 test images: 0.845\n",
      "Epoch [82/1000], Step [100/137], Loss: 0.0652\n",
      "train Accuracy of the model on the 17505 train images: 0.381\n",
      "val Accuracy of the model on the 5835 val images: 0.353\n",
      "test Accuracy of the model on the 5835 test images: 0.357\n",
      "Epoch [83/1000], Step [100/137], Loss: 0.0833\n",
      "train Accuracy of the model on the 17505 train images: 0.752\n",
      "val Accuracy of the model on the 5835 val images: 0.696\n",
      "test Accuracy of the model on the 5835 test images: 0.702\n",
      "Epoch [84/1000], Step [100/137], Loss: 0.0626\n",
      "train Accuracy of the model on the 17505 train images: 0.904\n",
      "val Accuracy of the model on the 5835 val images: 0.862\n",
      "test Accuracy of the model on the 5835 test images: 0.870\n",
      "Epoch [85/1000], Step [100/137], Loss: 0.0532\n",
      "train Accuracy of the model on the 17505 train images: 0.927\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [86/1000], Step [100/137], Loss: 0.0306\n",
      "train Accuracy of the model on the 17505 train images: 0.888\n",
      "val Accuracy of the model on the 5835 val images: 0.839\n",
      "test Accuracy of the model on the 5835 test images: 0.848\n",
      "Epoch [87/1000], Step [100/137], Loss: 0.0261\n",
      "train Accuracy of the model on the 17505 train images: 0.918\n",
      "val Accuracy of the model on the 5835 val images: 0.885\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [88/1000], Step [100/137], Loss: 0.0421\n",
      "train Accuracy of the model on the 17505 train images: 0.910\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [89/1000], Step [100/137], Loss: 0.0260\n",
      "train Accuracy of the model on the 17505 train images: 0.919\n",
      "val Accuracy of the model on the 5835 val images: 0.890\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [90/1000], Step [100/137], Loss: 0.0365\n",
      "train Accuracy of the model on the 17505 train images: 0.924\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [91/1000], Step [100/137], Loss: 0.0335\n",
      "train Accuracy of the model on the 17505 train images: 0.916\n",
      "val Accuracy of the model on the 5835 val images: 0.880\n",
      "test Accuracy of the model on the 5835 test images: 0.886\n",
      "Epoch [92/1000], Step [100/137], Loss: 0.0229\n",
      "train Accuracy of the model on the 17505 train images: 0.853\n",
      "val Accuracy of the model on the 5835 val images: 0.814\n",
      "test Accuracy of the model on the 5835 test images: 0.823\n",
      "Epoch [93/1000], Step [100/137], Loss: 0.0319\n",
      "train Accuracy of the model on the 17505 train images: 0.910\n",
      "val Accuracy of the model on the 5835 val images: 0.872\n",
      "test Accuracy of the model on the 5835 test images: 0.876\n",
      "Epoch [94/1000], Step [100/137], Loss: 0.0444\n",
      "train Accuracy of the model on the 17505 train images: 0.741\n",
      "val Accuracy of the model on the 5835 val images: 0.705\n",
      "test Accuracy of the model on the 5835 test images: 0.707\n",
      "Epoch [95/1000], Step [100/137], Loss: 0.0513\n",
      "train Accuracy of the model on the 17505 train images: 0.682\n",
      "val Accuracy of the model on the 5835 val images: 0.659\n",
      "test Accuracy of the model on the 5835 test images: 0.666\n",
      "Epoch [96/1000], Step [100/137], Loss: 0.0514\n",
      "train Accuracy of the model on the 17505 train images: 0.685\n",
      "val Accuracy of the model on the 5835 val images: 0.652\n",
      "test Accuracy of the model on the 5835 test images: 0.653\n",
      "Epoch [97/1000], Step [100/137], Loss: 0.0482\n",
      "train Accuracy of the model on the 17505 train images: 0.902\n",
      "val Accuracy of the model on the 5835 val images: 0.844\n",
      "test Accuracy of the model on the 5835 test images: 0.849\n",
      "Epoch [98/1000], Step [100/137], Loss: 0.0336\n",
      "train Accuracy of the model on the 17505 train images: 0.912\n",
      "val Accuracy of the model on the 5835 val images: 0.860\n",
      "test Accuracy of the model on the 5835 test images: 0.862\n",
      "Epoch [99/1000], Step [100/137], Loss: 0.0422\n",
      "train Accuracy of the model on the 17505 train images: 0.918\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [100/1000], Step [100/137], Loss: 0.0431\n",
      "train Accuracy of the model on the 17505 train images: 0.903\n",
      "val Accuracy of the model on the 5835 val images: 0.867\n",
      "test Accuracy of the model on the 5835 test images: 0.871\n",
      "Epoch [101/1000], Step [100/137], Loss: 0.0314\n",
      "train Accuracy of the model on the 17505 train images: 0.905\n",
      "val Accuracy of the model on the 5835 val images: 0.868\n",
      "test Accuracy of the model on the 5835 test images: 0.873\n",
      "Epoch [102/1000], Step [100/137], Loss: 0.0762\n",
      "train Accuracy of the model on the 17505 train images: 0.911\n",
      "val Accuracy of the model on the 5835 val images: 0.877\n",
      "test Accuracy of the model on the 5835 test images: 0.880\n",
      "Epoch [103/1000], Step [100/137], Loss: 0.0316\n",
      "train Accuracy of the model on the 17505 train images: 0.915\n",
      "val Accuracy of the model on the 5835 val images: 0.876\n",
      "test Accuracy of the model on the 5835 test images: 0.880\n",
      "Epoch [104/1000], Step [100/137], Loss: 0.0195\n",
      "train Accuracy of the model on the 17505 train images: 0.932\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [105/1000], Step [100/137], Loss: 0.0126\n",
      "train Accuracy of the model on the 17505 train images: 0.922\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [106/1000], Step [100/137], Loss: 0.0150\n",
      "train Accuracy of the model on the 17505 train images: 0.926\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [107/1000], Step [100/137], Loss: 0.0183\n",
      "train Accuracy of the model on the 17505 train images: 0.920\n",
      "val Accuracy of the model on the 5835 val images: 0.881\n",
      "test Accuracy of the model on the 5835 test images: 0.885\n",
      "Epoch [108/1000], Step [100/137], Loss: 0.0169\n",
      "train Accuracy of the model on the 17505 train images: 0.941\n",
      "val Accuracy of the model on the 5835 val images: 0.879\n",
      "test Accuracy of the model on the 5835 test images: 0.882\n",
      "Epoch [109/1000], Step [100/137], Loss: 0.0400\n",
      "train Accuracy of the model on the 17505 train images: 0.783\n",
      "val Accuracy of the model on the 5835 val images: 0.715\n",
      "test Accuracy of the model on the 5835 test images: 0.711\n",
      "Epoch [110/1000], Step [100/137], Loss: 0.0232\n",
      "train Accuracy of the model on the 17505 train images: 0.805\n",
      "val Accuracy of the model on the 5835 val images: 0.764\n",
      "test Accuracy of the model on the 5835 test images: 0.763\n",
      "Epoch [111/1000], Step [100/137], Loss: 0.0273\n",
      "train Accuracy of the model on the 17505 train images: 0.780\n",
      "val Accuracy of the model on the 5835 val images: 0.734\n",
      "test Accuracy of the model on the 5835 test images: 0.741\n",
      "Epoch [112/1000], Step [100/137], Loss: 0.0336\n",
      "train Accuracy of the model on the 17505 train images: 0.835\n",
      "val Accuracy of the model on the 5835 val images: 0.784\n",
      "test Accuracy of the model on the 5835 test images: 0.781\n",
      "Epoch [113/1000], Step [100/137], Loss: 0.0179\n",
      "train Accuracy of the model on the 17505 train images: 0.713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.662\n",
      "test Accuracy of the model on the 5835 test images: 0.678\n",
      "Epoch [114/1000], Step [100/137], Loss: 0.0138\n",
      "train Accuracy of the model on the 17505 train images: 0.640\n",
      "val Accuracy of the model on the 5835 val images: 0.607\n",
      "test Accuracy of the model on the 5835 test images: 0.613\n",
      "Epoch [115/1000], Step [100/137], Loss: 0.0137\n",
      "train Accuracy of the model on the 17505 train images: 0.898\n",
      "val Accuracy of the model on the 5835 val images: 0.863\n",
      "test Accuracy of the model on the 5835 test images: 0.863\n",
      "Epoch [116/1000], Step [100/137], Loss: 0.0163\n",
      "train Accuracy of the model on the 17505 train images: 0.752\n",
      "val Accuracy of the model on the 5835 val images: 0.710\n",
      "test Accuracy of the model on the 5835 test images: 0.725\n",
      "Epoch [117/1000], Step [100/137], Loss: 0.0212\n",
      "train Accuracy of the model on the 17505 train images: 0.852\n",
      "val Accuracy of the model on the 5835 val images: 0.826\n",
      "test Accuracy of the model on the 5835 test images: 0.826\n",
      "Epoch [118/1000], Step [100/137], Loss: 0.0326\n",
      "train Accuracy of the model on the 17505 train images: 0.864\n",
      "val Accuracy of the model on the 5835 val images: 0.831\n",
      "test Accuracy of the model on the 5835 test images: 0.839\n",
      "Epoch [119/1000], Step [100/137], Loss: 0.0349\n",
      "train Accuracy of the model on the 17505 train images: 0.870\n",
      "val Accuracy of the model on the 5835 val images: 0.816\n",
      "test Accuracy of the model on the 5835 test images: 0.821\n",
      "Epoch [120/1000], Step [100/137], Loss: 0.0406\n",
      "train Accuracy of the model on the 17505 train images: 0.734\n",
      "val Accuracy of the model on the 5835 val images: 0.687\n",
      "test Accuracy of the model on the 5835 test images: 0.696\n",
      "Epoch [121/1000], Step [100/137], Loss: 0.0299\n",
      "train Accuracy of the model on the 17505 train images: 0.895\n",
      "val Accuracy of the model on the 5835 val images: 0.852\n",
      "test Accuracy of the model on the 5835 test images: 0.854\n",
      "Epoch [122/1000], Step [100/137], Loss: 0.0288\n",
      "train Accuracy of the model on the 17505 train images: 0.943\n",
      "val Accuracy of the model on the 5835 val images: 0.884\n",
      "test Accuracy of the model on the 5835 test images: 0.891\n",
      "Epoch [123/1000], Step [100/137], Loss: 0.0178\n",
      "train Accuracy of the model on the 17505 train images: 0.905\n",
      "val Accuracy of the model on the 5835 val images: 0.847\n",
      "test Accuracy of the model on the 5835 test images: 0.856\n",
      "Epoch [124/1000], Step [100/137], Loss: 0.0283\n",
      "train Accuracy of the model on the 17505 train images: 0.938\n",
      "val Accuracy of the model on the 5835 val images: 0.889\n",
      "test Accuracy of the model on the 5835 test images: 0.891\n",
      "Epoch [125/1000], Step [100/137], Loss: 0.0178\n",
      "train Accuracy of the model on the 17505 train images: 0.899\n",
      "val Accuracy of the model on the 5835 val images: 0.831\n",
      "test Accuracy of the model on the 5835 test images: 0.840\n",
      "Epoch [126/1000], Step [100/137], Loss: 0.0359\n",
      "train Accuracy of the model on the 17505 train images: 0.851\n",
      "val Accuracy of the model on the 5835 val images: 0.780\n",
      "test Accuracy of the model on the 5835 test images: 0.775\n",
      "Epoch [127/1000], Step [100/137], Loss: 0.0423\n",
      "train Accuracy of the model on the 17505 train images: 0.889\n",
      "val Accuracy of the model on the 5835 val images: 0.846\n",
      "test Accuracy of the model on the 5835 test images: 0.847\n",
      "Epoch [128/1000], Step [100/137], Loss: 0.0092\n",
      "train Accuracy of the model on the 17505 train images: 0.939\n",
      "val Accuracy of the model on the 5835 val images: 0.881\n",
      "test Accuracy of the model on the 5835 test images: 0.882\n",
      "Epoch [129/1000], Step [100/137], Loss: 0.0194\n",
      "train Accuracy of the model on the 17505 train images: 0.721\n",
      "val Accuracy of the model on the 5835 val images: 0.655\n",
      "test Accuracy of the model on the 5835 test images: 0.652\n",
      "Epoch [130/1000], Step [100/137], Loss: 0.0208\n",
      "train Accuracy of the model on the 17505 train images: 0.918\n",
      "val Accuracy of the model on the 5835 val images: 0.842\n",
      "test Accuracy of the model on the 5835 test images: 0.842\n",
      "Epoch [131/1000], Step [100/137], Loss: 0.0163\n",
      "train Accuracy of the model on the 17505 train images: 0.953\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [132/1000], Step [100/137], Loss: 0.0073\n",
      "train Accuracy of the model on the 17505 train images: 0.950\n",
      "val Accuracy of the model on the 5835 val images: 0.889\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [133/1000], Step [100/137], Loss: 0.0061\n",
      "train Accuracy of the model on the 17505 train images: 0.947\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [134/1000], Step [100/137], Loss: 0.0070\n",
      "train Accuracy of the model on the 17505 train images: 0.936\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [135/1000], Step [100/137], Loss: 0.0094\n",
      "train Accuracy of the model on the 17505 train images: 0.922\n",
      "val Accuracy of the model on the 5835 val images: 0.855\n",
      "test Accuracy of the model on the 5835 test images: 0.857\n",
      "Epoch [136/1000], Step [100/137], Loss: 0.0186\n",
      "train Accuracy of the model on the 17505 train images: 0.934\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [137/1000], Step [100/137], Loss: 0.0169\n",
      "train Accuracy of the model on the 17505 train images: 0.922\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [138/1000], Step [100/137], Loss: 0.0296\n",
      "train Accuracy of the model on the 17505 train images: 0.928\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [139/1000], Step [100/137], Loss: 0.0053\n",
      "train Accuracy of the model on the 17505 train images: 0.955\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [140/1000], Step [100/137], Loss: 0.0080\n",
      "train Accuracy of the model on the 17505 train images: 0.953\n",
      "val Accuracy of the model on the 5835 val images: 0.883\n",
      "test Accuracy of the model on the 5835 test images: 0.879\n",
      "Epoch [141/1000], Step [100/137], Loss: 0.0057\n",
      "train Accuracy of the model on the 17505 train images: 0.960\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [142/1000], Step [100/137], Loss: 0.0035\n",
      "train Accuracy of the model on the 17505 train images: 0.956\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [143/1000], Step [100/137], Loss: 0.0080\n",
      "train Accuracy of the model on the 17505 train images: 0.953\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [144/1000], Step [100/137], Loss: 0.0135\n",
      "train Accuracy of the model on the 17505 train images: 0.956\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [145/1000], Step [100/137], Loss: 0.0145\n",
      "train Accuracy of the model on the 17505 train images: 0.937\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [146/1000], Step [100/137], Loss: 0.0392\n",
      "train Accuracy of the model on the 17505 train images: 0.934\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [147/1000], Step [100/137], Loss: 0.0607\n",
      "train Accuracy of the model on the 17505 train images: 0.939\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [148/1000], Step [100/137], Loss: 0.0084\n",
      "train Accuracy of the model on the 17505 train images: 0.948\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [149/1000], Step [100/137], Loss: 0.0088\n",
      "train Accuracy of the model on the 17505 train images: 0.946\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.884\n",
      "Epoch [150/1000], Step [100/137], Loss: 0.0061\n",
      "train Accuracy of the model on the 17505 train images: 0.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [151/1000], Step [100/137], Loss: 0.0108\n",
      "train Accuracy of the model on the 17505 train images: 0.929\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [152/1000], Step [100/137], Loss: 0.0040\n",
      "train Accuracy of the model on the 17505 train images: 0.936\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [153/1000], Step [100/137], Loss: 0.0059\n",
      "train Accuracy of the model on the 17505 train images: 0.929\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [154/1000], Step [100/137], Loss: 0.0059\n",
      "train Accuracy of the model on the 17505 train images: 0.924\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.890\n",
      "Epoch [155/1000], Step [100/137], Loss: 0.0025\n",
      "train Accuracy of the model on the 17505 train images: 0.933\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [156/1000], Step [100/137], Loss: 0.0035\n",
      "train Accuracy of the model on the 17505 train images: 0.935\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [157/1000], Step [100/137], Loss: 0.0043\n",
      "train Accuracy of the model on the 17505 train images: 0.940\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [158/1000], Step [100/137], Loss: 0.0200\n",
      "train Accuracy of the model on the 17505 train images: 0.933\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [159/1000], Step [100/137], Loss: 0.0508\n",
      "train Accuracy of the model on the 17505 train images: 0.933\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [160/1000], Step [100/137], Loss: 0.0035\n",
      "train Accuracy of the model on the 17505 train images: 0.939\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [161/1000], Step [100/137], Loss: 0.0065\n",
      "train Accuracy of the model on the 17505 train images: 0.935\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [162/1000], Step [100/137], Loss: 0.0070\n",
      "train Accuracy of the model on the 17505 train images: 0.928\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [163/1000], Step [100/137], Loss: 0.0049\n",
      "train Accuracy of the model on the 17505 train images: 0.943\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [164/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.924\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [165/1000], Step [100/137], Loss: 0.0035\n",
      "train Accuracy of the model on the 17505 train images: 0.948\n",
      "val Accuracy of the model on the 5835 val images: 0.905\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [166/1000], Step [100/137], Loss: 0.3286\n",
      "train Accuracy of the model on the 17505 train images: 0.934\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [167/1000], Step [100/137], Loss: 0.0056\n",
      "train Accuracy of the model on the 17505 train images: 0.913\n",
      "val Accuracy of the model on the 5835 val images: 0.885\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [168/1000], Step [100/137], Loss: 0.0163\n",
      "train Accuracy of the model on the 17505 train images: 0.937\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [169/1000], Step [100/137], Loss: 0.0086\n",
      "train Accuracy of the model on the 17505 train images: 0.950\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [170/1000], Step [100/137], Loss: 0.0062\n",
      "train Accuracy of the model on the 17505 train images: 0.942\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [171/1000], Step [100/137], Loss: 0.0158\n",
      "train Accuracy of the model on the 17505 train images: 0.947\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [172/1000], Step [100/137], Loss: 0.0066\n",
      "train Accuracy of the model on the 17505 train images: 0.944\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [173/1000], Step [100/137], Loss: 0.0044\n",
      "train Accuracy of the model on the 17505 train images: 0.950\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [174/1000], Step [100/137], Loss: 0.0067\n",
      "train Accuracy of the model on the 17505 train images: 0.941\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.886\n",
      "Epoch [175/1000], Step [100/137], Loss: 0.0059\n",
      "train Accuracy of the model on the 17505 train images: 0.942\n",
      "val Accuracy of the model on the 5835 val images: 0.877\n",
      "test Accuracy of the model on the 5835 test images: 0.878\n",
      "Epoch [176/1000], Step [100/137], Loss: 0.0324\n",
      "train Accuracy of the model on the 17505 train images: 0.937\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [177/1000], Step [100/137], Loss: 0.0249\n",
      "train Accuracy of the model on the 17505 train images: 0.953\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [178/1000], Step [100/137], Loss: 0.0094\n",
      "train Accuracy of the model on the 17505 train images: 0.948\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.883\n",
      "Epoch [179/1000], Step [100/137], Loss: 0.0056\n",
      "train Accuracy of the model on the 17505 train images: 0.961\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [180/1000], Step [100/137], Loss: 0.0100\n",
      "train Accuracy of the model on the 17505 train images: 0.944\n",
      "val Accuracy of the model on the 5835 val images: 0.880\n",
      "test Accuracy of the model on the 5835 test images: 0.879\n",
      "Epoch [181/1000], Step [100/137], Loss: 0.0051\n",
      "train Accuracy of the model on the 17505 train images: 0.957\n",
      "val Accuracy of the model on the 5835 val images: 0.880\n",
      "test Accuracy of the model on the 5835 test images: 0.879\n",
      "Epoch [182/1000], Step [100/137], Loss: 0.0043\n",
      "train Accuracy of the model on the 17505 train images: 0.856\n",
      "val Accuracy of the model on the 5835 val images: 0.777\n",
      "test Accuracy of the model on the 5835 test images: 0.768\n",
      "Epoch [183/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.939\n",
      "val Accuracy of the model on the 5835 val images: 0.859\n",
      "test Accuracy of the model on the 5835 test images: 0.854\n",
      "Epoch [184/1000], Step [100/137], Loss: 0.0103\n",
      "train Accuracy of the model on the 17505 train images: 0.785\n",
      "val Accuracy of the model on the 5835 val images: 0.717\n",
      "test Accuracy of the model on the 5835 test images: 0.709\n",
      "Epoch [185/1000], Step [100/137], Loss: 0.0032\n",
      "train Accuracy of the model on the 17505 train images: 0.895\n",
      "val Accuracy of the model on the 5835 val images: 0.822\n",
      "test Accuracy of the model on the 5835 test images: 0.822\n",
      "Epoch [186/1000], Step [100/137], Loss: 0.0037\n",
      "train Accuracy of the model on the 17505 train images: 0.848\n",
      "val Accuracy of the model on the 5835 val images: 0.793\n",
      "test Accuracy of the model on the 5835 test images: 0.797\n",
      "Epoch [187/1000], Step [100/137], Loss: 0.0291\n",
      "train Accuracy of the model on the 17505 train images: 0.896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.843\n",
      "test Accuracy of the model on the 5835 test images: 0.843\n",
      "Epoch [188/1000], Step [100/137], Loss: 0.0701\n",
      "train Accuracy of the model on the 17505 train images: 0.931\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [189/1000], Step [100/137], Loss: 0.0257\n",
      "train Accuracy of the model on the 17505 train images: 0.955\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [190/1000], Step [100/137], Loss: 0.0031\n",
      "train Accuracy of the model on the 17505 train images: 0.941\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.883\n",
      "Epoch [191/1000], Step [100/137], Loss: 0.0044\n",
      "train Accuracy of the model on the 17505 train images: 0.942\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [192/1000], Step [100/137], Loss: 0.0067\n",
      "train Accuracy of the model on the 17505 train images: 0.917\n",
      "val Accuracy of the model on the 5835 val images: 0.857\n",
      "test Accuracy of the model on the 5835 test images: 0.858\n",
      "Epoch [193/1000], Step [100/137], Loss: 0.0039\n",
      "train Accuracy of the model on the 17505 train images: 0.934\n",
      "val Accuracy of the model on the 5835 val images: 0.871\n",
      "test Accuracy of the model on the 5835 test images: 0.868\n",
      "Epoch [194/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.886\n",
      "val Accuracy of the model on the 5835 val images: 0.802\n",
      "test Accuracy of the model on the 5835 test images: 0.804\n",
      "Epoch [195/1000], Step [100/137], Loss: 0.0038\n",
      "train Accuracy of the model on the 17505 train images: 0.733\n",
      "val Accuracy of the model on the 5835 val images: 0.654\n",
      "test Accuracy of the model on the 5835 test images: 0.656\n",
      "Epoch [196/1000], Step [100/137], Loss: 0.0098\n",
      "train Accuracy of the model on the 17505 train images: 0.799\n",
      "val Accuracy of the model on the 5835 val images: 0.732\n",
      "test Accuracy of the model on the 5835 test images: 0.731\n",
      "Epoch [197/1000], Step [100/137], Loss: 0.0047\n",
      "train Accuracy of the model on the 17505 train images: 0.780\n",
      "val Accuracy of the model on the 5835 val images: 0.704\n",
      "test Accuracy of the model on the 5835 test images: 0.708\n",
      "Epoch [198/1000], Step [100/137], Loss: 0.0053\n",
      "train Accuracy of the model on the 17505 train images: 0.678\n",
      "val Accuracy of the model on the 5835 val images: 0.603\n",
      "test Accuracy of the model on the 5835 test images: 0.613\n",
      "Epoch [199/1000], Step [100/137], Loss: 0.0062\n",
      "train Accuracy of the model on the 17505 train images: 0.739\n",
      "val Accuracy of the model on the 5835 val images: 0.661\n",
      "test Accuracy of the model on the 5835 test images: 0.671\n",
      "Epoch [200/1000], Step [100/137], Loss: 0.0015\n",
      "train Accuracy of the model on the 17505 train images: 0.715\n",
      "val Accuracy of the model on the 5835 val images: 0.633\n",
      "test Accuracy of the model on the 5835 test images: 0.636\n",
      "Epoch [201/1000], Step [100/137], Loss: 0.0758\n",
      "train Accuracy of the model on the 17505 train images: 0.937\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [202/1000], Step [100/137], Loss: 0.0595\n",
      "train Accuracy of the model on the 17505 train images: 0.939\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [203/1000], Step [100/137], Loss: 0.0181\n",
      "train Accuracy of the model on the 17505 train images: 0.941\n",
      "val Accuracy of the model on the 5835 val images: 0.851\n",
      "test Accuracy of the model on the 5835 test images: 0.851\n",
      "Epoch [204/1000], Step [100/137], Loss: 0.0065\n",
      "train Accuracy of the model on the 17505 train images: 0.891\n",
      "val Accuracy of the model on the 5835 val images: 0.814\n",
      "test Accuracy of the model on the 5835 test images: 0.810\n",
      "Epoch [205/1000], Step [100/137], Loss: 0.0068\n",
      "train Accuracy of the model on the 17505 train images: 0.687\n",
      "val Accuracy of the model on the 5835 val images: 0.621\n",
      "test Accuracy of the model on the 5835 test images: 0.632\n",
      "Epoch [206/1000], Step [100/137], Loss: 0.0065\n",
      "train Accuracy of the model on the 17505 train images: 0.700\n",
      "val Accuracy of the model on the 5835 val images: 0.627\n",
      "test Accuracy of the model on the 5835 test images: 0.636\n",
      "Epoch [207/1000], Step [100/137], Loss: 0.0020\n",
      "train Accuracy of the model on the 17505 train images: 0.689\n",
      "val Accuracy of the model on the 5835 val images: 0.616\n",
      "test Accuracy of the model on the 5835 test images: 0.631\n",
      "Epoch [208/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.731\n",
      "val Accuracy of the model on the 5835 val images: 0.648\n",
      "test Accuracy of the model on the 5835 test images: 0.662\n",
      "Epoch [209/1000], Step [100/137], Loss: 0.0013\n",
      "train Accuracy of the model on the 17505 train images: 0.813\n",
      "val Accuracy of the model on the 5835 val images: 0.728\n",
      "test Accuracy of the model on the 5835 test images: 0.732\n",
      "Epoch [210/1000], Step [100/137], Loss: 0.0024\n",
      "train Accuracy of the model on the 17505 train images: 0.763\n",
      "val Accuracy of the model on the 5835 val images: 0.681\n",
      "test Accuracy of the model on the 5835 test images: 0.687\n",
      "Epoch [211/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.817\n",
      "val Accuracy of the model on the 5835 val images: 0.732\n",
      "test Accuracy of the model on the 5835 test images: 0.738\n",
      "Epoch [212/1000], Step [100/137], Loss: 0.0020\n",
      "train Accuracy of the model on the 17505 train images: 0.776\n",
      "val Accuracy of the model on the 5835 val images: 0.692\n",
      "test Accuracy of the model on the 5835 test images: 0.699\n",
      "Epoch [213/1000], Step [100/137], Loss: 0.0043\n",
      "train Accuracy of the model on the 17505 train images: 0.592\n",
      "val Accuracy of the model on the 5835 val images: 0.534\n",
      "test Accuracy of the model on the 5835 test images: 0.541\n",
      "Epoch [214/1000], Step [100/137], Loss: 0.0031\n",
      "train Accuracy of the model on the 17505 train images: 0.675\n",
      "val Accuracy of the model on the 5835 val images: 0.598\n",
      "test Accuracy of the model on the 5835 test images: 0.606\n",
      "Epoch [215/1000], Step [100/137], Loss: 0.0391\n",
      "train Accuracy of the model on the 17505 train images: 0.814\n",
      "val Accuracy of the model on the 5835 val images: 0.753\n",
      "test Accuracy of the model on the 5835 test images: 0.756\n",
      "Epoch [216/1000], Step [100/137], Loss: 0.0132\n",
      "train Accuracy of the model on the 17505 train images: 0.923\n",
      "val Accuracy of the model on the 5835 val images: 0.840\n",
      "test Accuracy of the model on the 5835 test images: 0.839\n",
      "Epoch [217/1000], Step [100/137], Loss: 0.0078\n",
      "train Accuracy of the model on the 17505 train images: 0.854\n",
      "val Accuracy of the model on the 5835 val images: 0.775\n",
      "test Accuracy of the model on the 5835 test images: 0.771\n",
      "Epoch [218/1000], Step [100/137], Loss: 0.0410\n",
      "train Accuracy of the model on the 17505 train images: 0.839\n",
      "val Accuracy of the model on the 5835 val images: 0.750\n",
      "test Accuracy of the model on the 5835 test images: 0.744\n",
      "Epoch [219/1000], Step [100/137], Loss: 0.0026\n",
      "train Accuracy of the model on the 17505 train images: 0.847\n",
      "val Accuracy of the model on the 5835 val images: 0.761\n",
      "test Accuracy of the model on the 5835 test images: 0.763\n",
      "Epoch [220/1000], Step [100/137], Loss: 0.0026\n",
      "train Accuracy of the model on the 17505 train images: 0.710\n",
      "val Accuracy of the model on the 5835 val images: 0.633\n",
      "test Accuracy of the model on the 5835 test images: 0.643\n",
      "Epoch [221/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.819\n",
      "val Accuracy of the model on the 5835 val images: 0.738\n",
      "test Accuracy of the model on the 5835 test images: 0.736\n",
      "Epoch [222/1000], Step [100/137], Loss: 0.0071\n",
      "train Accuracy of the model on the 17505 train images: 0.682\n",
      "val Accuracy of the model on the 5835 val images: 0.612\n",
      "test Accuracy of the model on the 5835 test images: 0.615\n",
      "Epoch [223/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.670\n",
      "val Accuracy of the model on the 5835 val images: 0.597\n",
      "test Accuracy of the model on the 5835 test images: 0.606\n",
      "Epoch [224/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.618\n",
      "test Accuracy of the model on the 5835 test images: 0.629\n",
      "Epoch [225/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.741\n",
      "val Accuracy of the model on the 5835 val images: 0.661\n",
      "test Accuracy of the model on the 5835 test images: 0.661\n",
      "Epoch [226/1000], Step [100/137], Loss: 0.0024\n",
      "train Accuracy of the model on the 17505 train images: 0.756\n",
      "val Accuracy of the model on the 5835 val images: 0.680\n",
      "test Accuracy of the model on the 5835 test images: 0.689\n",
      "Epoch [227/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.666\n",
      "val Accuracy of the model on the 5835 val images: 0.593\n",
      "test Accuracy of the model on the 5835 test images: 0.592\n",
      "Epoch [228/1000], Step [100/137], Loss: 0.0047\n",
      "train Accuracy of the model on the 17505 train images: 0.673\n",
      "val Accuracy of the model on the 5835 val images: 0.599\n",
      "test Accuracy of the model on the 5835 test images: 0.603\n",
      "Epoch [229/1000], Step [100/137], Loss: 0.0060\n",
      "train Accuracy of the model on the 17505 train images: 0.777\n",
      "val Accuracy of the model on the 5835 val images: 0.696\n",
      "test Accuracy of the model on the 5835 test images: 0.699\n",
      "Epoch [230/1000], Step [100/137], Loss: 0.0061\n",
      "train Accuracy of the model on the 17505 train images: 0.821\n",
      "val Accuracy of the model on the 5835 val images: 0.749\n",
      "test Accuracy of the model on the 5835 test images: 0.745\n",
      "Epoch [231/1000], Step [100/137], Loss: 0.0633\n",
      "train Accuracy of the model on the 17505 train images: 0.946\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.890\n",
      "Epoch [232/1000], Step [100/137], Loss: 0.0322\n",
      "train Accuracy of the model on the 17505 train images: 0.929\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [233/1000], Step [100/137], Loss: 0.0042\n",
      "train Accuracy of the model on the 17505 train images: 0.899\n",
      "val Accuracy of the model on the 5835 val images: 0.817\n",
      "test Accuracy of the model on the 5835 test images: 0.816\n",
      "Epoch [234/1000], Step [100/137], Loss: 0.0042\n",
      "train Accuracy of the model on the 17505 train images: 0.662\n",
      "val Accuracy of the model on the 5835 val images: 0.587\n",
      "test Accuracy of the model on the 5835 test images: 0.588\n",
      "Epoch [235/1000], Step [100/137], Loss: 0.0009\n",
      "train Accuracy of the model on the 17505 train images: 0.702\n",
      "val Accuracy of the model on the 5835 val images: 0.628\n",
      "test Accuracy of the model on the 5835 test images: 0.633\n",
      "Epoch [236/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.846\n",
      "val Accuracy of the model on the 5835 val images: 0.760\n",
      "test Accuracy of the model on the 5835 test images: 0.760\n",
      "Epoch [237/1000], Step [100/137], Loss: 0.0271\n",
      "train Accuracy of the model on the 17505 train images: 0.479\n",
      "val Accuracy of the model on the 5835 val images: 0.429\n",
      "test Accuracy of the model on the 5835 test images: 0.443\n",
      "Epoch [238/1000], Step [100/137], Loss: 0.0318\n",
      "train Accuracy of the model on the 17505 train images: 0.933\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.891\n",
      "Epoch [239/1000], Step [100/137], Loss: 0.0166\n",
      "train Accuracy of the model on the 17505 train images: 0.721\n",
      "val Accuracy of the model on the 5835 val images: 0.649\n",
      "test Accuracy of the model on the 5835 test images: 0.645\n",
      "Epoch [240/1000], Step [100/137], Loss: 0.0024\n",
      "train Accuracy of the model on the 17505 train images: 0.730\n",
      "val Accuracy of the model on the 5835 val images: 0.656\n",
      "test Accuracy of the model on the 5835 test images: 0.658\n",
      "Epoch [241/1000], Step [100/137], Loss: 0.0042\n",
      "train Accuracy of the model on the 17505 train images: 0.914\n",
      "val Accuracy of the model on the 5835 val images: 0.819\n",
      "test Accuracy of the model on the 5835 test images: 0.815\n",
      "Epoch [242/1000], Step [100/137], Loss: 0.0019\n",
      "train Accuracy of the model on the 17505 train images: 0.881\n",
      "val Accuracy of the model on the 5835 val images: 0.796\n",
      "test Accuracy of the model on the 5835 test images: 0.786\n",
      "Epoch [243/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.899\n",
      "val Accuracy of the model on the 5835 val images: 0.808\n",
      "test Accuracy of the model on the 5835 test images: 0.803\n",
      "Epoch [244/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.890\n",
      "val Accuracy of the model on the 5835 val images: 0.785\n",
      "test Accuracy of the model on the 5835 test images: 0.785\n",
      "Epoch [245/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.955\n",
      "val Accuracy of the model on the 5835 val images: 0.847\n",
      "test Accuracy of the model on the 5835 test images: 0.846\n",
      "Epoch [246/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.746\n",
      "val Accuracy of the model on the 5835 val images: 0.663\n",
      "test Accuracy of the model on the 5835 test images: 0.651\n",
      "Epoch [247/1000], Step [100/137], Loss: 0.0024\n",
      "train Accuracy of the model on the 17505 train images: 0.971\n",
      "val Accuracy of the model on the 5835 val images: 0.859\n",
      "test Accuracy of the model on the 5835 test images: 0.857\n",
      "Epoch [248/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.980\n",
      "val Accuracy of the model on the 5835 val images: 0.865\n",
      "test Accuracy of the model on the 5835 test images: 0.867\n",
      "Epoch [249/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.967\n",
      "val Accuracy of the model on the 5835 val images: 0.844\n",
      "test Accuracy of the model on the 5835 test images: 0.843\n",
      "Epoch [250/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.946\n",
      "val Accuracy of the model on the 5835 val images: 0.821\n",
      "test Accuracy of the model on the 5835 test images: 0.825\n",
      "Epoch [251/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.960\n",
      "val Accuracy of the model on the 5835 val images: 0.833\n",
      "test Accuracy of the model on the 5835 test images: 0.835\n",
      "Epoch [252/1000], Step [100/137], Loss: 0.0037\n",
      "train Accuracy of the model on the 17505 train images: 0.810\n",
      "val Accuracy of the model on the 5835 val images: 0.719\n",
      "test Accuracy of the model on the 5835 test images: 0.713\n",
      "Epoch [253/1000], Step [100/137], Loss: 0.0078\n",
      "train Accuracy of the model on the 17505 train images: 0.856\n",
      "val Accuracy of the model on the 5835 val images: 0.761\n",
      "test Accuracy of the model on the 5835 test images: 0.756\n",
      "Epoch [254/1000], Step [100/137], Loss: 0.0683\n",
      "train Accuracy of the model on the 17505 train images: 0.670\n",
      "val Accuracy of the model on the 5835 val images: 0.614\n",
      "test Accuracy of the model on the 5835 test images: 0.607\n",
      "Epoch [255/1000], Step [100/137], Loss: 0.0337\n",
      "train Accuracy of the model on the 17505 train images: 0.962\n",
      "val Accuracy of the model on the 5835 val images: 0.885\n",
      "test Accuracy of the model on the 5835 test images: 0.885\n",
      "Epoch [256/1000], Step [100/137], Loss: 0.0072\n",
      "train Accuracy of the model on the 17505 train images: 0.947\n",
      "val Accuracy of the model on the 5835 val images: 0.864\n",
      "test Accuracy of the model on the 5835 test images: 0.868\n",
      "Epoch [257/1000], Step [100/137], Loss: 0.0065\n",
      "train Accuracy of the model on the 17505 train images: 0.927\n",
      "val Accuracy of the model on the 5835 val images: 0.833\n",
      "test Accuracy of the model on the 5835 test images: 0.831\n",
      "Epoch [258/1000], Step [100/137], Loss: 0.0044\n",
      "train Accuracy of the model on the 17505 train images: 0.939\n",
      "val Accuracy of the model on the 5835 val images: 0.823\n",
      "test Accuracy of the model on the 5835 test images: 0.823\n",
      "Epoch [259/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.985\n",
      "val Accuracy of the model on the 5835 val images: 0.871\n",
      "test Accuracy of the model on the 5835 test images: 0.876\n",
      "Epoch [260/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [261/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [262/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [263/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [264/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [265/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [266/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [267/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [268/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [269/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [270/1000], Step [100/137], Loss: 0.0784\n",
      "train Accuracy of the model on the 17505 train images: 0.927\n",
      "val Accuracy of the model on the 5835 val images: 0.855\n",
      "test Accuracy of the model on the 5835 test images: 0.854\n",
      "Epoch [271/1000], Step [100/137], Loss: 0.0083\n",
      "train Accuracy of the model on the 17505 train images: 0.913\n",
      "val Accuracy of the model on the 5835 val images: 0.834\n",
      "test Accuracy of the model on the 5835 test images: 0.831\n",
      "Epoch [272/1000], Step [100/137], Loss: 0.0052\n",
      "train Accuracy of the model on the 17505 train images: 0.954\n",
      "val Accuracy of the model on the 5835 val images: 0.866\n",
      "test Accuracy of the model on the 5835 test images: 0.864\n",
      "Epoch [273/1000], Step [100/137], Loss: 0.0029\n",
      "train Accuracy of the model on the 17505 train images: 0.845\n",
      "val Accuracy of the model on the 5835 val images: 0.758\n",
      "test Accuracy of the model on the 5835 test images: 0.755\n",
      "Epoch [274/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.979\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [275/1000], Step [100/137], Loss: 0.0026\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [276/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [277/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [278/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [279/1000], Step [100/137], Loss: 0.0055\n",
      "train Accuracy of the model on the 17505 train images: 0.979\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [280/1000], Step [100/137], Loss: 0.0064\n",
      "train Accuracy of the model on the 17505 train images: 0.764\n",
      "val Accuracy of the model on the 5835 val images: 0.682\n",
      "test Accuracy of the model on the 5835 test images: 0.674\n",
      "Epoch [281/1000], Step [100/137], Loss: 0.0024\n",
      "train Accuracy of the model on the 17505 train images: 0.956\n",
      "val Accuracy of the model on the 5835 val images: 0.855\n",
      "test Accuracy of the model on the 5835 test images: 0.855\n",
      "Epoch [282/1000], Step [100/137], Loss: 0.0247\n",
      "train Accuracy of the model on the 17505 train images: 0.934\n",
      "val Accuracy of the model on the 5835 val images: 0.872\n",
      "test Accuracy of the model on the 5835 test images: 0.874\n",
      "Epoch [283/1000], Step [100/137], Loss: 0.0474\n",
      "train Accuracy of the model on the 17505 train images: 0.778\n",
      "val Accuracy of the model on the 5835 val images: 0.712\n",
      "test Accuracy of the model on the 5835 test images: 0.712\n",
      "Epoch [284/1000], Step [100/137], Loss: 0.0049\n",
      "train Accuracy of the model on the 17505 train images: 0.954\n",
      "val Accuracy of the model on the 5835 val images: 0.846\n",
      "test Accuracy of the model on the 5835 test images: 0.842\n",
      "Epoch [285/1000], Step [100/137], Loss: 0.0038\n",
      "train Accuracy of the model on the 17505 train images: 0.986\n",
      "val Accuracy of the model on the 5835 val images: 0.887\n",
      "test Accuracy of the model on the 5835 test images: 0.889\n",
      "Epoch [286/1000], Step [100/137], Loss: 0.0020\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [287/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [288/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [289/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [290/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [291/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [292/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [293/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [294/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [295/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [296/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [297/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [298/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [299/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.883\n",
      "test Accuracy of the model on the 5835 test images: 0.879\n",
      "Epoch [300/1000], Step [100/137], Loss: 0.0747\n",
      "train Accuracy of the model on the 17505 train images: 0.931\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [301/1000], Step [100/137], Loss: 0.0883\n",
      "train Accuracy of the model on the 17505 train images: 0.916\n",
      "val Accuracy of the model on the 5835 val images: 0.838\n",
      "test Accuracy of the model on the 5835 test images: 0.841\n",
      "Epoch [302/1000], Step [100/137], Loss: 0.0072\n",
      "train Accuracy of the model on the 17505 train images: 0.767\n",
      "val Accuracy of the model on the 5835 val images: 0.707\n",
      "test Accuracy of the model on the 5835 test images: 0.714\n",
      "Epoch [303/1000], Step [100/137], Loss: 0.0024\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [304/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [305/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [306/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [307/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [308/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [309/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [310/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [311/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [312/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [313/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [314/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [315/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.906\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [316/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [317/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [318/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [319/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.954\n",
      "val Accuracy of the model on the 5835 val images: 0.875\n",
      "test Accuracy of the model on the 5835 test images: 0.871\n",
      "Epoch [320/1000], Step [100/137], Loss: 0.0398\n",
      "train Accuracy of the model on the 17505 train images: 0.805\n",
      "val Accuracy of the model on the 5835 val images: 0.744\n",
      "test Accuracy of the model on the 5835 test images: 0.741\n",
      "Epoch [321/1000], Step [100/137], Loss: 0.0084\n",
      "train Accuracy of the model on the 17505 train images: 0.926\n",
      "val Accuracy of the model on the 5835 val images: 0.845\n",
      "test Accuracy of the model on the 5835 test images: 0.840\n",
      "Epoch [322/1000], Step [100/137], Loss: 0.0154\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.890\n",
      "test Accuracy of the model on the 5835 test images: 0.890\n",
      "Epoch [323/1000], Step [100/137], Loss: 0.0020\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [324/1000], Step [100/137], Loss: 0.0013\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [325/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [326/1000], Step [100/137], Loss: 0.0011\n",
      "train Accuracy of the model on the 17505 train images: 0.735\n",
      "val Accuracy of the model on the 5835 val images: 0.652\n",
      "test Accuracy of the model on the 5835 test images: 0.638\n",
      "Epoch [327/1000], Step [100/137], Loss: 0.0048\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [328/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [329/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [330/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [331/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [332/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [333/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [334/1000], Step [100/137], Loss: 0.0009\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.905\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [335/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [336/1000], Step [100/137], Loss: 0.0035\n",
      "train Accuracy of the model on the 17505 train images: 0.973\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.891\n",
      "Epoch [337/1000], Step [100/137], Loss: 0.0549\n",
      "train Accuracy of the model on the 17505 train images: 0.930\n",
      "val Accuracy of the model on the 5835 val images: 0.863\n",
      "test Accuracy of the model on the 5835 test images: 0.861\n",
      "Epoch [338/1000], Step [100/137], Loss: 0.0406\n",
      "train Accuracy of the model on the 17505 train images: 0.421\n",
      "val Accuracy of the model on the 5835 val images: 0.374\n",
      "test Accuracy of the model on the 5835 test images: 0.371\n",
      "Epoch [339/1000], Step [100/137], Loss: 0.0099\n",
      "train Accuracy of the model on the 17505 train images: 0.866\n",
      "val Accuracy of the model on the 5835 val images: 0.784\n",
      "test Accuracy of the model on the 5835 test images: 0.780\n",
      "Epoch [340/1000], Step [100/137], Loss: 0.0369\n",
      "train Accuracy of the model on the 17505 train images: 0.853\n",
      "val Accuracy of the model on the 5835 val images: 0.792\n",
      "test Accuracy of the model on the 5835 test images: 0.797\n",
      "Epoch [341/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.955\n",
      "val Accuracy of the model on the 5835 val images: 0.850\n",
      "test Accuracy of the model on the 5835 test images: 0.854\n",
      "Epoch [342/1000], Step [100/137], Loss: 0.0018\n",
      "train Accuracy of the model on the 17505 train images: 0.989\n",
      "val Accuracy of the model on the 5835 val images: 0.881\n",
      "test Accuracy of the model on the 5835 test images: 0.880\n",
      "Epoch [343/1000], Step [100/137], Loss: 0.0013\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [344/1000], Step [100/137], Loss: 0.0011\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [345/1000], Step [100/137], Loss: 0.0009\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [346/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [347/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [348/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [349/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [350/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.905\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [351/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.905\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [352/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [353/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [354/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [355/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [356/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [357/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [358/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [359/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [360/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [361/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [362/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [363/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [364/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [365/1000], Step [100/137], Loss: 0.0629\n",
      "train Accuracy of the model on the 17505 train images: 0.878\n",
      "val Accuracy of the model on the 5835 val images: 0.833\n",
      "test Accuracy of the model on the 5835 test images: 0.837\n",
      "Epoch [366/1000], Step [100/137], Loss: 0.0158\n",
      "train Accuracy of the model on the 17505 train images: 0.675\n",
      "val Accuracy of the model on the 5835 val images: 0.621\n",
      "test Accuracy of the model on the 5835 test images: 0.626\n",
      "Epoch [367/1000], Step [100/137], Loss: 0.0070\n",
      "train Accuracy of the model on the 17505 train images: 0.976\n",
      "val Accuracy of the model on the 5835 val images: 0.883\n",
      "test Accuracy of the model on the 5835 test images: 0.879\n",
      "Epoch [368/1000], Step [100/137], Loss: 0.0088\n",
      "train Accuracy of the model on the 17505 train images: 0.989\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [369/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [370/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [371/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.905\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [372/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [373/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [374/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [375/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [376/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [377/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [378/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [379/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [380/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [381/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [382/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [383/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [384/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [385/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [386/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [387/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [388/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [389/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [390/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [391/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [392/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [393/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [394/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [395/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [396/1000], Step [100/137], Loss: 0.1034\n",
      "train Accuracy of the model on the 17505 train images: 0.816\n",
      "val Accuracy of the model on the 5835 val images: 0.782\n",
      "test Accuracy of the model on the 5835 test images: 0.791\n",
      "Epoch [397/1000], Step [100/137], Loss: 0.0783\n",
      "train Accuracy of the model on the 17505 train images: 0.935\n",
      "val Accuracy of the model on the 5835 val images: 0.868\n",
      "test Accuracy of the model on the 5835 test images: 0.872\n",
      "Epoch [398/1000], Step [100/137], Loss: 0.0313\n",
      "train Accuracy of the model on the 17505 train images: 0.890\n",
      "val Accuracy of the model on the 5835 val images: 0.801\n",
      "test Accuracy of the model on the 5835 test images: 0.794\n",
      "Epoch [399/1000], Step [100/137], Loss: 0.0044\n",
      "train Accuracy of the model on the 17505 train images: 0.980\n",
      "val Accuracy of the model on the 5835 val images: 0.870\n",
      "test Accuracy of the model on the 5835 test images: 0.870\n",
      "Epoch [400/1000], Step [100/137], Loss: 0.0033\n",
      "train Accuracy of the model on the 17505 train images: 0.986\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [401/1000], Step [100/137], Loss: 0.0013\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [402/1000], Step [100/137], Loss: 0.0009\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [403/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [404/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [405/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [406/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [407/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [408/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [409/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [410/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [411/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [412/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [413/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [414/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [415/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [416/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [417/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [418/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [419/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [420/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [421/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [422/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [423/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [424/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [425/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [426/1000], Step [100/137], Loss: 0.1075\n",
      "train Accuracy of the model on the 17505 train images: 0.934\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.885\n",
      "Epoch [427/1000], Step [100/137], Loss: 0.0291\n",
      "train Accuracy of the model on the 17505 train images: 0.961\n",
      "val Accuracy of the model on the 5835 val images: 0.872\n",
      "test Accuracy of the model on the 5835 test images: 0.877\n",
      "Epoch [428/1000], Step [100/137], Loss: 0.0095\n",
      "train Accuracy of the model on the 17505 train images: 0.886\n",
      "val Accuracy of the model on the 5835 val images: 0.801\n",
      "test Accuracy of the model on the 5835 test images: 0.798\n",
      "Epoch [429/1000], Step [100/137], Loss: 0.0045\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [430/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.987\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [431/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [432/1000], Step [100/137], Loss: 0.0045\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [433/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [434/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [435/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [436/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [437/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [438/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [439/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [440/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [441/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [442/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [443/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [444/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [445/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [446/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [447/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.905\n",
      "Epoch [448/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [449/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.905\n",
      "Epoch [450/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [451/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [452/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [453/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [454/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [455/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [456/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [457/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [458/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.905\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [459/1000], Step [100/137], Loss: 0.2172\n",
      "train Accuracy of the model on the 17505 train images: 0.879\n",
      "val Accuracy of the model on the 5835 val images: 0.833\n",
      "test Accuracy of the model on the 5835 test images: 0.841\n",
      "Epoch [460/1000], Step [100/137], Loss: 0.0340\n",
      "train Accuracy of the model on the 17505 train images: 0.869\n",
      "val Accuracy of the model on the 5835 val images: 0.793\n",
      "test Accuracy of the model on the 5835 test images: 0.797\n",
      "Epoch [461/1000], Step [100/137], Loss: 0.0056\n",
      "train Accuracy of the model on the 17505 train images: 0.975\n",
      "val Accuracy of the model on the 5835 val images: 0.889\n",
      "test Accuracy of the model on the 5835 test images: 0.886\n",
      "Epoch [462/1000], Step [100/137], Loss: 0.0110\n",
      "train Accuracy of the model on the 17505 train images: 0.969\n",
      "val Accuracy of the model on the 5835 val images: 0.876\n",
      "test Accuracy of the model on the 5835 test images: 0.875\n",
      "Epoch [463/1000], Step [100/137], Loss: 0.0011\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [464/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [465/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [466/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [467/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.889\n",
      "test Accuracy of the model on the 5835 test images: 0.891\n",
      "Epoch [468/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.978\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [469/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.976\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [470/1000], Step [100/137], Loss: 0.0017\n",
      "train Accuracy of the model on the 17505 train images: 0.983\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [471/1000], Step [100/137], Loss: 0.0056\n",
      "train Accuracy of the model on the 17505 train images: 0.890\n",
      "val Accuracy of the model on the 5835 val images: 0.806\n",
      "test Accuracy of the model on the 5835 test images: 0.803\n",
      "Epoch [472/1000], Step [100/137], Loss: 0.0454\n",
      "train Accuracy of the model on the 17505 train images: 0.972\n",
      "val Accuracy of the model on the 5835 val images: 0.881\n",
      "test Accuracy of the model on the 5835 test images: 0.881\n",
      "Epoch [473/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [474/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.970\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [475/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.895\n",
      "Epoch [476/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [477/1000], Step [100/137], Loss: 0.0058\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [478/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.905\n",
      "Epoch [479/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [480/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [481/1000], Step [100/137], Loss: 0.0022\n",
      "train Accuracy of the model on the 17505 train images: 0.976\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.888\n",
      "Epoch [482/1000], Step [100/137], Loss: 0.0021\n",
      "train Accuracy of the model on the 17505 train images: 0.954\n",
      "val Accuracy of the model on the 5835 val images: 0.870\n",
      "test Accuracy of the model on the 5835 test images: 0.877\n",
      "Epoch [483/1000], Step [100/137], Loss: 0.0213\n",
      "train Accuracy of the model on the 17505 train images: 0.879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.772\n",
      "test Accuracy of the model on the 5835 test images: 0.786\n",
      "Epoch [484/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.989\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [485/1000], Step [100/137], Loss: 0.0378\n",
      "train Accuracy of the model on the 17505 train images: 0.984\n",
      "val Accuracy of the model on the 5835 val images: 0.879\n",
      "test Accuracy of the model on the 5835 test images: 0.881\n",
      "Epoch [486/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.893\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [487/1000], Step [100/137], Loss: 0.0031\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.871\n",
      "test Accuracy of the model on the 5835 test images: 0.871\n",
      "Epoch [488/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [489/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [490/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [491/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [492/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.894\n",
      "Epoch [493/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.887\n",
      "test Accuracy of the model on the 5835 test images: 0.888\n",
      "Epoch [494/1000], Step [100/137], Loss: 0.0029\n",
      "train Accuracy of the model on the 17505 train images: 0.963\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [495/1000], Step [100/137], Loss: 0.0019\n",
      "train Accuracy of the model on the 17505 train images: 0.955\n",
      "val Accuracy of the model on the 5835 val images: 0.879\n",
      "test Accuracy of the model on the 5835 test images: 0.882\n",
      "Epoch [496/1000], Step [100/137], Loss: 0.0068\n",
      "train Accuracy of the model on the 17505 train images: 0.963\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [497/1000], Step [100/137], Loss: 0.0140\n",
      "train Accuracy of the model on the 17505 train images: 0.950\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [498/1000], Step [100/137], Loss: 0.0183\n",
      "train Accuracy of the model on the 17505 train images: 0.984\n",
      "val Accuracy of the model on the 5835 val images: 0.887\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [499/1000], Step [100/137], Loss: 0.0173\n",
      "train Accuracy of the model on the 17505 train images: 0.989\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.905\n",
      "Epoch [500/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [501/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [502/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [503/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [504/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [505/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [506/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [507/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [508/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [509/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [510/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [511/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [512/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [513/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [514/1000], Step [100/137], Loss: 0.0264\n",
      "train Accuracy of the model on the 17505 train images: 0.850\n",
      "val Accuracy of the model on the 5835 val images: 0.785\n",
      "test Accuracy of the model on the 5835 test images: 0.794\n",
      "Epoch [515/1000], Step [100/137], Loss: 0.0072\n",
      "train Accuracy of the model on the 17505 train images: 0.911\n",
      "val Accuracy of the model on the 5835 val images: 0.807\n",
      "test Accuracy of the model on the 5835 test images: 0.807\n",
      "Epoch [516/1000], Step [100/137], Loss: 0.0013\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.892\n",
      "Epoch [517/1000], Step [100/137], Loss: 0.0032\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [518/1000], Step [100/137], Loss: 0.0071\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [519/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [520/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [521/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [522/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [523/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [524/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [525/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [526/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [527/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [528/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [529/1000], Step [100/137], Loss: 0.0104\n",
      "train Accuracy of the model on the 17505 train images: 0.901\n",
      "val Accuracy of the model on the 5835 val images: 0.809\n",
      "test Accuracy of the model on the 5835 test images: 0.808\n",
      "Epoch [530/1000], Step [100/137], Loss: 0.0083\n",
      "train Accuracy of the model on the 17505 train images: 0.951\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [531/1000], Step [100/137], Loss: 0.0017\n",
      "train Accuracy of the model on the 17505 train images: 0.966\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.893\n",
      "Epoch [532/1000], Step [100/137], Loss: 0.0179\n",
      "train Accuracy of the model on the 17505 train images: 0.965\n",
      "val Accuracy of the model on the 5835 val images: 0.874\n",
      "test Accuracy of the model on the 5835 test images: 0.877\n",
      "Epoch [533/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.889\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [534/1000], Step [100/137], Loss: 0.0020\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.877\n",
      "test Accuracy of the model on the 5835 test images: 0.881\n",
      "Epoch [535/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.985\n",
      "val Accuracy of the model on the 5835 val images: 0.906\n",
      "test Accuracy of the model on the 5835 test images: 0.906\n",
      "Epoch [536/1000], Step [100/137], Loss: 0.0198\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.906\n",
      "Epoch [537/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [538/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [539/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [540/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [541/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.965\n",
      "val Accuracy of the model on the 5835 val images: 0.877\n",
      "test Accuracy of the model on the 5835 test images: 0.877\n",
      "Epoch [542/1000], Step [100/137], Loss: 0.0068\n",
      "train Accuracy of the model on the 17505 train images: 0.950\n",
      "val Accuracy of the model on the 5835 val images: 0.847\n",
      "test Accuracy of the model on the 5835 test images: 0.846\n",
      "Epoch [543/1000], Step [100/137], Loss: 0.0012\n",
      "train Accuracy of the model on the 17505 train images: 0.957\n",
      "val Accuracy of the model on the 5835 val images: 0.892\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [544/1000], Step [100/137], Loss: 0.0257\n",
      "train Accuracy of the model on the 17505 train images: 0.967\n",
      "val Accuracy of the model on the 5835 val images: 0.869\n",
      "test Accuracy of the model on the 5835 test images: 0.877\n",
      "Epoch [545/1000], Step [100/137], Loss: 0.0030\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [546/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [547/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.971\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [548/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [549/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [550/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [551/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [552/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [553/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [554/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [555/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [556/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [557/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [558/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [559/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [560/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [561/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [562/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [563/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [564/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [565/1000], Step [100/137], Loss: 0.0649\n",
      "train Accuracy of the model on the 17505 train images: 0.889\n",
      "val Accuracy of the model on the 5835 val images: 0.827\n",
      "test Accuracy of the model on the 5835 test images: 0.832\n",
      "Epoch [566/1000], Step [100/137], Loss: 0.0038\n",
      "train Accuracy of the model on the 17505 train images: 0.940\n",
      "val Accuracy of the model on the 5835 val images: 0.866\n",
      "test Accuracy of the model on the 5835 test images: 0.872\n",
      "Epoch [567/1000], Step [100/137], Loss: 0.0016\n",
      "train Accuracy of the model on the 17505 train images: 0.968\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.878\n",
      "Epoch [568/1000], Step [100/137], Loss: 0.0013\n",
      "train Accuracy of the model on the 17505 train images: 0.985\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [569/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [570/1000], Step [100/137], Loss: 0.0009\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [571/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [572/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [573/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [574/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [575/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [576/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [577/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [578/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [579/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [580/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [581/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [582/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [583/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [584/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [585/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [586/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [587/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [588/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [589/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [590/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [591/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [592/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [593/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [594/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [595/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [596/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [597/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [598/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [599/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [600/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [601/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [602/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [603/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [604/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [605/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [606/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [607/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [608/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [609/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [610/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [611/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [612/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [613/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [614/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [615/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [616/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [617/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [618/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [619/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [620/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [621/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [622/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [623/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [624/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [625/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [626/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [627/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [628/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [629/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [630/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [631/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [632/1000], Step [100/137], Loss: 0.1823\n",
      "train Accuracy of the model on the 17505 train images: 0.276\n",
      "val Accuracy of the model on the 5835 val images: 0.263\n",
      "test Accuracy of the model on the 5835 test images: 0.263\n",
      "Epoch [633/1000], Step [100/137], Loss: 0.0240\n",
      "train Accuracy of the model on the 17505 train images: 0.968\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [634/1000], Step [100/137], Loss: 0.0028\n",
      "train Accuracy of the model on the 17505 train images: 0.988\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [635/1000], Step [100/137], Loss: 0.0066\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [636/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [637/1000], Step [100/137], Loss: 0.0010\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [638/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [639/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [640/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [641/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [642/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [643/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [644/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [645/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [646/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [647/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [648/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [649/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [650/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [651/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [652/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [653/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [654/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [655/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [656/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [657/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [658/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [659/1000], Step [100/137], Loss: 0.1375\n",
      "train Accuracy of the model on the 17505 train images: 0.913\n",
      "val Accuracy of the model on the 5835 val images: 0.874\n",
      "test Accuracy of the model on the 5835 test images: 0.875\n",
      "Epoch [660/1000], Step [100/137], Loss: 0.0166\n",
      "train Accuracy of the model on the 17505 train images: 0.978\n",
      "val Accuracy of the model on the 5835 val images: 0.888\n",
      "test Accuracy of the model on the 5835 test images: 0.882\n",
      "Epoch [661/1000], Step [100/137], Loss: 0.0097\n",
      "train Accuracy of the model on the 17505 train images: 0.986\n",
      "val Accuracy of the model on the 5835 val images: 0.884\n",
      "test Accuracy of the model on the 5835 test images: 0.886\n",
      "Epoch [662/1000], Step [100/137], Loss: 0.0099\n",
      "train Accuracy of the model on the 17505 train images: 0.987\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [663/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [664/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [665/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [666/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [667/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [668/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [669/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [670/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [671/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [672/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [673/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [674/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [675/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [676/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [677/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [678/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [679/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [680/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [681/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [682/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [683/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [684/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [685/1000], Step [100/137], Loss: 0.0312\n",
      "train Accuracy of the model on the 17505 train images: 0.859\n",
      "val Accuracy of the model on the 5835 val images: 0.798\n",
      "test Accuracy of the model on the 5835 test images: 0.791\n",
      "Epoch [686/1000], Step [100/137], Loss: 0.0137\n",
      "train Accuracy of the model on the 17505 train images: 0.804\n",
      "val Accuracy of the model on the 5835 val images: 0.705\n",
      "test Accuracy of the model on the 5835 test images: 0.702\n",
      "Epoch [687/1000], Step [100/137], Loss: 0.0031\n",
      "train Accuracy of the model on the 17505 train images: 0.981\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [688/1000], Step [100/137], Loss: 0.0006\n",
      "train Accuracy of the model on the 17505 train images: 0.984\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [689/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.993\n",
      "val Accuracy of the model on the 5835 val images: 0.895\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [690/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.987\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [691/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.995\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [692/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [693/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [694/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [695/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.906\n",
      "Epoch [696/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [697/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [698/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [699/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [700/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [701/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [702/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [703/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [704/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [705/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [706/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [707/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [708/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [709/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [710/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [711/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [712/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [713/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [714/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [715/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [716/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [717/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [718/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [719/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [720/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [721/1000], Step [100/137], Loss: 0.0438\n",
      "train Accuracy of the model on the 17505 train images: 0.502\n",
      "val Accuracy of the model on the 5835 val images: 0.471\n",
      "test Accuracy of the model on the 5835 test images: 0.477\n",
      "Epoch [722/1000], Step [100/137], Loss: 0.0660\n",
      "train Accuracy of the model on the 17505 train images: 0.951\n",
      "val Accuracy of the model on the 5835 val images: 0.871\n",
      "test Accuracy of the model on the 5835 test images: 0.869\n",
      "Epoch [723/1000], Step [100/137], Loss: 0.0021\n",
      "train Accuracy of the model on the 17505 train images: 0.978\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.897\n",
      "Epoch [724/1000], Step [100/137], Loss: 0.0048\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [725/1000], Step [100/137], Loss: 0.0008\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [726/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.998\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [727/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [728/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [729/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [730/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [731/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [732/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [733/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [734/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [735/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [736/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [737/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [738/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [739/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [740/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [741/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [742/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [743/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [744/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [745/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [746/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [747/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [748/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [749/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [750/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [751/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [752/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [753/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [754/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [755/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [756/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [757/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [758/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [759/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [760/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [761/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [762/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [763/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [764/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [765/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [766/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [767/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [768/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [769/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [770/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [771/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [772/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [773/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [774/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [775/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [776/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [777/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [778/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [779/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [780/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [781/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [782/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [783/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [784/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [785/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [786/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [787/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [788/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [789/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [790/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [791/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [792/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [793/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [794/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [795/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [796/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [797/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [798/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [799/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [800/1000], Step [100/137], Loss: 0.0402\n",
      "train Accuracy of the model on the 17505 train images: 0.818\n",
      "val Accuracy of the model on the 5835 val images: 0.758\n",
      "test Accuracy of the model on the 5835 test images: 0.753\n",
      "Epoch [801/1000], Step [100/137], Loss: 0.0034\n",
      "train Accuracy of the model on the 17505 train images: 0.960\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [802/1000], Step [100/137], Loss: 0.0173\n",
      "train Accuracy of the model on the 17505 train images: 0.983\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [803/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [804/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.990\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.906\n",
      "Epoch [805/1000], Step [100/137], Loss: 0.0005\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [806/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.994\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.906\n",
      "Epoch [807/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [808/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.905\n",
      "Epoch [809/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [810/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [811/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [812/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.903\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [813/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [814/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [815/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [816/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [817/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [818/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [819/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [820/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [821/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [822/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [823/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [824/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [825/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [826/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [827/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [828/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [829/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [830/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [831/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [832/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [833/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [834/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [835/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [836/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [837/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [838/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [839/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [840/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [841/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [842/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [843/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [844/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [845/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [846/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [847/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [848/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [849/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [850/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [851/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [852/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [853/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [854/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [855/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [856/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [857/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [858/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [859/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [860/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [861/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [862/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [863/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [864/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [865/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [866/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [867/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [868/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [869/1000], Step [100/137], Loss: 0.1155\n",
      "train Accuracy of the model on the 17505 train images: 0.919\n",
      "val Accuracy of the model on the 5835 val images: 0.882\n",
      "test Accuracy of the model on the 5835 test images: 0.887\n",
      "Epoch [870/1000], Step [100/137], Loss: 0.0317\n",
      "train Accuracy of the model on the 17505 train images: 0.923\n",
      "val Accuracy of the model on the 5835 val images: 0.838\n",
      "test Accuracy of the model on the 5835 test images: 0.838\n",
      "Epoch [871/1000], Step [100/137], Loss: 0.0042\n",
      "train Accuracy of the model on the 17505 train images: 0.972\n",
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [872/1000], Step [100/137], Loss: 0.0041\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [873/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.989\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [874/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.991\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [875/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [876/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [877/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.904\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [878/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.983\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [879/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [880/1000], Step [100/137], Loss: 0.0007\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.894\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [881/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [882/1000], Step [100/137], Loss: 0.0002\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [883/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [884/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [885/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [886/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [887/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [888/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [889/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [890/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [891/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [892/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [893/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [894/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [895/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [896/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [897/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [898/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [899/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [900/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [901/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [902/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [903/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [904/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [905/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [906/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [907/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [908/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [909/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [910/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [911/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [912/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [913/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [914/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [915/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [916/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [917/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [918/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [919/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [920/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [921/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [922/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [923/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [924/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [925/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [926/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.902\n",
      "Epoch [927/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [928/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [929/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [930/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [931/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [932/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [933/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [934/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [935/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [936/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [937/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [938/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [939/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [940/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [941/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [942/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [943/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [944/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [945/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [946/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [947/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [948/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [949/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [950/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [951/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [952/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [953/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [954/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [955/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [956/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [957/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [958/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [959/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [960/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.903\n",
      "Epoch [961/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.904\n",
      "Epoch [962/1000], Step [100/137], Loss: 0.0726\n",
      "train Accuracy of the model on the 17505 train images: 0.951\n",
      "val Accuracy of the model on the 5835 val images: 0.886\n",
      "test Accuracy of the model on the 5835 test images: 0.882\n",
      "Epoch [963/1000], Step [100/137], Loss: 0.0223\n",
      "train Accuracy of the model on the 17505 train images: 0.985\n",
      "val Accuracy of the model on the 5835 val images: 0.891\n",
      "test Accuracy of the model on the 5835 test images: 0.883\n",
      "Epoch [964/1000], Step [100/137], Loss: 0.0014\n",
      "train Accuracy of the model on the 17505 train images: 0.984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Accuracy of the model on the 5835 val images: 0.901\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [965/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.992\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [966/1000], Step [100/137], Loss: 0.0003\n",
      "train Accuracy of the model on the 17505 train images: 0.996\n",
      "val Accuracy of the model on the 5835 val images: 0.902\n",
      "test Accuracy of the model on the 5835 test images: 0.898\n",
      "Epoch [967/1000], Step [100/137], Loss: 0.0004\n",
      "train Accuracy of the model on the 17505 train images: 0.997\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.896\n",
      "Epoch [968/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [969/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.900\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [970/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [971/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [972/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [973/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 0.999\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [974/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [975/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [976/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [977/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [978/1000], Step [100/137], Loss: 0.0001\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [979/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.899\n",
      "test Accuracy of the model on the 5835 test images: 0.899\n",
      "Epoch [980/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [981/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [982/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [983/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [984/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [985/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [986/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [987/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [988/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [989/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [990/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [991/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [992/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [993/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [994/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [995/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.898\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [996/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [997/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n",
      "Epoch [998/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [999/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.897\n",
      "test Accuracy of the model on the 5835 test images: 0.900\n",
      "Epoch [1000/1000], Step [100/137], Loss: 0.0000\n",
      "train Accuracy of the model on the 17505 train images: 1.000\n",
      "val Accuracy of the model on the 5835 val images: 0.896\n",
      "test Accuracy of the model on the 5835 test images: 0.901\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_epochs = 1000\n",
    "cnn_epochs = {}\n",
    "\n",
    "def train(num_epochs, cnn, loaders):\n",
    "    \n",
    "    cnn.train()\n",
    "        \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(loaders['train']):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            b_x = Variable(images)   # batch x\n",
    "            b_y = Variable(labels)   # batch y\n",
    "            output = cnn(b_x)[0]               \n",
    "            loss = loss_func(output, b_y)\n",
    "            \n",
    "            # clear gradients for this training step   \n",
    "            optimizer.zero_grad()           \n",
    "            \n",
    "            # backpropagation, compute gradients \n",
    "            loss.backward()    \n",
    "            # apply gradients             \n",
    "            optimizer.step()                \n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "                cnn_epochs[epoch] = {\"loss\": loss.item(), \"train\": eval_cnn(\"train\"), \n",
    "                             \"val\": eval_cnn(\"val\"), \"test\": eval_cnn(\"test\")}\n",
    "                cnn.train()\n",
    "                pass\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "train(num_epochs, cnn, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4872efac",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 16, 5, 5], expected input[128, 64, 3, 3] to have 16 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_94475/2557412106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_94475/2928619253.py\u001b[0m in \u001b[0;36meval_cnn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpred_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_ys\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_94475/3403479315.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;31m# flatten the output of conv2 to (batch_size, 32 * 7 * 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 16, 5, 5], expected input[128, 64, 3, 3] to have 16 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "eval_cnn(\"val\")\n",
    "eval_cnn(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff8ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = ROC(num_classes=2)\n",
    "fpr, tpr, thresholds = roc(torch.FloatTensor(pred_ys), torch.FloatTensor(labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5f88265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hwixley/Documents/4th-Year/Honours-Project/localhost-data-preprocessing/ml-models/polar-lag0/viz/cnn-1000epochs-crossEnt-adamOpt-fullBatchNorm.png'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "dum_input = torch.ones(1,1,28,28)\n",
    "a, b = cnn(dum_input)\n",
    "make_dot(a, params=dict(cnn.named_parameters())).render(f\"{model_save_dir}viz/cnn-{num_epochs}epochs-crossEnt-adamOpt-fullBatchNorm\",format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bfd7daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    ep = cnn_epochs[i]\n",
    "    train.append(ep[\"train\"][2])\n",
    "    val.append(ep[\"val\"][2])\n",
    "    test.append(ep[\"test\"][2])\n",
    "    loss.append(ep[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "050b7f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x7f4197c866d0>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197d7ef10>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197d7e4f0>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197cbd700>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197cbde50>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197c45610>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197cbde20>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197c45d60>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197c4a190>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197c4a8e0>,\n",
       "  <matplotlib.axis.YTick at 0x7f4197c50130>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(0,num_epochs,1)\n",
    "\n",
    "plt.plot(epochs, train, c=\"r\", lw=0.7, label=\"Train\")\n",
    "plt.plot(epochs, val, c=\"b\", lw=0.7, label=\"Validation\")\n",
    "plt.plot(epochs, test, c=\"g\", lw=0.7, label=\"Test\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"CNN w/ full BN (polar features, lag 0) - Epoch vs. Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.yticks(np.arange(0,1.1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a039cf",
   "metadata": {},
   "source": [
    "## CORE ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c838e38",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a15df4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1596e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.eval()\n",
    "dummy_input = torch.randn(1, 1, 28, 28)\n",
    "input_names = [ \"actual_input\" ]\n",
    "output_names = [ \"output\" ]\n",
    "model = cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "85daba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, \n",
    "                  dummy_input,\n",
    "                  \"polar-window10-lag0-cnn.onnx\",\n",
    "                  verbose=False,\n",
    "                  input_names=input_names,\n",
    "                  output_names=output_names,\n",
    "                  export_params=True,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ded36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9428115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91d51b3b",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c74227a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "batch_size = 128\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a295902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e99521b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72258840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [100/137], Loss: 0.4103\n",
      "Accuracy of the model on the 17505 samples: 86.12967723507569 %\n",
      "Accuracy of the model on the 5835 samples: 85.48414738646102 %\n",
      "Accuracy of the model on the 5835 samples: 86.0325621251071 %\n",
      "Epoch [2/1000], Step [100/137], Loss: 0.3699\n",
      "Accuracy of the model on the 17505 samples: 86.48957440731219 %\n",
      "Accuracy of the model on the 5835 samples: 85.80976863753213 %\n",
      "Accuracy of the model on the 5835 samples: 86.54670094258783 %\n",
      "Epoch [3/1000], Step [100/137], Loss: 0.3612\n",
      "Accuracy of the model on the 17505 samples: 86.77520708369038 %\n",
      "Accuracy of the model on the 5835 samples: 86.37532133676092 %\n",
      "Accuracy of the model on the 5835 samples: 86.40959725792631 %\n",
      "Epoch [4/1000], Step [100/137], Loss: 0.3725\n",
      "Accuracy of the model on the 17505 samples: 87.20365609825764 %\n",
      "Accuracy of the model on the 5835 samples: 86.75235646958012 %\n",
      "Accuracy of the model on the 5835 samples: 87.09511568123393 %\n",
      "Epoch [5/1000], Step [100/137], Loss: 0.3542\n",
      "Accuracy of the model on the 17505 samples: 85.9240217080834 %\n",
      "Accuracy of the model on the 5835 samples: 85.62125107112254 %\n",
      "Accuracy of the model on the 5835 samples: 86.88946015424165 %\n",
      "Epoch [6/1000], Step [100/137], Loss: 0.3593\n",
      "Accuracy of the model on the 17505 samples: 86.49528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 86.2553556126821 %\n",
      "Accuracy of the model on the 5835 samples: 87.47215081405312 %\n",
      "Epoch [7/1000], Step [100/137], Loss: 0.3310\n",
      "Accuracy of the model on the 17505 samples: 86.72950585546987 %\n",
      "Accuracy of the model on the 5835 samples: 85.98114824335904 %\n",
      "Accuracy of the model on the 5835 samples: 87.28363324764354 %\n",
      "Epoch [8/1000], Step [100/137], Loss: 0.3529\n",
      "Accuracy of the model on the 17505 samples: 87.12367894887176 %\n",
      "Accuracy of the model on the 5835 samples: 86.76949443016281 %\n",
      "Accuracy of the model on the 5835 samples: 87.26649528706083 %\n",
      "Epoch [9/1000], Step [100/137], Loss: 0.3259\n",
      "Accuracy of the model on the 17505 samples: 87.36932305055699 %\n",
      "Accuracy of the model on the 5835 samples: 87.19794344473007 %\n",
      "Accuracy of the model on the 5835 samples: 87.23221936589546 %\n",
      "Epoch [10/1000], Step [100/137], Loss: 0.3284\n",
      "Accuracy of the model on the 17505 samples: 87.460725506998 %\n",
      "Accuracy of the model on the 5835 samples: 86.9751499571551 %\n",
      "Accuracy of the model on the 5835 samples: 86.87232219365896 %\n",
      "Epoch [11/1000], Step [100/137], Loss: 0.3368\n",
      "Accuracy of the model on the 17505 samples: 87.98628963153385 %\n",
      "Accuracy of the model on the 5835 samples: 87.45501285347044 %\n",
      "Accuracy of the model on the 5835 samples: 87.30077120822622 %\n",
      "Epoch [12/1000], Step [100/137], Loss: 0.3270\n",
      "Accuracy of the model on the 17505 samples: 87.89488717509283 %\n",
      "Accuracy of the model on the 5835 samples: 87.43787489288775 %\n",
      "Accuracy of the model on the 5835 samples: 87.3350471293916 %\n",
      "Epoch [13/1000], Step [100/137], Loss: 0.3366\n",
      "Accuracy of the model on the 17505 samples: 87.87774921451015 %\n",
      "Accuracy of the model on the 5835 samples: 87.42073693230506 %\n",
      "Accuracy of the model on the 5835 samples: 87.28363324764354 %\n",
      "Epoch [14/1000], Step [100/137], Loss: 0.3397\n",
      "Accuracy of the model on the 17505 samples: 88.09483004855755 %\n",
      "Accuracy of the model on the 5835 samples: 87.59211653813196 %\n",
      "Accuracy of the model on the 5835 samples: 87.7120822622108 %\n",
      "Epoch [15/1000], Step [100/137], Loss: 0.3316\n",
      "Accuracy of the model on the 17505 samples: 86.60382747786346 %\n",
      "Accuracy of the model on the 5835 samples: 86.11825192802057 %\n",
      "Accuracy of the model on the 5835 samples: 86.99228791773778 %\n",
      "Epoch [16/1000], Step [100/137], Loss: 0.3199\n",
      "Accuracy of the model on the 17505 samples: 87.79205941159668 %\n",
      "Accuracy of the model on the 5835 samples: 87.28363324764354 %\n",
      "Accuracy of the model on the 5835 samples: 87.62639245929735 %\n",
      "Epoch [17/1000], Step [100/137], Loss: 0.3055\n",
      "Accuracy of the model on the 17505 samples: 88.11196800914024 %\n",
      "Accuracy of the model on the 5835 samples: 87.69494430162811 %\n",
      "Accuracy of the model on the 5835 samples: 87.86632390745501 %\n",
      "Epoch [18/1000], Step [100/137], Loss: 0.2983\n",
      "Accuracy of the model on the 17505 samples: 88.38617537846329 %\n",
      "Accuracy of the model on the 5835 samples: 88.19194515852614 %\n",
      "Accuracy of the model on the 5835 samples: 88.02056555269922 %\n",
      "Epoch [19/1000], Step [100/137], Loss: 0.2868\n",
      "Accuracy of the model on the 17505 samples: 88.6203941730934 %\n",
      "Accuracy of the model on the 5835 samples: 88.08911739502999 %\n",
      "Accuracy of the model on the 5835 samples: 88.05484147386461 %\n",
      "Epoch [20/1000], Step [100/137], Loss: 0.2808\n",
      "Accuracy of the model on the 17505 samples: 88.67752070836903 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.46615252784919 %\n",
      "Epoch [21/1000], Step [100/137], Loss: 0.2862\n",
      "Accuracy of the model on the 17505 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 87.96915167095116 %\n",
      "Accuracy of the model on the 5835 samples: 88.02056555269922 %\n",
      "Epoch [22/1000], Step [100/137], Loss: 0.3122\n",
      "Accuracy of the model on the 17505 samples: 88.85461296772351 %\n",
      "Accuracy of the model on the 5835 samples: 88.22622107969151 %\n",
      "Accuracy of the model on the 5835 samples: 88.4490145672665 %\n",
      "Epoch [23/1000], Step [100/137], Loss: 0.2806\n",
      "Accuracy of the model on the 17505 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 88.03770351328193 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Epoch [24/1000], Step [100/137], Loss: 0.2767\n",
      "Accuracy of the model on the 17505 samples: 88.95744073121965 %\n",
      "Accuracy of the model on the 5835 samples: 88.03770351328193 %\n",
      "Accuracy of the model on the 5835 samples: 88.19194515852614 %\n",
      "Epoch [25/1000], Step [100/137], Loss: 0.2789\n",
      "Accuracy of the model on the 17505 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.17480719794345 %\n",
      "Accuracy of the model on the 5835 samples: 88.19194515852614 %\n",
      "Epoch [26/1000], Step [100/137], Loss: 0.2814\n",
      "Accuracy of the model on the 17505 samples: 88.76892316481005 %\n",
      "Accuracy of the model on the 5835 samples: 87.93487574978577 %\n",
      "Accuracy of the model on the 5835 samples: 88.34618680377035 %\n",
      "Epoch [27/1000], Step [100/137], Loss: 0.2684\n",
      "Accuracy of the model on the 17505 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 87.83204798628964 %\n",
      "Accuracy of the model on the 5835 samples: 87.96915167095116 %\n",
      "Epoch [28/1000], Step [100/137], Loss: 0.2656\n",
      "Accuracy of the model on the 17505 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 88.19194515852614 %\n",
      "Accuracy of the model on the 5835 samples: 88.4490145672665 %\n",
      "Epoch [29/1000], Step [100/137], Loss: 0.2557\n",
      "Accuracy of the model on the 17505 samples: 89.28877463581834 %\n",
      "Accuracy of the model on the 5835 samples: 88.56898029134533 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [30/1000], Step [100/137], Loss: 0.2711\n",
      "Accuracy of the model on the 17505 samples: 89.21451013996001 %\n",
      "Accuracy of the model on the 5835 samples: 88.55184233076264 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Epoch [31/1000], Step [100/137], Loss: 0.2779\n",
      "Accuracy of the model on the 17505 samples: 89.2944872893459 %\n",
      "Accuracy of the model on the 5835 samples: 88.4490145672665 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Epoch [32/1000], Step [100/137], Loss: 0.2583\n",
      "Accuracy of the model on the 17505 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 87.98628963153385 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Epoch [33/1000], Step [100/137], Loss: 0.2645\n",
      "Accuracy of the model on the 17505 samples: 89.43730362753499 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Epoch [34/1000], Step [100/137], Loss: 0.2776\n",
      "Accuracy of the model on the 17505 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 88.36332476435304 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [35/1000], Step [100/137], Loss: 0.2559\n",
      "Accuracy of the model on the 17505 samples: 89.04884318766067 %\n",
      "Accuracy of the model on the 5835 samples: 88.60325621251071 %\n",
      "Accuracy of the model on the 5835 samples: 88.41473864610111 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/1000], Step [100/137], Loss: 0.2598\n",
      "Accuracy of the model on the 17505 samples: 89.26021136818052 %\n",
      "Accuracy of the model on the 5835 samples: 88.22622107969151 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Epoch [37/1000], Step [100/137], Loss: 0.2554\n",
      "Accuracy of the model on the 17505 samples: 89.2316481005427 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Epoch [38/1000], Step [100/137], Loss: 0.2593\n",
      "Accuracy of the model on the 17505 samples: 89.31733790345615 %\n",
      "Accuracy of the model on the 5835 samples: 88.0719794344473 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Epoch [39/1000], Step [100/137], Loss: 0.2380\n",
      "Accuracy of the model on the 17505 samples: 89.55726935161383 %\n",
      "Accuracy of the model on the 5835 samples: 88.4490145672665 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Epoch [40/1000], Step [100/137], Loss: 0.2490\n",
      "Accuracy of the model on the 17505 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 88.6203941730934 %\n",
      "Accuracy of the model on the 5835 samples: 88.51756640959726 %\n",
      "Epoch [41/1000], Step [100/137], Loss: 0.2643\n",
      "Accuracy of the model on the 17505 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Epoch [42/1000], Step [100/137], Loss: 0.2378\n",
      "Accuracy of the model on the 17505 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Epoch [43/1000], Step [100/137], Loss: 0.2462\n",
      "Accuracy of the model on the 17505 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Epoch [44/1000], Step [100/137], Loss: 0.2527\n",
      "Accuracy of the model on the 17505 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Epoch [45/1000], Step [100/137], Loss: 0.2343\n",
      "Accuracy of the model on the 17505 samples: 89.82576406740931 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [46/1000], Step [100/137], Loss: 0.2275\n",
      "Accuracy of the model on the 17505 samples: 89.46586689517281 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Epoch [47/1000], Step [100/137], Loss: 0.2309\n",
      "Accuracy of the model on the 17505 samples: 89.38017709225936 %\n",
      "Accuracy of the model on the 5835 samples: 88.34618680377035 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Epoch [48/1000], Step [100/137], Loss: 0.2270\n",
      "Accuracy of the model on the 17505 samples: 89.74578691802343 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [49/1000], Step [100/137], Loss: 0.2180\n",
      "Accuracy of the model on the 17505 samples: 89.95144244501572 %\n",
      "Accuracy of the model on the 5835 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Epoch [50/1000], Step [100/137], Loss: 0.2241\n",
      "Accuracy of the model on the 17505 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [51/1000], Step [100/137], Loss: 0.2162\n",
      "Accuracy of the model on the 17505 samples: 90.00285632676378 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [52/1000], Step [100/137], Loss: 0.2129\n",
      "Accuracy of the model on the 17505 samples: 90.24278777492145 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Epoch [53/1000], Step [100/137], Loss: 0.2226\n",
      "Accuracy of the model on the 17505 samples: 90.03713224792916 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [54/1000], Step [100/137], Loss: 0.2014\n",
      "Accuracy of the model on the 17505 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Epoch [55/1000], Step [100/137], Loss: 0.2719\n",
      "Accuracy of the model on the 17505 samples: 89.34590117109397 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Accuracy of the model on the 5835 samples: 88.55184233076264 %\n",
      "Epoch [56/1000], Step [100/137], Loss: 0.2318\n",
      "Accuracy of the model on the 17505 samples: 90.32276492430734 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [57/1000], Step [100/137], Loss: 0.2265\n",
      "Accuracy of the model on the 17505 samples: 90.01428163381891 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [58/1000], Step [100/137], Loss: 0.2187\n",
      "Accuracy of the model on the 17505 samples: 90.34561553841759 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [59/1000], Step [100/137], Loss: 0.2486\n",
      "Accuracy of the model on the 17505 samples: 90.22564981433877 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [60/1000], Step [100/137], Loss: 0.2471\n",
      "Accuracy of the model on the 17505 samples: 89.04884318766067 %\n",
      "Accuracy of the model on the 5835 samples: 88.38046272493574 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n",
      "Epoch [61/1000], Step [100/137], Loss: 0.2297\n",
      "Accuracy of the model on the 17505 samples: 89.91716652385033 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [62/1000], Step [100/137], Loss: 0.2203\n",
      "Accuracy of the model on the 17505 samples: 89.83147672093688 %\n",
      "Accuracy of the model on the 5835 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [63/1000], Step [100/137], Loss: 0.2375\n",
      "Accuracy of the model on the 17505 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [64/1000], Step [100/137], Loss: 0.2126\n",
      "Accuracy of the model on the 17505 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [65/1000], Step [100/137], Loss: 0.2185\n",
      "Accuracy of the model on the 17505 samples: 90.22564981433877 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [66/1000], Step [100/137], Loss: 0.2082\n",
      "Accuracy of the model on the 17505 samples: 90.49985718366182 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [67/1000], Step [100/137], Loss: 0.2919\n",
      "Accuracy of the model on the 17505 samples: 89.58583261925165 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n",
      "Epoch [68/1000], Step [100/137], Loss: 0.2072\n",
      "Accuracy of the model on the 17505 samples: 90.37417880605541 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [69/1000], Step [100/137], Loss: 0.2271\n",
      "Accuracy of the model on the 17505 samples: 90.2827763496144 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [70/1000], Step [100/137], Loss: 0.2282\n",
      "Accuracy of the model on the 17505 samples: 90.16852327906312 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/1000], Step [100/137], Loss: 0.2156\n",
      "Accuracy of the model on the 17505 samples: 90.41416738074835 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [72/1000], Step [100/137], Loss: 0.2186\n",
      "Accuracy of the model on the 17505 samples: 90.49414453013425 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [73/1000], Step [100/137], Loss: 0.2123\n",
      "Accuracy of the model on the 17505 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [74/1000], Step [100/137], Loss: 0.2012\n",
      "Accuracy of the model on the 17505 samples: 90.60268494715795 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [75/1000], Step [100/137], Loss: 0.2148\n",
      "Accuracy of the model on the 17505 samples: 90.27135104255927 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [76/1000], Step [100/137], Loss: 0.2029\n",
      "Accuracy of the model on the 17505 samples: 90.69408740359897 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [77/1000], Step [100/137], Loss: 0.2085\n",
      "Accuracy of the model on the 17505 samples: 90.81976578120538 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [78/1000], Step [100/137], Loss: 0.1957\n",
      "Accuracy of the model on the 17505 samples: 90.62553556126821 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [79/1000], Step [100/137], Loss: 0.2054\n",
      "Accuracy of the model on the 17505 samples: 90.72265067123679 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [80/1000], Step [100/137], Loss: 0.2152\n",
      "Accuracy of the model on the 17505 samples: 90.82547843473293 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [81/1000], Step [100/137], Loss: 0.2006\n",
      "Accuracy of the model on the 17505 samples: 90.9397315052842 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [82/1000], Step [100/137], Loss: 0.2066\n",
      "Accuracy of the model on the 17505 samples: 90.49985718366182 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [83/1000], Step [100/137], Loss: 0.2154\n",
      "Accuracy of the model on the 17505 samples: 90.4084547272208 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [84/1000], Step [100/137], Loss: 0.2081\n",
      "Accuracy of the model on the 17505 samples: 91.0197086546701 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [85/1000], Step [100/137], Loss: 0.1978\n",
      "Accuracy of the model on the 17505 samples: 91.1282490716938 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [86/1000], Step [100/137], Loss: 0.2057\n",
      "Accuracy of the model on the 17505 samples: 90.85404170237075 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [87/1000], Step [100/137], Loss: 0.2067\n",
      "Accuracy of the model on the 17505 samples: 90.85404170237075 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [88/1000], Step [100/137], Loss: 0.2126\n",
      "Accuracy of the model on the 17505 samples: 90.83690374178806 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [89/1000], Step [100/137], Loss: 0.2167\n",
      "Accuracy of the model on the 17505 samples: 90.95115681233933 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [90/1000], Step [100/137], Loss: 0.2211\n",
      "Accuracy of the model on the 17505 samples: 90.95115681233933 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [91/1000], Step [100/137], Loss: 0.1947\n",
      "Accuracy of the model on the 17505 samples: 90.75692659240217 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [92/1000], Step [100/137], Loss: 0.1956\n",
      "Accuracy of the model on the 17505 samples: 91.16252499285919 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [93/1000], Step [100/137], Loss: 0.2012\n",
      "Accuracy of the model on the 17505 samples: 90.62553556126821 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Epoch [94/1000], Step [100/137], Loss: 0.1861\n",
      "Accuracy of the model on the 17505 samples: 91.33390459868609 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [95/1000], Step [100/137], Loss: 0.1895\n",
      "Accuracy of the model on the 17505 samples: 91.11111111111111 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [96/1000], Step [100/137], Loss: 0.1828\n",
      "Accuracy of the model on the 17505 samples: 91.53384747215081 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [97/1000], Step [100/137], Loss: 0.1874\n",
      "Accuracy of the model on the 17505 samples: 90.59125964010283 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [98/1000], Step [100/137], Loss: 0.1894\n",
      "Accuracy of the model on the 17505 samples: 91.43101970865467 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Epoch [99/1000], Step [100/137], Loss: 0.2093\n",
      "Accuracy of the model on the 17505 samples: 91.11111111111111 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [100/1000], Step [100/137], Loss: 0.1923\n",
      "Accuracy of the model on the 17505 samples: 90.61982290774064 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [101/1000], Step [100/137], Loss: 0.1795\n",
      "Accuracy of the model on the 17505 samples: 91.33390459868609 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [102/1000], Step [100/137], Loss: 0.2008\n",
      "Accuracy of the model on the 17505 samples: 91.17395029991431 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [103/1000], Step [100/137], Loss: 0.1858\n",
      "Accuracy of the model on the 17505 samples: 91.47100828334762 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [104/1000], Step [100/137], Loss: 0.2275\n",
      "Accuracy of the model on the 17505 samples: 89.90002856326764 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Epoch [105/1000], Step [100/137], Loss: 0.2196\n",
      "Accuracy of the model on the 17505 samples: 90.59697229363039 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/1000], Step [100/137], Loss: 0.2044\n",
      "Accuracy of the model on the 17505 samples: 91.07683518994573 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [107/1000], Step [100/137], Loss: 0.2063\n",
      "Accuracy of the model on the 17505 samples: 90.57412167952013 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [108/1000], Step [100/137], Loss: 0.2309\n",
      "Accuracy of the model on the 17505 samples: 90.31133961725222 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [109/1000], Step [100/137], Loss: 0.1869\n",
      "Accuracy of the model on the 17505 samples: 91.20251356755213 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [110/1000], Step [100/137], Loss: 0.2120\n",
      "Accuracy of the model on the 17505 samples: 91.04827192230792 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [111/1000], Step [100/137], Loss: 0.1891\n",
      "Accuracy of the model on the 17505 samples: 91.63667523564696 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [112/1000], Step [100/137], Loss: 0.1943\n",
      "Accuracy of the model on the 17505 samples: 90.92830619822908 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [113/1000], Step [100/137], Loss: 0.2032\n",
      "Accuracy of the model on the 17505 samples: 91.24821479577264 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [114/1000], Step [100/137], Loss: 0.1931\n",
      "Accuracy of the model on the 17505 samples: 91.43673236218223 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [115/1000], Step [100/137], Loss: 0.2046\n",
      "Accuracy of the model on the 17505 samples: 91.36246786632391 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [116/1000], Step [100/137], Loss: 0.1880\n",
      "Accuracy of the model on the 17505 samples: 91.19680091402456 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [117/1000], Step [100/137], Loss: 0.1848\n",
      "Accuracy of the model on the 17505 samples: 91.6309625821194 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [118/1000], Step [100/137], Loss: 0.1699\n",
      "Accuracy of the model on the 17505 samples: 91.45958297629248 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [119/1000], Step [100/137], Loss: 0.2155\n",
      "Accuracy of the model on the 17505 samples: 90.71693801770923 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [120/1000], Step [100/137], Loss: 0.2473\n",
      "Accuracy of the model on the 17505 samples: 89.65438446158241 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [121/1000], Step [100/137], Loss: 0.1987\n",
      "Accuracy of the model on the 17505 samples: 90.99114538703228 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [122/1000], Step [100/137], Loss: 0.2266\n",
      "Accuracy of the model on the 17505 samples: 91.05398457583547 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [123/1000], Step [100/137], Loss: 0.1905\n",
      "Accuracy of the model on the 17505 samples: 91.29962867752072 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [124/1000], Step [100/137], Loss: 0.2041\n",
      "Accuracy of the model on the 17505 samples: 91.00257069408741 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [125/1000], Step [100/137], Loss: 0.2002\n",
      "Accuracy of the model on the 17505 samples: 91.2710654098829 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [126/1000], Step [100/137], Loss: 0.2184\n",
      "Accuracy of the model on the 17505 samples: 90.6883747500714 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [127/1000], Step [100/137], Loss: 0.1867\n",
      "Accuracy of the model on the 17505 samples: 91.75664095972579 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [128/1000], Step [100/137], Loss: 0.1914\n",
      "Accuracy of the model on the 17505 samples: 91.3796058269066 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [129/1000], Step [100/137], Loss: 0.1966\n",
      "Accuracy of the model on the 17505 samples: 91.92230791202513 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [130/1000], Step [100/137], Loss: 0.2576\n",
      "Accuracy of the model on the 17505 samples: 90.96829477292202 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [131/1000], Step [100/137], Loss: 0.1814\n",
      "Accuracy of the model on the 17505 samples: 91.47100828334762 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [132/1000], Step [100/137], Loss: 0.1774\n",
      "Accuracy of the model on the 17505 samples: 90.7683518994573 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [133/1000], Step [100/137], Loss: 0.1807\n",
      "Accuracy of the model on the 17505 samples: 91.45387032276493 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [134/1000], Step [100/137], Loss: 0.1997\n",
      "Accuracy of the model on the 17505 samples: 90.8311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [135/1000], Step [100/137], Loss: 0.1729\n",
      "Accuracy of the model on the 17505 samples: 91.28820337046558 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [136/1000], Step [100/137], Loss: 0.2039\n",
      "Accuracy of the model on the 17505 samples: 91.6309625821194 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [137/1000], Step [100/137], Loss: 0.1726\n",
      "Accuracy of the model on the 17505 samples: 91.7395029991431 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [138/1000], Step [100/137], Loss: 0.1692\n",
      "Accuracy of the model on the 17505 samples: 91.57383604684375 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [139/1000], Step [100/137], Loss: 0.1796\n",
      "Accuracy of the model on the 17505 samples: 91.26535275635533 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [140/1000], Step [100/137], Loss: 0.1800\n",
      "Accuracy of the model on the 17505 samples: 92.15081405312768 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/1000], Step [100/137], Loss: 0.1719\n",
      "Accuracy of the model on the 17505 samples: 91.819480148529 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [142/1000], Step [100/137], Loss: 0.1860\n",
      "Accuracy of the model on the 17505 samples: 91.84233076263925 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [143/1000], Step [100/137], Loss: 0.1915\n",
      "Accuracy of the model on the 17505 samples: 91.71093973150528 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [144/1000], Step [100/137], Loss: 0.1638\n",
      "Accuracy of the model on the 17505 samples: 91.4253070551271 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.4370179948586 %\n",
      "Epoch [145/1000], Step [100/137], Loss: 0.1680\n",
      "Accuracy of the model on the 17505 samples: 91.99657240788346 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [146/1000], Step [100/137], Loss: 0.1742\n",
      "Accuracy of the model on the 17505 samples: 92.15652670665524 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [147/1000], Step [100/137], Loss: 0.1706\n",
      "Accuracy of the model on the 17505 samples: 92.07654955726935 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 90.36846615252784 %\n",
      "Epoch [148/1000], Step [100/137], Loss: 0.1773\n",
      "Accuracy of the model on the 17505 samples: 91.60811196800914 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [149/1000], Step [100/137], Loss: 0.2222\n",
      "Accuracy of the model on the 17505 samples: 90.47700656955156 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [150/1000], Step [100/137], Loss: 0.1818\n",
      "Accuracy of the model on the 17505 samples: 91.52242216509569 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [151/1000], Step [100/137], Loss: 0.1585\n",
      "Accuracy of the model on the 17505 samples: 92.05941159668666 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [152/1000], Step [100/137], Loss: 0.1940\n",
      "Accuracy of the model on the 17505 samples: 91.29391602399315 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Epoch [153/1000], Step [100/137], Loss: 0.1650\n",
      "Accuracy of the model on the 17505 samples: 92.12225078548985 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [154/1000], Step [100/137], Loss: 0.1772\n",
      "Accuracy of the model on the 17505 samples: 92.20222793487575 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [155/1000], Step [100/137], Loss: 0.1574\n",
      "Accuracy of the model on the 17505 samples: 92.63067694944301 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 90.53984575835476 %\n",
      "Epoch [156/1000], Step [100/137], Loss: 0.1933\n",
      "Accuracy of the model on the 17505 samples: 92.38503284775778 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [157/1000], Step [100/137], Loss: 0.1835\n",
      "Accuracy of the model on the 17505 samples: 91.88231933733219 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [158/1000], Step [100/137], Loss: 0.1799\n",
      "Accuracy of the model on the 17505 samples: 92.19080262782062 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [159/1000], Step [100/137], Loss: 0.1958\n",
      "Accuracy of the model on the 17505 samples: 91.98514710082833 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [160/1000], Step [100/137], Loss: 0.1696\n",
      "Accuracy of the model on the 17505 samples: 91.81376749500143 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [161/1000], Step [100/137], Loss: 0.1551\n",
      "Accuracy of the model on the 17505 samples: 92.49357326478149 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 90.29991431019708 %\n",
      "Epoch [162/1000], Step [100/137], Loss: 0.1626\n",
      "Accuracy of the model on the 17505 samples: 92.05369894315909 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [163/1000], Step [100/137], Loss: 0.1532\n",
      "Accuracy of the model on the 17505 samples: 92.33361896600971 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Accuracy of the model on the 5835 samples: 90.41988003427592 %\n",
      "Epoch [164/1000], Step [100/137], Loss: 0.1481\n",
      "Accuracy of the model on the 17505 samples: 92.78491859468723 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [165/1000], Step [100/137], Loss: 0.1525\n",
      "Accuracy of the model on the 17505 samples: 91.77949157383604 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [166/1000], Step [100/137], Loss: 0.2872\n",
      "Accuracy of the model on the 17505 samples: 88.35761211082549 %\n",
      "Accuracy of the model on the 5835 samples: 87.76349614395887 %\n",
      "Accuracy of the model on the 5835 samples: 87.91773778920309 %\n",
      "Epoch [167/1000], Step [100/137], Loss: 0.2812\n",
      "Accuracy of the model on the 17505 samples: 89.123107683519 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [168/1000], Step [100/137], Loss: 0.2518\n",
      "Accuracy of the model on the 17505 samples: 89.99714367323622 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Epoch [169/1000], Step [100/137], Loss: 0.2462\n",
      "Accuracy of the model on the 17505 samples: 90.43130534133105 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [170/1000], Step [100/137], Loss: 0.1816\n",
      "Accuracy of the model on the 17505 samples: 91.15681233933162 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [171/1000], Step [100/137], Loss: 0.1849\n",
      "Accuracy of the model on the 17505 samples: 91.70522707797772 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [172/1000], Step [100/137], Loss: 0.1821\n",
      "Accuracy of the model on the 17505 samples: 91.81376749500143 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [173/1000], Step [100/137], Loss: 0.1678\n",
      "Accuracy of the model on the 17505 samples: 91.52813481862324 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [174/1000], Step [100/137], Loss: 0.1646\n",
      "Accuracy of the model on the 17505 samples: 92.20222793487575 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [175/1000], Step [100/137], Loss: 0.1732\n",
      "Accuracy of the model on the 17505 samples: 92.33361896600971 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [176/1000], Step [100/137], Loss: 0.1701\n",
      "Accuracy of the model on the 17505 samples: 92.07083690374179 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [177/1000], Step [100/137], Loss: 0.1571\n",
      "Accuracy of the model on the 17505 samples: 92.63067694944301 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [178/1000], Step [100/137], Loss: 0.1659\n",
      "Accuracy of the model on the 17505 samples: 92.49928591830906 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [179/1000], Step [100/137], Loss: 0.1847\n",
      "Accuracy of the model on the 17505 samples: 91.96229648671807 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [180/1000], Step [100/137], Loss: 0.2033\n",
      "Accuracy of the model on the 17505 samples: 92.57926306769494 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [181/1000], Step [100/137], Loss: 0.1731\n",
      "Accuracy of the model on the 17505 samples: 92.46500999714367 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [182/1000], Step [100/137], Loss: 0.1983\n",
      "Accuracy of the model on the 17505 samples: 91.49957155098544 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [183/1000], Step [100/137], Loss: 0.1523\n",
      "Accuracy of the model on the 17505 samples: 92.01942302199372 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [184/1000], Step [100/137], Loss: 0.2577\n",
      "Accuracy of the model on the 17505 samples: 90.06569551556699 %\n",
      "Accuracy of the model on the 5835 samples: 88.70608397600685 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [185/1000], Step [100/137], Loss: 0.1866\n",
      "Accuracy of the model on the 17505 samples: 91.95087117966295 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [186/1000], Step [100/137], Loss: 0.1636\n",
      "Accuracy of the model on the 17505 samples: 92.0365609825764 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [187/1000], Step [100/137], Loss: 0.1568\n",
      "Accuracy of the model on the 17505 samples: 92.20794058840332 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [188/1000], Step [100/137], Loss: 0.1606\n",
      "Accuracy of the model on the 17505 samples: 91.97943444730078 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [189/1000], Step [100/137], Loss: 0.1916\n",
      "Accuracy of the model on the 17505 samples: 91.81376749500143 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [190/1000], Step [100/137], Loss: 0.1835\n",
      "Accuracy of the model on the 17505 samples: 90.81405312767781 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [191/1000], Step [100/137], Loss: 0.1665\n",
      "Accuracy of the model on the 17505 samples: 91.95658383319052 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [192/1000], Step [100/137], Loss: 0.2046\n",
      "Accuracy of the model on the 17505 samples: 91.8023421879463 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [193/1000], Step [100/137], Loss: 0.1593\n",
      "Accuracy of the model on the 17505 samples: 92.48786061125392 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [194/1000], Step [100/137], Loss: 0.1578\n",
      "Accuracy of the model on the 17505 samples: 91.99657240788346 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [195/1000], Step [100/137], Loss: 0.1673\n",
      "Accuracy of the model on the 17505 samples: 92.79634390174236 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [196/1000], Step [100/137], Loss: 0.1601\n",
      "Accuracy of the model on the 17505 samples: 91.92230791202513 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [197/1000], Step [100/137], Loss: 0.1637\n",
      "Accuracy of the model on the 17505 samples: 92.37360754070265 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [198/1000], Step [100/137], Loss: 0.1767\n",
      "Accuracy of the model on the 17505 samples: 92.35646958011996 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [199/1000], Step [100/137], Loss: 0.1796\n",
      "Accuracy of the model on the 17505 samples: 92.35646958011996 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [200/1000], Step [100/137], Loss: 0.1763\n",
      "Accuracy of the model on the 17505 samples: 92.19651528134818 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [201/1000], Step [100/137], Loss: 0.1832\n",
      "Accuracy of the model on the 17505 samples: 92.18508997429306 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [202/1000], Step [100/137], Loss: 0.1539\n",
      "Accuracy of the model on the 17505 samples: 92.33361896600971 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [203/1000], Step [100/137], Loss: 0.1634\n",
      "Accuracy of the model on the 17505 samples: 92.68209083119109 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [204/1000], Step [100/137], Loss: 0.1492\n",
      "Accuracy of the model on the 17505 samples: 92.65352756355327 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [205/1000], Step [100/137], Loss: 0.1481\n",
      "Accuracy of the model on the 17505 samples: 92.6192516423879 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [206/1000], Step [100/137], Loss: 0.1452\n",
      "Accuracy of the model on the 17505 samples: 93.03056269637247 %\n",
      "Accuracy of the model on the 5835 samples: 90.29991431019708 %\n",
      "Accuracy of the model on the 5835 samples: 90.36846615252784 %\n",
      "Epoch [207/1000], Step [100/137], Loss: 0.1341\n",
      "Accuracy of the model on the 17505 samples: 93.05341331048272 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [208/1000], Step [100/137], Loss: 0.1524\n",
      "Accuracy of the model on the 17505 samples: 92.24792916309626 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [209/1000], Step [100/137], Loss: 0.1675\n",
      "Accuracy of the model on the 17505 samples: 91.8480434161668 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [210/1000], Step [100/137], Loss: 0.2837\n",
      "Accuracy of the model on the 17505 samples: 88.66609540131391 %\n",
      "Accuracy of the model on the 5835 samples: 88.0719794344473 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [211/1000], Step [100/137], Loss: 0.2990\n",
      "Accuracy of the model on the 17505 samples: 88.52327906312482 %\n",
      "Accuracy of the model on the 5835 samples: 87.76349614395887 %\n",
      "Accuracy of the model on the 5835 samples: 87.98628963153385 %\n",
      "Epoch [212/1000], Step [100/137], Loss: 0.2711\n",
      "Accuracy of the model on the 17505 samples: 89.04884318766067 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Accuracy of the model on the 5835 samples: 88.4318766066838 %\n",
      "Epoch [213/1000], Step [100/137], Loss: 0.2651\n",
      "Accuracy of the model on the 17505 samples: 89.12882033704656 %\n",
      "Accuracy of the model on the 5835 samples: 88.24335904027421 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n",
      "Epoch [214/1000], Step [100/137], Loss: 0.2607\n",
      "Accuracy of the model on the 17505 samples: 89.43159097400742 %\n",
      "Accuracy of the model on the 5835 samples: 88.51756640959726 %\n",
      "Accuracy of the model on the 5835 samples: 88.60325621251071 %\n",
      "Epoch [215/1000], Step [100/137], Loss: 0.2442\n",
      "Accuracy of the model on the 17505 samples: 89.60297057983433 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Accuracy of the model on the 5835 samples: 88.808911739503 %\n",
      "Epoch [216/1000], Step [100/137], Loss: 0.2288\n",
      "Accuracy of the model on the 17505 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 88.70608397600685 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Epoch [217/1000], Step [100/137], Loss: 0.2339\n",
      "Accuracy of the model on the 17505 samples: 89.68866038274778 %\n",
      "Accuracy of the model on the 5835 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [218/1000], Step [100/137], Loss: 0.2076\n",
      "Accuracy of the model on the 17505 samples: 89.92859183090546 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Epoch [219/1000], Step [100/137], Loss: 0.2172\n",
      "Accuracy of the model on the 17505 samples: 89.89431590974007 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [220/1000], Step [100/137], Loss: 0.2230\n",
      "Accuracy of the model on the 17505 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Epoch [221/1000], Step [100/137], Loss: 0.2162\n",
      "Accuracy of the model on the 17505 samples: 90.29420165666953 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [222/1000], Step [100/137], Loss: 0.2102\n",
      "Accuracy of the model on the 17505 samples: 90.27706369608683 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [223/1000], Step [100/137], Loss: 0.2104\n",
      "Accuracy of the model on the 17505 samples: 90.48271922307912 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [224/1000], Step [100/137], Loss: 0.2208\n",
      "Accuracy of the model on the 17505 samples: 90.48271922307912 %\n",
      "Accuracy of the model on the 5835 samples: 88.89460154241645 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [225/1000], Step [100/137], Loss: 0.2146\n",
      "Accuracy of the model on the 17505 samples: 90.471293916024 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [226/1000], Step [100/137], Loss: 0.2106\n",
      "Accuracy of the model on the 17505 samples: 90.57412167952013 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [227/1000], Step [100/137], Loss: 0.2186\n",
      "Accuracy of the model on the 17505 samples: 90.46558126249643 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Epoch [228/1000], Step [100/137], Loss: 0.2054\n",
      "Accuracy of the model on the 17505 samples: 90.45986860896886 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [229/1000], Step [100/137], Loss: 0.2088\n",
      "Accuracy of the model on the 17505 samples: 90.6598114824336 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [230/1000], Step [100/137], Loss: 0.2008\n",
      "Accuracy of the model on the 17505 samples: 90.59125964010283 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [231/1000], Step [100/137], Loss: 0.1973\n",
      "Accuracy of the model on the 17505 samples: 90.36275349900029 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [232/1000], Step [100/137], Loss: 0.2325\n",
      "Accuracy of the model on the 17505 samples: 90.67123678948872 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [233/1000], Step [100/137], Loss: 0.1904\n",
      "Accuracy of the model on the 17505 samples: 90.61411025421309 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [234/1000], Step [100/137], Loss: 0.2056\n",
      "Accuracy of the model on the 17505 samples: 90.55127106540988 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [235/1000], Step [100/137], Loss: 0.1891\n",
      "Accuracy of the model on the 17505 samples: 90.52270779777207 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [236/1000], Step [100/137], Loss: 0.1878\n",
      "Accuracy of the model on the 17505 samples: 90.96258211939445 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [237/1000], Step [100/137], Loss: 0.2268\n",
      "Accuracy of the model on the 17505 samples: 90.65409882890603 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [238/1000], Step [100/137], Loss: 0.1955\n",
      "Accuracy of the model on the 17505 samples: 90.63696086832334 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [239/1000], Step [100/137], Loss: 0.2201\n",
      "Accuracy of the model on the 17505 samples: 90.7055127106541 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [240/1000], Step [100/137], Loss: 0.2192\n",
      "Accuracy of the model on the 17505 samples: 90.44273064838617 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Epoch [241/1000], Step [100/137], Loss: 0.2460\n",
      "Accuracy of the model on the 17505 samples: 90.25421308197657 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [242/1000], Step [100/137], Loss: 0.2295\n",
      "Accuracy of the model on the 17505 samples: 91.05969722936304 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [243/1000], Step [100/137], Loss: 0.2033\n",
      "Accuracy of the model on the 17505 samples: 90.71122536418166 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [244/1000], Step [100/137], Loss: 0.2044\n",
      "Accuracy of the model on the 17505 samples: 90.79691516709512 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [245/1000], Step [100/137], Loss: 0.1812\n",
      "Accuracy of the model on the 17505 samples: 91.32819194515852 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [246/1000], Step [100/137], Loss: 0.1819\n",
      "Accuracy of the model on the 17505 samples: 91.2082262210797 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [247/1000], Step [100/137], Loss: 0.1719\n",
      "Accuracy of the model on the 17505 samples: 91.40245644101685 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [248/1000], Step [100/137], Loss: 0.1922\n",
      "Accuracy of the model on the 17505 samples: 90.88831762353614 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [249/1000], Step [100/137], Loss: 0.1957\n",
      "Accuracy of the model on the 17505 samples: 91.28820337046558 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Epoch [250/1000], Step [100/137], Loss: 0.1807\n",
      "Accuracy of the model on the 17505 samples: 91.30534133104827 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [251/1000], Step [100/137], Loss: 0.2215\n",
      "Accuracy of the model on the 17505 samples: 90.0314195944016 %\n",
      "Accuracy of the model on the 5835 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [252/1000], Step [100/137], Loss: 0.1920\n",
      "Accuracy of the model on the 17505 samples: 91.45387032276493 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [253/1000], Step [100/137], Loss: 0.1740\n",
      "Accuracy of the model on the 17505 samples: 91.07683518994573 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Epoch [254/1000], Step [100/137], Loss: 0.1819\n",
      "Accuracy of the model on the 17505 samples: 91.5681233933162 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Epoch [255/1000], Step [100/137], Loss: 0.1926\n",
      "Accuracy of the model on the 17505 samples: 91.55669808626106 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [256/1000], Step [100/137], Loss: 0.1798\n",
      "Accuracy of the model on the 17505 samples: 91.72236503856041 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [257/1000], Step [100/137], Loss: 0.2086\n",
      "Accuracy of the model on the 17505 samples: 91.596686660954 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [258/1000], Step [100/137], Loss: 0.2380\n",
      "Accuracy of the model on the 17505 samples: 90.97400742644959 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [259/1000], Step [100/137], Loss: 0.1615\n",
      "Accuracy of the model on the 17505 samples: 91.53384747215081 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [260/1000], Step [100/137], Loss: 0.1896\n",
      "Accuracy of the model on the 17505 samples: 91.36246786632391 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [261/1000], Step [100/137], Loss: 0.1778\n",
      "Accuracy of the model on the 17505 samples: 92.09368751785205 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [262/1000], Step [100/137], Loss: 0.1740\n",
      "Accuracy of the model on the 17505 samples: 92.1793773207655 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [263/1000], Step [100/137], Loss: 0.2016\n",
      "Accuracy of the model on the 17505 samples: 91.47672093687518 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [264/1000], Step [100/137], Loss: 0.1840\n",
      "Accuracy of the model on the 17505 samples: 91.75664095972579 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [265/1000], Step [100/137], Loss: 0.2069\n",
      "Accuracy of the model on the 17505 samples: 91.76806626678092 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [266/1000], Step [100/137], Loss: 0.1797\n",
      "Accuracy of the model on the 17505 samples: 91.4253070551271 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [267/1000], Step [100/137], Loss: 0.2491\n",
      "Accuracy of the model on the 17505 samples: 90.8311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [268/1000], Step [100/137], Loss: 0.1768\n",
      "Accuracy of the model on the 17505 samples: 92.34504427306484 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [269/1000], Step [100/137], Loss: 0.1665\n",
      "Accuracy of the model on the 17505 samples: 92.3678948871751 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [270/1000], Step [100/137], Loss: 0.2470\n",
      "Accuracy of the model on the 17505 samples: 90.48843187660668 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [271/1000], Step [100/137], Loss: 0.2170\n",
      "Accuracy of the model on the 17505 samples: 90.47700656955156 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [272/1000], Step [100/137], Loss: 0.2090\n",
      "Accuracy of the model on the 17505 samples: 91.79091688089117 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [273/1000], Step [100/137], Loss: 0.1901\n",
      "Accuracy of the model on the 17505 samples: 92.12225078548985 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [274/1000], Step [100/137], Loss: 0.1769\n",
      "Accuracy of the model on the 17505 samples: 92.49928591830906 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [275/1000], Step [100/137], Loss: 0.1755\n",
      "Accuracy of the model on the 17505 samples: 92.01371036846615 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [276/1000], Step [100/137], Loss: 0.1717\n",
      "Accuracy of the model on the 17505 samples: 92.07654955726935 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [277/1000], Step [100/137], Loss: 0.2382\n",
      "Accuracy of the model on the 17505 samples: 90.09997143673236 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [278/1000], Step [100/137], Loss: 0.2098\n",
      "Accuracy of the model on the 17505 samples: 91.08826049700086 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [279/1000], Step [100/137], Loss: 0.1964\n",
      "Accuracy of the model on the 17505 samples: 91.33390459868609 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [280/1000], Step [100/137], Loss: 0.1881\n",
      "Accuracy of the model on the 17505 samples: 91.91088260497001 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [281/1000], Step [100/137], Loss: 0.1888\n",
      "Accuracy of the model on the 17505 samples: 92.26506712367895 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [282/1000], Step [100/137], Loss: 0.1845\n",
      "Accuracy of the model on the 17505 samples: 92.35646958011996 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [283/1000], Step [100/137], Loss: 0.1705\n",
      "Accuracy of the model on the 17505 samples: 92.26506712367895 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [284/1000], Step [100/137], Loss: 0.1726\n",
      "Accuracy of the model on the 17505 samples: 92.4764353041988 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [285/1000], Step [100/137], Loss: 0.1660\n",
      "Accuracy of the model on the 17505 samples: 92.09368751785205 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [286/1000], Step [100/137], Loss: 0.1732\n",
      "Accuracy of the model on the 17505 samples: 92.56212510711225 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [287/1000], Step [100/137], Loss: 0.1919\n",
      "Accuracy of the model on the 17505 samples: 91.68237646386747 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [288/1000], Step [100/137], Loss: 0.1713\n",
      "Accuracy of the model on the 17505 samples: 92.21365324193087 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [289/1000], Step [100/137], Loss: 0.1864\n",
      "Accuracy of the model on the 17505 samples: 92.25935447015138 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [290/1000], Step [100/137], Loss: 0.1543\n",
      "Accuracy of the model on the 17505 samples: 92.1793773207655 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [291/1000], Step [100/137], Loss: 0.1702\n",
      "Accuracy of the model on the 17505 samples: 92.72207940588403 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [292/1000], Step [100/137], Loss: 0.1972\n",
      "Accuracy of the model on the 17505 samples: 91.96800914024564 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [293/1000], Step [100/137], Loss: 0.2118\n",
      "Accuracy of the model on the 17505 samples: 91.59097400742645 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [294/1000], Step [100/137], Loss: 0.1664\n",
      "Accuracy of the model on the 17505 samples: 92.64210225649815 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [295/1000], Step [100/137], Loss: 0.1672\n",
      "Accuracy of the model on the 17505 samples: 92.82490716938018 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 90.55698371893745 %\n",
      "Epoch [296/1000], Step [100/137], Loss: 0.1617\n",
      "Accuracy of the model on the 17505 samples: 92.73921736646672 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [297/1000], Step [100/137], Loss: 0.2598\n",
      "Accuracy of the model on the 17505 samples: 90.63696086832334 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Epoch [298/1000], Step [100/137], Loss: 0.1860\n",
      "Accuracy of the model on the 17505 samples: 91.68237646386747 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [299/1000], Step [100/137], Loss: 0.1719\n",
      "Accuracy of the model on the 17505 samples: 92.71065409882891 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [300/1000], Step [100/137], Loss: 0.1852\n",
      "Accuracy of the model on the 17505 samples: 92.72207940588403 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 90.33419023136247 %\n",
      "Epoch [301/1000], Step [100/137], Loss: 0.1554\n",
      "Accuracy of the model on the 17505 samples: 92.39074550128535 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [302/1000], Step [100/137], Loss: 0.1716\n",
      "Accuracy of the model on the 17505 samples: 92.9791488146244 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [303/1000], Step [100/137], Loss: 0.1887\n",
      "Accuracy of the model on the 17505 samples: 92.59068837475007 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [304/1000], Step [100/137], Loss: 0.1830\n",
      "Accuracy of the model on the 17505 samples: 92.29934304484433 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [305/1000], Step [100/137], Loss: 0.1643\n",
      "Accuracy of the model on the 17505 samples: 92.65924021708084 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [306/1000], Step [100/137], Loss: 0.1746\n",
      "Accuracy of the model on the 17505 samples: 91.88803199085976 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [307/1000], Step [100/137], Loss: 0.1775\n",
      "Accuracy of the model on the 17505 samples: 92.62496429591545 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [308/1000], Step [100/137], Loss: 0.1541\n",
      "Accuracy of the model on the 17505 samples: 92.88774635818338 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 90.33419023136247 %\n",
      "Epoch [309/1000], Step [100/137], Loss: 0.1589\n",
      "Accuracy of the model on the 17505 samples: 92.44215938303341 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [310/1000], Step [100/137], Loss: 0.1642\n",
      "Accuracy of the model on the 17505 samples: 93.09340188517567 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 90.60839760068552 %\n",
      "Epoch [311/1000], Step [100/137], Loss: 0.1782\n",
      "Accuracy of the model on the 17505 samples: 92.69922879177378 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [312/1000], Step [100/137], Loss: 0.2180\n",
      "Accuracy of the model on the 17505 samples: 92.88774635818338 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [313/1000], Step [100/137], Loss: 0.1758\n",
      "Accuracy of the model on the 17505 samples: 92.75064267352185 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [314/1000], Step [100/137], Loss: 0.1807\n",
      "Accuracy of the model on the 17505 samples: 92.84775778349044 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [315/1000], Step [100/137], Loss: 0.1535\n",
      "Accuracy of the model on the 17505 samples: 93.24764353041988 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [316/1000], Step [100/137], Loss: 0.1474\n",
      "Accuracy of the model on the 17505 samples: 92.9163096258212 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [317/1000], Step [100/137], Loss: 0.1685\n",
      "Accuracy of the model on the 17505 samples: 93.06483861753784 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 90.55698371893745 %\n",
      "Epoch [318/1000], Step [100/137], Loss: 0.1816\n",
      "Accuracy of the model on the 17505 samples: 92.88203370465581 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [319/1000], Step [100/137], Loss: 0.1741\n",
      "Accuracy of the model on the 17505 samples: 92.96201085404171 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [320/1000], Step [100/137], Loss: 0.1540\n",
      "Accuracy of the model on the 17505 samples: 93.59040274207369 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [321/1000], Step [100/137], Loss: 0.1752\n",
      "Accuracy of the model on the 17505 samples: 92.92773493287632 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Accuracy of the model on the 5835 samples: 90.33419023136247 %\n",
      "Epoch [322/1000], Step [100/137], Loss: 0.1808\n",
      "Accuracy of the model on the 17505 samples: 92.90488431876607 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Epoch [323/1000], Step [100/137], Loss: 0.1771\n",
      "Accuracy of the model on the 17505 samples: 93.28191945158527 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [324/1000], Step [100/137], Loss: 0.1722\n",
      "Accuracy of the model on the 17505 samples: 93.0876892316481 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [325/1000], Step [100/137], Loss: 0.2461\n",
      "Accuracy of the model on the 17505 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 88.60325621251071 %\n",
      "Accuracy of the model on the 5835 samples: 88.808911739503 %\n",
      "Epoch [326/1000], Step [100/137], Loss: 0.2471\n",
      "Accuracy of the model on the 17505 samples: 89.92859183090546 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Epoch [327/1000], Step [100/137], Loss: 0.2397\n",
      "Accuracy of the model on the 17505 samples: 90.60268494715795 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [328/1000], Step [100/137], Loss: 0.2330\n",
      "Accuracy of the model on the 17505 samples: 90.78548986003999 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [329/1000], Step [100/137], Loss: 0.1978\n",
      "Accuracy of the model on the 17505 samples: 91.76235361325335 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [330/1000], Step [100/137], Loss: 0.2011\n",
      "Accuracy of the model on the 17505 samples: 92.52784918594688 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [331/1000], Step [100/137], Loss: 0.1640\n",
      "Accuracy of the model on the 17505 samples: 92.60782633533276 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [332/1000], Step [100/137], Loss: 0.1613\n",
      "Accuracy of the model on the 17505 samples: 93.31048271922307 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 90.79691516709512 %\n",
      "Epoch [333/1000], Step [100/137], Loss: 0.1699\n",
      "Accuracy of the model on the 17505 samples: 93.01342473578977 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 90.41988003427592 %\n",
      "Epoch [334/1000], Step [100/137], Loss: 0.1628\n",
      "Accuracy of the model on the 17505 samples: 92.81348186232505 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [335/1000], Step [100/137], Loss: 0.1484\n",
      "Accuracy of the model on the 17505 samples: 92.8363324764353 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [336/1000], Step [100/137], Loss: 0.1576\n",
      "Accuracy of the model on the 17505 samples: 92.45929734361611 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [337/1000], Step [100/137], Loss: 0.1620\n",
      "Accuracy of the model on the 17505 samples: 93.13910311339617 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [338/1000], Step [100/137], Loss: 0.1735\n",
      "Accuracy of the model on the 17505 samples: 92.96201085404171 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Accuracy of the model on the 5835 samples: 90.6426735218509 %\n",
      "Epoch [339/1000], Step [100/137], Loss: 0.1831\n",
      "Accuracy of the model on the 17505 samples: 92.07654955726935 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [340/1000], Step [100/137], Loss: 0.1717\n",
      "Accuracy of the model on the 17505 samples: 93.04770065695516 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [341/1000], Step [100/137], Loss: 0.1656\n",
      "Accuracy of the model on the 17505 samples: 92.29363039131677 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Epoch [342/1000], Step [100/137], Loss: 0.1560\n",
      "Accuracy of the model on the 17505 samples: 92.30505569837189 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [343/1000], Step [100/137], Loss: 0.1503\n",
      "Accuracy of the model on the 17505 samples: 93.12767780634104 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 90.59125964010283 %\n",
      "Epoch [344/1000], Step [100/137], Loss: 0.1503\n",
      "Accuracy of the model on the 17505 samples: 92.8991716652385 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 90.57412167952013 %\n",
      "Epoch [345/1000], Step [100/137], Loss: 0.1533\n",
      "Accuracy of the model on the 17505 samples: 93.10482719223079 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 90.55698371893745 %\n",
      "Epoch [346/1000], Step [100/137], Loss: 0.2468\n",
      "Accuracy of the model on the 17505 samples: 91.85375606969437 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [347/1000], Step [100/137], Loss: 0.1693\n",
      "Accuracy of the model on the 17505 samples: 93.28763210511282 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Accuracy of the model on the 5835 samples: 90.4370179948586 %\n",
      "Epoch [348/1000], Step [100/137], Loss: 0.1555\n",
      "Accuracy of the model on the 17505 samples: 92.60782633533276 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [349/1000], Step [100/137], Loss: 0.1844\n",
      "Accuracy of the model on the 17505 samples: 92.60211368180519 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 90.40274207369323 %\n",
      "Epoch [350/1000], Step [100/137], Loss: 0.1827\n",
      "Accuracy of the model on the 17505 samples: 92.85347043701799 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/1000], Step [100/137], Loss: 0.1700\n",
      "Accuracy of the model on the 17505 samples: 91.43673236218223 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [352/1000], Step [100/137], Loss: 0.1736\n",
      "Accuracy of the model on the 17505 samples: 93.25906883747501 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [353/1000], Step [100/137], Loss: 0.1541\n",
      "Accuracy of the model on the 17505 samples: 93.47614967152242 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 90.67694944301628 %\n",
      "Epoch [354/1000], Step [100/137], Loss: 0.1637\n",
      "Accuracy of the model on the 17505 samples: 93.11053984575835 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [355/1000], Step [100/137], Loss: 0.1546\n",
      "Accuracy of the model on the 17505 samples: 93.21908026278206 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [356/1000], Step [100/137], Loss: 0.1946\n",
      "Accuracy of the model on the 17505 samples: 92.78491859468723 %\n",
      "Accuracy of the model on the 5835 samples: 90.40274207369323 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [357/1000], Step [100/137], Loss: 0.1762\n",
      "Accuracy of the model on the 17505 samples: 93.14481576692374 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [358/1000], Step [100/137], Loss: 0.2424\n",
      "Accuracy of the model on the 17505 samples: 90.6883747500714 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [359/1000], Step [100/137], Loss: 0.1782\n",
      "Accuracy of the model on the 17505 samples: 92.20794058840332 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [360/1000], Step [100/137], Loss: 0.2124\n",
      "Accuracy of the model on the 17505 samples: 92.47072265067123 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [361/1000], Step [100/137], Loss: 0.1641\n",
      "Accuracy of the model on the 17505 samples: 92.9163096258212 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [362/1000], Step [100/137], Loss: 0.1528\n",
      "Accuracy of the model on the 17505 samples: 93.13910311339617 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [363/1000], Step [100/137], Loss: 0.1558\n",
      "Accuracy of the model on the 17505 samples: 92.99628677520708 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [364/1000], Step [100/137], Loss: 0.1641\n",
      "Accuracy of the model on the 17505 samples: 93.30477006569552 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [365/1000], Step [100/137], Loss: 0.1480\n",
      "Accuracy of the model on the 17505 samples: 93.39045986860897 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [366/1000], Step [100/137], Loss: 0.1873\n",
      "Accuracy of the model on the 17505 samples: 92.72207940588403 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [367/1000], Step [100/137], Loss: 0.1484\n",
      "Accuracy of the model on the 17505 samples: 93.47043701799485 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [368/1000], Step [100/137], Loss: 0.1671\n",
      "Accuracy of the model on the 17505 samples: 93.29905741216795 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [369/1000], Step [100/137], Loss: 0.1483\n",
      "Accuracy of the model on the 17505 samples: 93.30477006569552 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [370/1000], Step [100/137], Loss: 0.1524\n",
      "Accuracy of the model on the 17505 samples: 93.42473578977435 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Epoch [371/1000], Step [100/137], Loss: 0.1603\n",
      "Accuracy of the model on the 17505 samples: 93.3847472150814 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [372/1000], Step [100/137], Loss: 0.1477\n",
      "Accuracy of the model on the 17505 samples: 93.51042559268781 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [373/1000], Step [100/137], Loss: 0.1823\n",
      "Accuracy of the model on the 17505 samples: 92.87632105112824 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [374/1000], Step [100/137], Loss: 0.1551\n",
      "Accuracy of the model on the 17505 samples: 92.4764353041988 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [375/1000], Step [100/137], Loss: 0.1755\n",
      "Accuracy of the model on the 17505 samples: 93.26478149100257 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [376/1000], Step [100/137], Loss: 0.1568\n",
      "Accuracy of the model on the 17505 samples: 93.4647243644673 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [377/1000], Step [100/137], Loss: 0.1560\n",
      "Accuracy of the model on the 17505 samples: 93.33333333333333 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [378/1000], Step [100/137], Loss: 0.2277\n",
      "Accuracy of the model on the 17505 samples: 92.89345901171095 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [379/1000], Step [100/137], Loss: 0.1999\n",
      "Accuracy of the model on the 17505 samples: 92.93916023993145 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [380/1000], Step [100/137], Loss: 0.1556\n",
      "Accuracy of the model on the 17505 samples: 93.73893173379035 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 90.29991431019708 %\n",
      "Epoch [381/1000], Step [100/137], Loss: 0.1458\n",
      "Accuracy of the model on the 17505 samples: 93.78463296201086 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [382/1000], Step [100/137], Loss: 0.1396\n",
      "Accuracy of the model on the 17505 samples: 93.67037989145959 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [383/1000], Step [100/137], Loss: 0.3487\n",
      "Accuracy of the model on the 17505 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n",
      "Accuracy of the model on the 5835 samples: 88.14053127677806 %\n",
      "Epoch [384/1000], Step [100/137], Loss: 0.2753\n",
      "Accuracy of the model on the 17505 samples: 89.79148814624394 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 88.70608397600685 %\n",
      "Epoch [385/1000], Step [100/137], Loss: 0.2704\n",
      "Accuracy of the model on the 17505 samples: 90.55127106540988 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [386/1000], Step [100/137], Loss: 0.2231\n",
      "Accuracy of the model on the 17505 samples: 92.32219365895459 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [387/1000], Step [100/137], Loss: 0.2329\n",
      "Accuracy of the model on the 17505 samples: 92.17366466723793 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [388/1000], Step [100/137], Loss: 0.2225\n",
      "Accuracy of the model on the 17505 samples: 91.66523850328477 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [389/1000], Step [100/137], Loss: 0.1913\n",
      "Accuracy of the model on the 17505 samples: 92.98486146815196 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [390/1000], Step [100/137], Loss: 0.1635\n",
      "Accuracy of the model on the 17505 samples: 93.37332190802628 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [391/1000], Step [100/137], Loss: 0.1615\n",
      "Accuracy of the model on the 17505 samples: 93.32762067980578 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [392/1000], Step [100/137], Loss: 0.1530\n",
      "Accuracy of the model on the 17505 samples: 93.61325335618395 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [393/1000], Step [100/137], Loss: 0.1428\n",
      "Accuracy of the model on the 17505 samples: 93.6361039702942 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [394/1000], Step [100/137], Loss: 0.1525\n",
      "Accuracy of the model on the 17505 samples: 93.56755212796344 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [395/1000], Step [100/137], Loss: 0.1517\n",
      "Accuracy of the model on the 17505 samples: 93.23621822336476 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [396/1000], Step [100/137], Loss: 0.1554\n",
      "Accuracy of the model on the 17505 samples: 93.51042559268781 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [397/1000], Step [100/137], Loss: 0.1472\n",
      "Accuracy of the model on the 17505 samples: 93.5275635532705 %\n",
      "Accuracy of the model on the 5835 samples: 90.33419023136247 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [398/1000], Step [100/137], Loss: 0.1545\n",
      "Accuracy of the model on the 17505 samples: 94.03598971722366 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 90.4370179948586 %\n",
      "Epoch [399/1000], Step [100/137], Loss: 0.1610\n",
      "Accuracy of the model on the 17505 samples: 93.70465581262496 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [400/1000], Step [100/137], Loss: 0.1529\n",
      "Accuracy of the model on the 17505 samples: 93.62467866323908 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [401/1000], Step [100/137], Loss: 0.1614\n",
      "Accuracy of the model on the 17505 samples: 93.18480434161668 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [402/1000], Step [100/137], Loss: 0.1468\n",
      "Accuracy of the model on the 17505 samples: 93.58469008854613 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [403/1000], Step [100/137], Loss: 0.2325\n",
      "Accuracy of the model on the 17505 samples: 90.80834047415024 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [404/1000], Step [100/137], Loss: 0.1896\n",
      "Accuracy of the model on the 17505 samples: 92.31648100542702 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [405/1000], Step [100/137], Loss: 0.1416\n",
      "Accuracy of the model on the 17505 samples: 93.20194230219937 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [406/1000], Step [100/137], Loss: 0.1953\n",
      "Accuracy of the model on the 17505 samples: 93.1505284204513 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [407/1000], Step [100/137], Loss: 0.1624\n",
      "Accuracy of the model on the 17505 samples: 93.6361039702942 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Epoch [408/1000], Step [100/137], Loss: 0.1581\n",
      "Accuracy of the model on the 17505 samples: 93.31619537275064 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [409/1000], Step [100/137], Loss: 0.1606\n",
      "Accuracy of the model on the 17505 samples: 93.34475864038846 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Epoch [410/1000], Step [100/137], Loss: 0.1719\n",
      "Accuracy of the model on the 17505 samples: 93.31619537275064 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Epoch [411/1000], Step [100/137], Loss: 0.3049\n",
      "Accuracy of the model on the 17505 samples: 88.85461296772351 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Accuracy of the model on the 5835 samples: 88.24335904027421 %\n",
      "Epoch [412/1000], Step [100/137], Loss: 0.2693\n",
      "Accuracy of the model on the 17505 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [413/1000], Step [100/137], Loss: 0.1866\n",
      "Accuracy of the model on the 17505 samples: 91.53956012567838 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [414/1000], Step [100/137], Loss: 0.1744\n",
      "Accuracy of the model on the 17505 samples: 92.37932019423022 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [415/1000], Step [100/137], Loss: 0.1660\n",
      "Accuracy of the model on the 17505 samples: 93.01342473578977 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Epoch [416/1000], Step [100/137], Loss: 0.1604\n",
      "Accuracy of the model on the 17505 samples: 93.36189660097115 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [417/1000], Step [100/137], Loss: 0.1446\n",
      "Accuracy of the model on the 17505 samples: 93.28763210511282 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [418/1000], Step [100/137], Loss: 0.2261\n",
      "Accuracy of the model on the 17505 samples: 92.6192516423879 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [419/1000], Step [100/137], Loss: 0.1898\n",
      "Accuracy of the model on the 17505 samples: 92.9791488146244 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [420/1000], Step [100/137], Loss: 0.1792\n",
      "Accuracy of the model on the 17505 samples: 92.99057412167951 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 90.36846615252784 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [421/1000], Step [100/137], Loss: 0.1750\n",
      "Accuracy of the model on the 17505 samples: 92.37932019423022 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [422/1000], Step [100/137], Loss: 0.1872\n",
      "Accuracy of the model on the 17505 samples: 93.25335618394745 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [423/1000], Step [100/137], Loss: 0.1766\n",
      "Accuracy of the model on the 17505 samples: 93.83604684375892 %\n",
      "Accuracy of the model on the 5835 samples: 90.40274207369323 %\n",
      "Accuracy of the model on the 5835 samples: 90.36846615252784 %\n",
      "Epoch [424/1000], Step [100/137], Loss: 0.1651\n",
      "Accuracy of the model on the 17505 samples: 93.4475864038846 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [425/1000], Step [100/137], Loss: 0.2064\n",
      "Accuracy of the model on the 17505 samples: 92.83061982290774 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [426/1000], Step [100/137], Loss: 0.1469\n",
      "Accuracy of the model on the 17505 samples: 93.59040274207369 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [427/1000], Step [100/137], Loss: 0.1520\n",
      "Accuracy of the model on the 17505 samples: 93.62467866323908 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [428/1000], Step [100/137], Loss: 0.1418\n",
      "Accuracy of the model on the 17505 samples: 93.80748357612111 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Epoch [429/1000], Step [100/137], Loss: 0.1399\n",
      "Accuracy of the model on the 17505 samples: 93.36189660097115 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [430/1000], Step [100/137], Loss: 0.1644\n",
      "Accuracy of the model on the 17505 samples: 92.95058554698657 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [431/1000], Step [100/137], Loss: 0.1477\n",
      "Accuracy of the model on the 17505 samples: 92.86489574407312 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [432/1000], Step [100/137], Loss: 0.1507\n",
      "Accuracy of the model on the 17505 samples: 93.64752927734934 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.48843187660668 %\n",
      "Epoch [433/1000], Step [100/137], Loss: 0.1815\n",
      "Accuracy of the model on the 17505 samples: 92.71636675235646 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [434/1000], Step [100/137], Loss: 0.1883\n",
      "Accuracy of the model on the 17505 samples: 93.71036846615253 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Accuracy of the model on the 5835 samples: 90.38560411311055 %\n",
      "Epoch [435/1000], Step [100/137], Loss: 0.1447\n",
      "Accuracy of the model on the 17505 samples: 93.87603541845188 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Epoch [436/1000], Step [100/137], Loss: 0.1514\n",
      "Accuracy of the model on the 17505 samples: 93.44187375035705 %\n",
      "Accuracy of the model on the 5835 samples: 90.24850042844902 %\n",
      "Accuracy of the model on the 5835 samples: 90.31705227077978 %\n",
      "Epoch [437/1000], Step [100/137], Loss: 0.1433\n",
      "Accuracy of the model on the 17505 samples: 93.573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [438/1000], Step [100/137], Loss: 0.1603\n",
      "Accuracy of the model on the 17505 samples: 93.43616109682948 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.52270779777207 %\n",
      "Epoch [439/1000], Step [100/137], Loss: 0.1711\n",
      "Accuracy of the model on the 17505 samples: 93.12196515281349 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [440/1000], Step [100/137], Loss: 0.1480\n",
      "Accuracy of the model on the 17505 samples: 93.3390459868609 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Epoch [441/1000], Step [100/137], Loss: 0.1424\n",
      "Accuracy of the model on the 17505 samples: 93.67037989145959 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Epoch [442/1000], Step [100/137], Loss: 0.1694\n",
      "Accuracy of the model on the 17505 samples: 92.96772350756926 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [443/1000], Step [100/137], Loss: 0.3626\n",
      "Accuracy of the model on the 17505 samples: 88.0091402456441 %\n",
      "Accuracy of the model on the 5835 samples: 87.57497857754927 %\n",
      "Accuracy of the model on the 5835 samples: 87.3350471293916 %\n",
      "Epoch [444/1000], Step [100/137], Loss: 0.3056\n",
      "Accuracy of the model on the 17505 samples: 88.40902599257355 %\n",
      "Accuracy of the model on the 5835 samples: 87.66066838046272 %\n",
      "Accuracy of the model on the 5835 samples: 87.47215081405312 %\n",
      "Epoch [445/1000], Step [100/137], Loss: 0.2965\n",
      "Accuracy of the model on the 17505 samples: 88.7632105112825 %\n",
      "Accuracy of the model on the 5835 samples: 88.15766923736075 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Epoch [446/1000], Step [100/137], Loss: 0.2826\n",
      "Accuracy of the model on the 17505 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 88.38046272493574 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Epoch [447/1000], Step [100/137], Loss: 0.2760\n",
      "Accuracy of the model on the 17505 samples: 88.95172807769208 %\n",
      "Accuracy of the model on the 5835 samples: 88.39760068551843 %\n",
      "Accuracy of the model on the 5835 samples: 88.0719794344473 %\n",
      "Epoch [448/1000], Step [100/137], Loss: 0.2678\n",
      "Accuracy of the model on the 17505 samples: 89.21451013996001 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.19194515852614 %\n",
      "Epoch [449/1000], Step [100/137], Loss: 0.2644\n",
      "Accuracy of the model on the 17505 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 88.05484147386461 %\n",
      "Accuracy of the model on the 5835 samples: 87.9005998286204 %\n",
      "Epoch [450/1000], Step [100/137], Loss: 0.2644\n",
      "Accuracy of the model on the 17505 samples: 89.11168237646386 %\n",
      "Accuracy of the model on the 5835 samples: 88.22622107969151 %\n",
      "Accuracy of the model on the 5835 samples: 88.29477292202228 %\n",
      "Epoch [451/1000], Step [100/137], Loss: 0.2637\n",
      "Accuracy of the model on the 17505 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 88.46615252784919 %\n",
      "Accuracy of the model on the 5835 samples: 88.51756640959726 %\n",
      "Epoch [452/1000], Step [100/137], Loss: 0.2832\n",
      "Accuracy of the model on the 17505 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 88.22622107969151 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n",
      "Epoch [453/1000], Step [100/137], Loss: 0.2610\n",
      "Accuracy of the model on the 17505 samples: 89.84861468151956 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.39760068551843 %\n",
      "Epoch [454/1000], Step [100/137], Loss: 0.2811\n",
      "Accuracy of the model on the 17505 samples: 88.58611825192801 %\n",
      "Accuracy of the model on the 5835 samples: 87.40359897172236 %\n",
      "Accuracy of the model on the 5835 samples: 88.10625535561269 %\n",
      "Epoch [455/1000], Step [100/137], Loss: 0.2523\n",
      "Accuracy of the model on the 17505 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 88.46615252784919 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [456/1000], Step [100/137], Loss: 0.2511\n",
      "Accuracy of the model on the 17505 samples: 89.79720079977149 %\n",
      "Accuracy of the model on the 5835 samples: 88.4490145672665 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Epoch [457/1000], Step [100/137], Loss: 0.2332\n",
      "Accuracy of the model on the 17505 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 88.60325621251071 %\n",
      "Epoch [458/1000], Step [100/137], Loss: 0.2222\n",
      "Accuracy of the model on the 17505 samples: 90.15138531848044 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [459/1000], Step [100/137], Loss: 0.2205\n",
      "Accuracy of the model on the 17505 samples: 90.07140816909454 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [460/1000], Step [100/137], Loss: 0.2286\n",
      "Accuracy of the model on the 17505 samples: 90.24850042844902 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [461/1000], Step [100/137], Loss: 0.2393\n",
      "Accuracy of the model on the 17505 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 88.70608397600685 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Epoch [462/1000], Step [100/137], Loss: 0.2245\n",
      "Accuracy of the model on the 17505 samples: 90.0314195944016 %\n",
      "Accuracy of the model on the 5835 samples: 88.27763496143959 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [463/1000], Step [100/137], Loss: 0.2240\n",
      "Accuracy of the model on the 17505 samples: 90.37417880605541 %\n",
      "Accuracy of the model on the 5835 samples: 89.04884318766067 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Epoch [464/1000], Step [100/137], Loss: 0.2501\n",
      "Accuracy of the model on the 17505 samples: 90.17423593259069 %\n",
      "Accuracy of the model on the 5835 samples: 88.50042844901456 %\n",
      "Accuracy of the model on the 5835 samples: 88.60325621251071 %\n",
      "Epoch [465/1000], Step [100/137], Loss: 0.2280\n",
      "Accuracy of the model on the 17505 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 88.34618680377035 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n",
      "Epoch [466/1000], Step [100/137], Loss: 0.2259\n",
      "Accuracy of the model on the 17505 samples: 90.26563838903171 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Epoch [467/1000], Step [100/137], Loss: 0.2147\n",
      "Accuracy of the model on the 17505 samples: 90.61982290774064 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [468/1000], Step [100/137], Loss: 0.2231\n",
      "Accuracy of the model on the 17505 samples: 90.48271922307912 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [469/1000], Step [100/137], Loss: 0.2225\n",
      "Accuracy of the model on the 17505 samples: 90.61982290774064 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Epoch [470/1000], Step [100/137], Loss: 0.2208\n",
      "Accuracy of the model on the 17505 samples: 90.59125964010283 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [471/1000], Step [100/137], Loss: 0.2017\n",
      "Accuracy of the model on the 17505 samples: 90.36275349900029 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n",
      "Accuracy of the model on the 5835 samples: 88.41473864610111 %\n",
      "Epoch [472/1000], Step [100/137], Loss: 0.2421\n",
      "Accuracy of the model on the 17505 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Epoch [473/1000], Step [100/137], Loss: 0.2111\n",
      "Accuracy of the model on the 17505 samples: 90.55127106540988 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [474/1000], Step [100/137], Loss: 0.2287\n",
      "Accuracy of the model on the 17505 samples: 90.59697229363039 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Epoch [475/1000], Step [100/137], Loss: 0.2055\n",
      "Accuracy of the model on the 17505 samples: 91.03684661525278 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [476/1000], Step [100/137], Loss: 0.2169\n",
      "Accuracy of the model on the 17505 samples: 90.84832904884318 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [477/1000], Step [100/137], Loss: 0.2005\n",
      "Accuracy of the model on the 17505 samples: 91.03684661525278 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Epoch [478/1000], Step [100/137], Loss: 0.2002\n",
      "Accuracy of the model on the 17505 samples: 90.76263924592973 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [479/1000], Step [100/137], Loss: 0.2022\n",
      "Accuracy of the model on the 17505 samples: 90.88831762353614 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [480/1000], Step [100/137], Loss: 0.2047\n",
      "Accuracy of the model on the 17505 samples: 90.77977720651242 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Epoch [481/1000], Step [100/137], Loss: 0.2172\n",
      "Accuracy of the model on the 17505 samples: 90.94544415881177 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [482/1000], Step [100/137], Loss: 0.2084\n",
      "Accuracy of the model on the 17505 samples: 90.94544415881177 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [483/1000], Step [100/137], Loss: 0.2252\n",
      "Accuracy of the model on the 17505 samples: 90.9397315052842 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [484/1000], Step [100/137], Loss: 0.2386\n",
      "Accuracy of the model on the 17505 samples: 90.09997143673236 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [485/1000], Step [100/137], Loss: 0.2265\n",
      "Accuracy of the model on the 17505 samples: 90.78548986003999 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [486/1000], Step [100/137], Loss: 0.2141\n",
      "Accuracy of the model on the 17505 samples: 91.17966295344188 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [487/1000], Step [100/137], Loss: 0.2108\n",
      "Accuracy of the model on the 17505 samples: 90.97400742644959 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Epoch [488/1000], Step [100/137], Loss: 0.2240\n",
      "Accuracy of the model on the 17505 samples: 91.19680091402456 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [489/1000], Step [100/137], Loss: 0.2079\n",
      "Accuracy of the model on the 17505 samples: 91.02542130819766 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [490/1000], Step [100/137], Loss: 0.2095\n",
      "Accuracy of the model on the 17505 samples: 90.80262782062267 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [491/1000], Step [100/137], Loss: 0.2058\n",
      "Accuracy of the model on the 17505 samples: 90.77977720651242 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [492/1000], Step [100/137], Loss: 0.2166\n",
      "Accuracy of the model on the 17505 samples: 90.67123678948872 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [493/1000], Step [100/137], Loss: 0.2208\n",
      "Accuracy of the model on the 17505 samples: 91.18537560696944 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [494/1000], Step [100/137], Loss: 0.2335\n",
      "Accuracy of the model on the 17505 samples: 91.16252499285919 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [495/1000], Step [100/137], Loss: 0.2202\n",
      "Accuracy of the model on the 17505 samples: 90.97972007997716 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Epoch [496/1000], Step [100/137], Loss: 0.2168\n",
      "Accuracy of the model on the 17505 samples: 91.13396172522137 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [497/1000], Step [100/137], Loss: 0.1985\n",
      "Accuracy of the model on the 17505 samples: 91.32247929163097 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [498/1000], Step [100/137], Loss: 0.2039\n",
      "Accuracy of the model on the 17505 samples: 90.94544415881177 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [499/1000], Step [100/137], Loss: 0.1921\n",
      "Accuracy of the model on the 17505 samples: 90.85404170237075 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [500/1000], Step [100/137], Loss: 0.2323\n",
      "Accuracy of the model on the 17505 samples: 90.75121393887461 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [501/1000], Step [100/137], Loss: 0.2381\n",
      "Accuracy of the model on the 17505 samples: 91.09397315052843 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [502/1000], Step [100/137], Loss: 0.2055\n",
      "Accuracy of the model on the 17505 samples: 91.09397315052843 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [503/1000], Step [100/137], Loss: 0.2408\n",
      "Accuracy of the model on the 17505 samples: 90.31133961725222 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [504/1000], Step [100/137], Loss: 0.1903\n",
      "Accuracy of the model on the 17505 samples: 91.07112253641817 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [505/1000], Step [100/137], Loss: 0.1936\n",
      "Accuracy of the model on the 17505 samples: 90.94544415881177 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [506/1000], Step [100/137], Loss: 0.1927\n",
      "Accuracy of the model on the 17505 samples: 91.2082262210797 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [507/1000], Step [100/137], Loss: 0.2086\n",
      "Accuracy of the model on the 17505 samples: 90.97400742644959 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [508/1000], Step [100/137], Loss: 0.2124\n",
      "Accuracy of the model on the 17505 samples: 90.77977720651242 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [509/1000], Step [100/137], Loss: 0.2098\n",
      "Accuracy of the model on the 17505 samples: 90.97972007997716 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [510/1000], Step [100/137], Loss: 0.1855\n",
      "Accuracy of the model on the 17505 samples: 91.44815766923736 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [511/1000], Step [100/137], Loss: 0.1896\n",
      "Accuracy of the model on the 17505 samples: 90.98543273350471 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [512/1000], Step [100/137], Loss: 0.2066\n",
      "Accuracy of the model on the 17505 samples: 90.72836332476436 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [513/1000], Step [100/137], Loss: 0.1838\n",
      "Accuracy of the model on the 17505 samples: 91.71093973150528 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [514/1000], Step [100/137], Loss: 0.2159\n",
      "Accuracy of the model on the 17505 samples: 90.97400742644959 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [515/1000], Step [100/137], Loss: 0.2170\n",
      "Accuracy of the model on the 17505 samples: 90.88260497000857 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [516/1000], Step [100/137], Loss: 0.2078\n",
      "Accuracy of the model on the 17505 samples: 90.89403027706369 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [517/1000], Step [100/137], Loss: 0.2114\n",
      "Accuracy of the model on the 17505 samples: 91.31105398457584 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Epoch [518/1000], Step [100/137], Loss: 0.2067\n",
      "Accuracy of the model on the 17505 samples: 91.0654098828906 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [519/1000], Step [100/137], Loss: 0.1988\n",
      "Accuracy of the model on the 17505 samples: 91.56241073978863 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [520/1000], Step [100/137], Loss: 0.1971\n",
      "Accuracy of the model on the 17505 samples: 90.96829477292202 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [521/1000], Step [100/137], Loss: 0.1934\n",
      "Accuracy of the model on the 17505 samples: 91.27677806341046 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [522/1000], Step [100/137], Loss: 0.1966\n",
      "Accuracy of the model on the 17505 samples: 91.39674378748929 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [523/1000], Step [100/137], Loss: 0.1732\n",
      "Accuracy of the model on the 17505 samples: 92.07083690374179 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [524/1000], Step [100/137], Loss: 0.1807\n",
      "Accuracy of the model on the 17505 samples: 91.88803199085976 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [525/1000], Step [100/137], Loss: 0.1757\n",
      "Accuracy of the model on the 17505 samples: 91.71665238503284 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [526/1000], Step [100/137], Loss: 0.1975\n",
      "Accuracy of the model on the 17505 samples: 91.89945729791488 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [527/1000], Step [100/137], Loss: 0.2173\n",
      "Accuracy of the model on the 17505 samples: 90.7683518994573 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Epoch [528/1000], Step [100/137], Loss: 0.1797\n",
      "Accuracy of the model on the 17505 samples: 90.99685804055984 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [529/1000], Step [100/137], Loss: 0.2063\n",
      "Accuracy of the model on the 17505 samples: 91.57954870037132 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [530/1000], Step [100/137], Loss: 0.1962\n",
      "Accuracy of the model on the 17505 samples: 91.65381319622965 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [531/1000], Step [100/137], Loss: 0.1903\n",
      "Accuracy of the model on the 17505 samples: 91.95087117966295 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [532/1000], Step [100/137], Loss: 0.1851\n",
      "Accuracy of the model on the 17505 samples: 91.61382462153671 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [533/1000], Step [100/137], Loss: 0.1868\n",
      "Accuracy of the model on the 17505 samples: 91.61953727506426 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [534/1000], Step [100/137], Loss: 0.1996\n",
      "Accuracy of the model on the 17505 samples: 91.69951442445016 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [535/1000], Step [100/137], Loss: 0.2004\n",
      "Accuracy of the model on the 17505 samples: 92.01942302199372 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [536/1000], Step [100/137], Loss: 0.1745\n",
      "Accuracy of the model on the 17505 samples: 92.20794058840332 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [537/1000], Step [100/137], Loss: 0.1909\n",
      "Accuracy of the model on the 17505 samples: 92.08797486432448 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [538/1000], Step [100/137], Loss: 0.2130\n",
      "Accuracy of the model on the 17505 samples: 91.76235361325335 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [539/1000], Step [100/137], Loss: 0.1756\n",
      "Accuracy of the model on the 17505 samples: 92.04798628963154 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [540/1000], Step [100/137], Loss: 0.1929\n",
      "Accuracy of the model on the 17505 samples: 91.85375606969437 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Epoch [541/1000], Step [100/137], Loss: 0.2027\n",
      "Accuracy of the model on the 17505 samples: 90.97972007997716 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [542/1000], Step [100/137], Loss: 0.2405\n",
      "Accuracy of the model on the 17505 samples: 90.99114538703228 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [543/1000], Step [100/137], Loss: 0.1902\n",
      "Accuracy of the model on the 17505 samples: 91.64810054270208 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [544/1000], Step [100/137], Loss: 0.2318\n",
      "Accuracy of the model on the 17505 samples: 91.65952584975722 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [545/1000], Step [100/137], Loss: 0.1985\n",
      "Accuracy of the model on the 17505 samples: 91.97943444730078 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [546/1000], Step [100/137], Loss: 0.2003\n",
      "Accuracy of the model on the 17505 samples: 92.1793773207655 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [547/1000], Step [100/137], Loss: 0.2166\n",
      "Accuracy of the model on the 17505 samples: 92.07654955726935 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [548/1000], Step [100/137], Loss: 0.2113\n",
      "Accuracy of the model on the 17505 samples: 91.79662953441874 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [549/1000], Step [100/137], Loss: 0.1847\n",
      "Accuracy of the model on the 17505 samples: 92.29934304484433 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [550/1000], Step [100/137], Loss: 0.1820\n",
      "Accuracy of the model on the 17505 samples: 92.59640102827764 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [551/1000], Step [100/137], Loss: 0.1856\n",
      "Accuracy of the model on the 17505 samples: 92.36218223364753 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [552/1000], Step [100/137], Loss: 0.1731\n",
      "Accuracy of the model on the 17505 samples: 92.45929734361611 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [553/1000], Step [100/137], Loss: 0.1700\n",
      "Accuracy of the model on the 17505 samples: 92.25364181662383 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [554/1000], Step [100/137], Loss: 0.1688\n",
      "Accuracy of the model on the 17505 samples: 92.2879177377892 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [555/1000], Step [100/137], Loss: 0.2132\n",
      "Accuracy of the model on the 17505 samples: 91.05398457583547 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [556/1000], Step [100/137], Loss: 0.2039\n",
      "Accuracy of the model on the 17505 samples: 92.28220508426163 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [557/1000], Step [100/137], Loss: 0.1820\n",
      "Accuracy of the model on the 17505 samples: 92.44215938303341 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [558/1000], Step [100/137], Loss: 0.1916\n",
      "Accuracy of the model on the 17505 samples: 92.12225078548985 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [559/1000], Step [100/137], Loss: 0.1825\n",
      "Accuracy of the model on the 17505 samples: 91.95658383319052 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [560/1000], Step [100/137], Loss: 0.1997\n",
      "Accuracy of the model on the 17505 samples: 91.32819194515852 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [561/1000], Step [100/137], Loss: 0.1865\n",
      "Accuracy of the model on the 17505 samples: 92.55641245358468 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [562/1000], Step [100/137], Loss: 0.1736\n",
      "Accuracy of the model on the 17505 samples: 92.18508997429306 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [563/1000], Step [100/137], Loss: 0.2497\n",
      "Accuracy of the model on the 17505 samples: 92.10511282490717 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [564/1000], Step [100/137], Loss: 0.2060\n",
      "Accuracy of the model on the 17505 samples: 92.15652670665524 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [565/1000], Step [100/137], Loss: 0.1616\n",
      "Accuracy of the model on the 17505 samples: 92.11082547843473 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [566/1000], Step [100/137], Loss: 0.1914\n",
      "Accuracy of the model on the 17505 samples: 92.08226221079691 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [567/1000], Step [100/137], Loss: 0.1607\n",
      "Accuracy of the model on the 17505 samples: 92.4764353041988 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [568/1000], Step [100/137], Loss: 0.1709\n",
      "Accuracy of the model on the 17505 samples: 92.52784918594688 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [569/1000], Step [100/137], Loss: 0.1811\n",
      "Accuracy of the model on the 17505 samples: 92.54498714652956 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Epoch [570/1000], Step [100/137], Loss: 0.2576\n",
      "Accuracy of the model on the 17505 samples: 89.77435018566123 %\n",
      "Accuracy of the model on the 5835 samples: 88.46615252784919 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Epoch [571/1000], Step [100/137], Loss: 0.2254\n",
      "Accuracy of the model on the 17505 samples: 91.01399600114253 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Epoch [572/1000], Step [100/137], Loss: 0.1875\n",
      "Accuracy of the model on the 17505 samples: 91.61953727506426 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [573/1000], Step [100/137], Loss: 0.2915\n",
      "Accuracy of the model on the 17505 samples: 89.39731505284205 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Accuracy of the model on the 5835 samples: 87.8834618680377 %\n",
      "Epoch [574/1000], Step [100/137], Loss: 0.2624\n",
      "Accuracy of the model on the 17505 samples: 91.2539274493002 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [575/1000], Step [100/137], Loss: 0.2169\n",
      "Accuracy of the model on the 17505 samples: 92.12796343901742 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [576/1000], Step [100/137], Loss: 0.2006\n",
      "Accuracy of the model on the 17505 samples: 91.45958297629248 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [577/1000], Step [100/137], Loss: 0.1929\n",
      "Accuracy of the model on the 17505 samples: 92.37932019423022 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [578/1000], Step [100/137], Loss: 0.1846\n",
      "Accuracy of the model on the 17505 samples: 92.56212510711225 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [579/1000], Step [100/137], Loss: 0.1780\n",
      "Accuracy of the model on the 17505 samples: 92.31076835189945 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [580/1000], Step [100/137], Loss: 0.1700\n",
      "Accuracy of the model on the 17505 samples: 92.82490716938018 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [581/1000], Step [100/137], Loss: 0.1919\n",
      "Accuracy of the model on the 17505 samples: 92.60211368180519 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [582/1000], Step [100/137], Loss: 0.1768\n",
      "Accuracy of the model on the 17505 samples: 92.6192516423879 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [583/1000], Step [100/137], Loss: 0.1837\n",
      "Accuracy of the model on the 17505 samples: 92.00228506141103 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [584/1000], Step [100/137], Loss: 0.1908\n",
      "Accuracy of the model on the 17505 samples: 92.5849757212225 %\n",
      "Accuracy of the model on the 5835 samples: 90.36846615252784 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [585/1000], Step [100/137], Loss: 0.1813\n",
      "Accuracy of the model on the 17505 samples: 92.86489574407312 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [586/1000], Step [100/137], Loss: 0.1667\n",
      "Accuracy of the model on the 17505 samples: 93.01342473578977 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [587/1000], Step [100/137], Loss: 0.1666\n",
      "Accuracy of the model on the 17505 samples: 92.55641245358468 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [588/1000], Step [100/137], Loss: 0.1762\n",
      "Accuracy of the model on the 17505 samples: 92.99057412167951 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [589/1000], Step [100/137], Loss: 0.1712\n",
      "Accuracy of the model on the 17505 samples: 92.79634390174236 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [590/1000], Step [100/137], Loss: 0.1594\n",
      "Accuracy of the model on the 17505 samples: 92.3507569265924 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [591/1000], Step [100/137], Loss: 0.2087\n",
      "Accuracy of the model on the 17505 samples: 92.02513567552128 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [592/1000], Step [100/137], Loss: 0.1996\n",
      "Accuracy of the model on the 17505 samples: 91.44815766923736 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Epoch [593/1000], Step [100/137], Loss: 0.1630\n",
      "Accuracy of the model on the 17505 samples: 92.67637817766352 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [594/1000], Step [100/137], Loss: 0.1841\n",
      "Accuracy of the model on the 17505 samples: 92.6478149100257 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [595/1000], Step [100/137], Loss: 0.2041\n",
      "Accuracy of the model on the 17505 samples: 92.60782633533276 %\n",
      "Accuracy of the model on the 5835 samples: 90.471293916024 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [596/1000], Step [100/137], Loss: 0.2789\n",
      "Accuracy of the model on the 17505 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.00342759211654 %\n",
      "Accuracy of the model on the 5835 samples: 87.86632390745501 %\n",
      "Epoch [597/1000], Step [100/137], Loss: 0.3140\n",
      "Accuracy of the model on the 17505 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 87.74635818337617 %\n",
      "Accuracy of the model on the 5835 samples: 88.05484147386461 %\n",
      "Epoch [598/1000], Step [100/137], Loss: 0.2474\n",
      "Accuracy of the model on the 17505 samples: 89.84861468151956 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Accuracy of the model on the 5835 samples: 88.6203941730934 %\n",
      "Epoch [599/1000], Step [100/137], Loss: 0.2208\n",
      "Accuracy of the model on the 17505 samples: 90.48843187660668 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Epoch [600/1000], Step [100/137], Loss: 0.2134\n",
      "Accuracy of the model on the 17505 samples: 91.28820337046558 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [601/1000], Step [100/137], Loss: 0.2135\n",
      "Accuracy of the model on the 17505 samples: 91.87660668380462 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [602/1000], Step [100/137], Loss: 0.1837\n",
      "Accuracy of the model on the 17505 samples: 92.44215938303341 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [603/1000], Step [100/137], Loss: 0.1797\n",
      "Accuracy of the model on the 17505 samples: 92.78491859468723 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [604/1000], Step [100/137], Loss: 0.1721\n",
      "Accuracy of the model on the 17505 samples: 92.24221650956869 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [605/1000], Step [100/137], Loss: 0.1640\n",
      "Accuracy of the model on the 17505 samples: 92.59640102827764 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [606/1000], Step [100/137], Loss: 0.1865\n",
      "Accuracy of the model on the 17505 samples: 92.6649528706084 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [607/1000], Step [100/137], Loss: 0.1633\n",
      "Accuracy of the model on the 17505 samples: 92.92202227934875 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [608/1000], Step [100/137], Loss: 0.1622\n",
      "Accuracy of the model on the 17505 samples: 93.00199942873465 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [609/1000], Step [100/137], Loss: 0.1597\n",
      "Accuracy of the model on the 17505 samples: 92.53356183947443 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [610/1000], Step [100/137], Loss: 0.1620\n",
      "Accuracy of the model on the 17505 samples: 92.40217080834047 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Epoch [611/1000], Step [100/137], Loss: 0.1618\n",
      "Accuracy of the model on the 17505 samples: 92.51071122536418 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Epoch [612/1000], Step [100/137], Loss: 0.1833\n",
      "Accuracy of the model on the 17505 samples: 92.24792916309626 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [613/1000], Step [100/137], Loss: 0.1531\n",
      "Accuracy of the model on the 17505 samples: 93.07626392459298 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [614/1000], Step [100/137], Loss: 0.1820\n",
      "Accuracy of the model on the 17505 samples: 92.57355041416739 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [615/1000], Step [100/137], Loss: 0.1871\n",
      "Accuracy of the model on the 17505 samples: 91.8651813767495 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [616/1000], Step [100/137], Loss: 0.1625\n",
      "Accuracy of the model on the 17505 samples: 92.73350471293917 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [617/1000], Step [100/137], Loss: 0.2225\n",
      "Accuracy of the model on the 17505 samples: 91.01399600114253 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n",
      "Epoch [618/1000], Step [100/137], Loss: 0.2654\n",
      "Accuracy of the model on the 17505 samples: 91.02542130819766 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [619/1000], Step [100/137], Loss: 0.2041\n",
      "Accuracy of the model on the 17505 samples: 92.37360754070265 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [620/1000], Step [100/137], Loss: 0.1802\n",
      "Accuracy of the model on the 17505 samples: 92.62496429591545 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [621/1000], Step [100/137], Loss: 0.1612\n",
      "Accuracy of the model on the 17505 samples: 92.73350471293917 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [622/1000], Step [100/137], Loss: 0.1942\n",
      "Accuracy of the model on the 17505 samples: 92.75635532704942 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [623/1000], Step [100/137], Loss: 0.2348\n",
      "Accuracy of the model on the 17505 samples: 92.30505569837189 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [624/1000], Step [100/137], Loss: 0.1545\n",
      "Accuracy of the model on the 17505 samples: 92.92202227934875 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [625/1000], Step [100/137], Loss: 0.1661\n",
      "Accuracy of the model on the 17505 samples: 92.86489574407312 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [626/1000], Step [100/137], Loss: 0.1738\n",
      "Accuracy of the model on the 17505 samples: 93.16195372750643 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [627/1000], Step [100/137], Loss: 0.1646\n",
      "Accuracy of the model on the 17505 samples: 93.11053984575835 %\n",
      "Accuracy of the model on the 5835 samples: 90.2827763496144 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [628/1000], Step [100/137], Loss: 0.2337\n",
      "Accuracy of the model on the 17505 samples: 91.64238788917451 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Epoch [629/1000], Step [100/137], Loss: 0.1767\n",
      "Accuracy of the model on the 17505 samples: 92.56783776063982 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [630/1000], Step [100/137], Loss: 0.1706\n",
      "Accuracy of the model on the 17505 samples: 92.944872893459 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [631/1000], Step [100/137], Loss: 0.1892\n",
      "Accuracy of the model on the 17505 samples: 92.6192516423879 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [632/1000], Step [100/137], Loss: 0.1817\n",
      "Accuracy of the model on the 17505 samples: 92.63067694944301 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [633/1000], Step [100/137], Loss: 0.1694\n",
      "Accuracy of the model on the 17505 samples: 92.76778063410454 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [634/1000], Step [100/137], Loss: 0.1564\n",
      "Accuracy of the model on the 17505 samples: 93.05912596401028 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 90.35132819194516 %\n",
      "Epoch [635/1000], Step [100/137], Loss: 0.1659\n",
      "Accuracy of the model on the 17505 samples: 92.3507569265924 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [636/1000], Step [100/137], Loss: 0.1598\n",
      "Accuracy of the model on the 17505 samples: 93.22479291630962 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [637/1000], Step [100/137], Loss: 0.1763\n",
      "Accuracy of the model on the 17505 samples: 93.03056269637247 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [638/1000], Step [100/137], Loss: 0.1766\n",
      "Accuracy of the model on the 17505 samples: 93.17909168808912 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [639/1000], Step [100/137], Loss: 0.1811\n",
      "Accuracy of the model on the 17505 samples: 92.6478149100257 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [640/1000], Step [100/137], Loss: 0.1896\n",
      "Accuracy of the model on the 17505 samples: 92.35646958011996 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [641/1000], Step [100/137], Loss: 0.1591\n",
      "Accuracy of the model on the 17505 samples: 93.11053984575835 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [642/1000], Step [100/137], Loss: 0.1535\n",
      "Accuracy of the model on the 17505 samples: 93.48186232504999 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [643/1000], Step [100/137], Loss: 0.1713\n",
      "Accuracy of the model on the 17505 samples: 93.25335618394745 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [644/1000], Step [100/137], Loss: 0.1593\n",
      "Accuracy of the model on the 17505 samples: 93.41331048271923 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [645/1000], Step [100/137], Loss: 0.1579\n",
      "Accuracy of the model on the 17505 samples: 93.1962296486718 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [646/1000], Step [100/137], Loss: 0.1857\n",
      "Accuracy of the model on the 17505 samples: 93.17337903456155 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [647/1000], Step [100/137], Loss: 0.1712\n",
      "Accuracy of the model on the 17505 samples: 93.23050556983719 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [648/1000], Step [100/137], Loss: 0.1564\n",
      "Accuracy of the model on the 17505 samples: 92.71636675235646 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [649/1000], Step [100/137], Loss: 0.1643\n",
      "Accuracy of the model on the 17505 samples: 92.80776920879748 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [650/1000], Step [100/137], Loss: 0.1566\n",
      "Accuracy of the model on the 17505 samples: 93.03627534990002 %\n",
      "Accuracy of the model on the 5835 samples: 90.29991431019708 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Epoch [651/1000], Step [100/137], Loss: 0.2089\n",
      "Accuracy of the model on the 17505 samples: 92.31648100542702 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Epoch [652/1000], Step [100/137], Loss: 0.1556\n",
      "Accuracy of the model on the 17505 samples: 93.33333333333333 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [653/1000], Step [100/137], Loss: 0.1599\n",
      "Accuracy of the model on the 17505 samples: 93.28763210511282 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Epoch [654/1000], Step [100/137], Loss: 0.1511\n",
      "Accuracy of the model on the 17505 samples: 93.13339045986861 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [655/1000], Step [100/137], Loss: 0.1752\n",
      "Accuracy of the model on the 17505 samples: 93.07055127106541 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [656/1000], Step [100/137], Loss: 0.1603\n",
      "Accuracy of the model on the 17505 samples: 93.17337903456155 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [657/1000], Step [100/137], Loss: 0.1853\n",
      "Accuracy of the model on the 17505 samples: 92.77920594115967 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [658/1000], Step [100/137], Loss: 0.1665\n",
      "Accuracy of the model on the 17505 samples: 93.55041416738075 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [659/1000], Step [100/137], Loss: 0.1606\n",
      "Accuracy of the model on the 17505 samples: 92.95629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [660/1000], Step [100/137], Loss: 0.1548\n",
      "Accuracy of the model on the 17505 samples: 92.95629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [661/1000], Step [100/137], Loss: 0.1812\n",
      "Accuracy of the model on the 17505 samples: 92.38503284775778 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [662/1000], Step [100/137], Loss: 0.1821\n",
      "Accuracy of the model on the 17505 samples: 92.63067694944301 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [663/1000], Step [100/137], Loss: 0.1965\n",
      "Accuracy of the model on the 17505 samples: 91.97372179377321 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [664/1000], Step [100/137], Loss: 0.1880\n",
      "Accuracy of the model on the 17505 samples: 93.01342473578977 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [665/1000], Step [100/137], Loss: 0.1533\n",
      "Accuracy of the model on the 17505 samples: 93.19051699514425 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [666/1000], Step [100/137], Loss: 0.1751\n",
      "Accuracy of the model on the 17505 samples: 93.23050556983719 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [667/1000], Step [100/137], Loss: 0.1841\n",
      "Accuracy of the model on the 17505 samples: 92.57355041416739 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [668/1000], Step [100/137], Loss: 0.1744\n",
      "Accuracy of the model on the 17505 samples: 93.01342473578977 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [669/1000], Step [100/137], Loss: 0.1682\n",
      "Accuracy of the model on the 17505 samples: 92.77920594115967 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [670/1000], Step [100/137], Loss: 0.1619\n",
      "Accuracy of the model on the 17505 samples: 92.81919451585262 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [671/1000], Step [100/137], Loss: 0.1988\n",
      "Accuracy of the model on the 17505 samples: 92.09368751785205 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Epoch [672/1000], Step [100/137], Loss: 0.1609\n",
      "Accuracy of the model on the 17505 samples: 93.00199942873465 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [673/1000], Step [100/137], Loss: 0.1640\n",
      "Accuracy of the model on the 17505 samples: 92.96772350756926 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [674/1000], Step [100/137], Loss: 0.1778\n",
      "Accuracy of the model on the 17505 samples: 92.73921736646672 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Epoch [675/1000], Step [100/137], Loss: 0.1611\n",
      "Accuracy of the model on the 17505 samples: 93.17337903456155 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [676/1000], Step [100/137], Loss: 0.1546\n",
      "Accuracy of the model on the 17505 samples: 93.21908026278206 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [677/1000], Step [100/137], Loss: 0.1737\n",
      "Accuracy of the model on the 17505 samples: 92.7734932876321 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [678/1000], Step [100/137], Loss: 0.2174\n",
      "Accuracy of the model on the 17505 samples: 92.42502142245073 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Epoch [679/1000], Step [100/137], Loss: 0.1598\n",
      "Accuracy of the model on the 17505 samples: 92.81919451585262 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [680/1000], Step [100/137], Loss: 0.2121\n",
      "Accuracy of the model on the 17505 samples: 92.95058554698657 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [681/1000], Step [100/137], Loss: 0.1624\n",
      "Accuracy of the model on the 17505 samples: 93.24764353041988 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [682/1000], Step [100/137], Loss: 0.1524\n",
      "Accuracy of the model on the 17505 samples: 93.25906883747501 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Epoch [683/1000], Step [100/137], Loss: 0.1719\n",
      "Accuracy of the model on the 17505 samples: 93.0248500428449 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [684/1000], Step [100/137], Loss: 0.1502\n",
      "Accuracy of the model on the 17505 samples: 93.40759782919166 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [685/1000], Step [100/137], Loss: 0.1580\n",
      "Accuracy of the model on the 17505 samples: 92.39074550128535 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [686/1000], Step [100/137], Loss: 0.1517\n",
      "Accuracy of the model on the 17505 samples: 93.24764353041988 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [687/1000], Step [100/137], Loss: 0.1491\n",
      "Accuracy of the model on the 17505 samples: 93.55612682090832 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [688/1000], Step [100/137], Loss: 0.1578\n",
      "Accuracy of the model on the 17505 samples: 93.04770065695516 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [689/1000], Step [100/137], Loss: 0.1607\n",
      "Accuracy of the model on the 17505 samples: 93.35618394744358 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [690/1000], Step [100/137], Loss: 0.2543\n",
      "Accuracy of the model on the 17505 samples: 91.13396172522137 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Epoch [691/1000], Step [100/137], Loss: 0.1591\n",
      "Accuracy of the model on the 17505 samples: 92.4307340759783 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [692/1000], Step [100/137], Loss: 0.1526\n",
      "Accuracy of the model on the 17505 samples: 92.9163096258212 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [693/1000], Step [100/137], Loss: 0.1520\n",
      "Accuracy of the model on the 17505 samples: 93.43044844330191 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Epoch [694/1000], Step [100/137], Loss: 0.1716\n",
      "Accuracy of the model on the 17505 samples: 93.48757497857756 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Epoch [695/1000], Step [100/137], Loss: 0.1835\n",
      "Accuracy of the model on the 17505 samples: 93.14481576692374 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [696/1000], Step [100/137], Loss: 0.1524\n",
      "Accuracy of the model on the 17505 samples: 93.43044844330191 %\n",
      "Accuracy of the model on the 5835 samples: 90.29991431019708 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [697/1000], Step [100/137], Loss: 0.1583\n",
      "Accuracy of the model on the 17505 samples: 93.08197657812053 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [698/1000], Step [100/137], Loss: 0.1412\n",
      "Accuracy of the model on the 17505 samples: 93.39045986860897 %\n",
      "Accuracy of the model on the 5835 samples: 90.23136246786632 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [699/1000], Step [100/137], Loss: 0.1355\n",
      "Accuracy of the model on the 17505 samples: 93.36189660097115 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [700/1000], Step [100/137], Loss: 0.1493\n",
      "Accuracy of the model on the 17505 samples: 92.88774635818338 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [701/1000], Step [100/137], Loss: 0.1483\n",
      "Accuracy of the model on the 17505 samples: 93.45901171093973 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [702/1000], Step [100/137], Loss: 0.1436\n",
      "Accuracy of the model on the 17505 samples: 93.59040274207369 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [703/1000], Step [100/137], Loss: 0.1655\n",
      "Accuracy of the model on the 17505 samples: 92.12225078548985 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Epoch [704/1000], Step [100/137], Loss: 0.1726\n",
      "Accuracy of the model on the 17505 samples: 92.25364181662383 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Epoch [705/1000], Step [100/137], Loss: 0.1835\n",
      "Accuracy of the model on the 17505 samples: 93.26478149100257 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [706/1000], Step [100/137], Loss: 0.1509\n",
      "Accuracy of the model on the 17505 samples: 93.15624107397886 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Epoch [707/1000], Step [100/137], Loss: 0.1689\n",
      "Accuracy of the model on the 17505 samples: 93.53898886032562 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Epoch [708/1000], Step [100/137], Loss: 0.1523\n",
      "Accuracy of the model on the 17505 samples: 93.1962296486718 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [709/1000], Step [100/137], Loss: 0.1803\n",
      "Accuracy of the model on the 17505 samples: 92.96201085404171 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [710/1000], Step [100/137], Loss: 0.1631\n",
      "Accuracy of the model on the 17505 samples: 93.06483861753784 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [711/1000], Step [100/137], Loss: 0.1836\n",
      "Accuracy of the model on the 17505 samples: 92.24221650956869 %\n",
      "Accuracy of the model on the 5835 samples: 88.89460154241645 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Epoch [712/1000], Step [100/137], Loss: 0.1807\n",
      "Accuracy of the model on the 17505 samples: 92.76206798057697 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [713/1000], Step [100/137], Loss: 0.2803\n",
      "Accuracy of the model on the 17505 samples: 89.50014281633818 %\n",
      "Accuracy of the model on the 5835 samples: 88.17480719794345 %\n",
      "Accuracy of the model on the 5835 samples: 88.03770351328193 %\n",
      "Epoch [714/1000], Step [100/137], Loss: 0.2941\n",
      "Accuracy of the model on the 17505 samples: 89.66009711510996 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Epoch [715/1000], Step [100/137], Loss: 0.2687\n",
      "Accuracy of the model on the 17505 samples: 90.63124821479578 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Epoch [716/1000], Step [100/137], Loss: 0.2198\n",
      "Accuracy of the model on the 17505 samples: 91.15681233933162 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [717/1000], Step [100/137], Loss: 0.2169\n",
      "Accuracy of the model on the 17505 samples: 91.09397315052843 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [718/1000], Step [100/137], Loss: 0.2147\n",
      "Accuracy of the model on the 17505 samples: 91.92230791202513 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Epoch [719/1000], Step [100/137], Loss: 0.2110\n",
      "Accuracy of the model on the 17505 samples: 91.33961725221366 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [720/1000], Step [100/137], Loss: 0.2103\n",
      "Accuracy of the model on the 17505 samples: 91.97372179377321 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [721/1000], Step [100/137], Loss: 0.1890\n",
      "Accuracy of the model on the 17505 samples: 92.56783776063982 %\n",
      "Accuracy of the model on the 5835 samples: 90.33419023136247 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [722/1000], Step [100/137], Loss: 0.1948\n",
      "Accuracy of the model on the 17505 samples: 92.7277920594116 %\n",
      "Accuracy of the model on the 5835 samples: 90.40274207369323 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [723/1000], Step [100/137], Loss: 0.1974\n",
      "Accuracy of the model on the 17505 samples: 92.2879177377892 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [724/1000], Step [100/137], Loss: 0.1673\n",
      "Accuracy of the model on the 17505 samples: 92.87632105112824 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [725/1000], Step [100/137], Loss: 0.1658\n",
      "Accuracy of the model on the 17505 samples: 92.88203370465581 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [726/1000], Step [100/137], Loss: 0.1616\n",
      "Accuracy of the model on the 17505 samples: 93.16195372750643 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [727/1000], Step [100/137], Loss: 0.1612\n",
      "Accuracy of the model on the 17505 samples: 93.57897743501857 %\n",
      "Accuracy of the model on the 5835 samples: 90.33419023136247 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [728/1000], Step [100/137], Loss: 0.2064\n",
      "Accuracy of the model on the 17505 samples: 92.37932019423022 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Epoch [729/1000], Step [100/137], Loss: 0.1571\n",
      "Accuracy of the model on the 17505 samples: 93.22479291630962 %\n",
      "Accuracy of the model on the 5835 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 90.17994858611826 %\n",
      "Epoch [730/1000], Step [100/137], Loss: 0.1895\n",
      "Accuracy of the model on the 17505 samples: 93.07626392459298 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [731/1000], Step [100/137], Loss: 0.1667\n",
      "Accuracy of the model on the 17505 samples: 93.5275635532705 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [732/1000], Step [100/137], Loss: 0.1704\n",
      "Accuracy of the model on the 17505 samples: 93.17909168808912 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [733/1000], Step [100/137], Loss: 0.1556\n",
      "Accuracy of the model on the 17505 samples: 93.52185089974293 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [734/1000], Step [100/137], Loss: 0.1506\n",
      "Accuracy of the model on the 17505 samples: 92.7906312482148 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Epoch [735/1000], Step [100/137], Loss: 0.2127\n",
      "Accuracy of the model on the 17505 samples: 92.3964581548129 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [736/1000], Step [100/137], Loss: 0.3557\n",
      "Accuracy of the model on the 17505 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 87.7120822622108 %\n",
      "Accuracy of the model on the 5835 samples: 87.30077120822622 %\n",
      "Epoch [737/1000], Step [100/137], Loss: 0.1729\n",
      "Accuracy of the model on the 17505 samples: 92.39074550128535 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [738/1000], Step [100/137], Loss: 0.1645\n",
      "Accuracy of the model on the 17505 samples: 93.29905741216795 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [739/1000], Step [100/137], Loss: 0.1629\n",
      "Accuracy of the model on the 17505 samples: 92.53356183947443 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [740/1000], Step [100/137], Loss: 0.2056\n",
      "Accuracy of the model on the 17505 samples: 92.46500999714367 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [741/1000], Step [100/137], Loss: 0.1525\n",
      "Accuracy of the model on the 17505 samples: 93.35618394744358 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [742/1000], Step [100/137], Loss: 0.1555\n",
      "Accuracy of the model on the 17505 samples: 93.53898886032562 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Accuracy of the model on the 5835 samples: 90.40274207369323 %\n",
      "Epoch [743/1000], Step [100/137], Loss: 0.1680\n",
      "Accuracy of the model on the 17505 samples: 92.92773493287632 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Epoch [744/1000], Step [100/137], Loss: 0.1538\n",
      "Accuracy of the model on the 17505 samples: 93.87603541845188 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Epoch [745/1000], Step [100/137], Loss: 0.1763\n",
      "Accuracy of the model on the 17505 samples: 93.36760925449872 %\n",
      "Accuracy of the model on the 5835 samples: 90.12853470437018 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Epoch [746/1000], Step [100/137], Loss: 0.1852\n",
      "Accuracy of the model on the 17505 samples: 92.19651528134818 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [747/1000], Step [100/137], Loss: 0.1545\n",
      "Accuracy of the model on the 17505 samples: 92.9791488146244 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [748/1000], Step [100/137], Loss: 0.1407\n",
      "Accuracy of the model on the 17505 samples: 93.48186232504999 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Epoch [749/1000], Step [100/137], Loss: 0.1886\n",
      "Accuracy of the model on the 17505 samples: 92.7734932876321 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Epoch [750/1000], Step [100/137], Loss: 0.2202\n",
      "Accuracy of the model on the 17505 samples: 92.63067694944301 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.04884318766067 %\n",
      "Epoch [751/1000], Step [100/137], Loss: 0.1808\n",
      "Accuracy of the model on the 17505 samples: 93.01913738931734 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [752/1000], Step [100/137], Loss: 0.1653\n",
      "Accuracy of the model on the 17505 samples: 92.76778063410454 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [753/1000], Step [100/137], Loss: 0.1742\n",
      "Accuracy of the model on the 17505 samples: 93.07626392459298 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [754/1000], Step [100/137], Loss: 0.1520\n",
      "Accuracy of the model on the 17505 samples: 93.07626392459298 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [755/1000], Step [100/137], Loss: 0.1658\n",
      "Accuracy of the model on the 17505 samples: 93.00771208226222 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [756/1000], Step [100/137], Loss: 0.1659\n",
      "Accuracy of the model on the 17505 samples: 93.63039131676663 %\n",
      "Accuracy of the model on the 5835 samples: 90.41988003427592 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [757/1000], Step [100/137], Loss: 0.1488\n",
      "Accuracy of the model on the 17505 samples: 92.98486146815196 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [758/1000], Step [100/137], Loss: 0.1454\n",
      "Accuracy of the model on the 17505 samples: 93.25335618394745 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [759/1000], Step [100/137], Loss: 0.1768\n",
      "Accuracy of the model on the 17505 samples: 93.29334475864039 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Epoch [760/1000], Step [100/137], Loss: 0.2053\n",
      "Accuracy of the model on the 17505 samples: 93.27049414453013 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [761/1000], Step [100/137], Loss: 0.1479\n",
      "Accuracy of the model on the 17505 samples: 93.573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [762/1000], Step [100/137], Loss: 0.2008\n",
      "Accuracy of the model on the 17505 samples: 92.16795201371036 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 90.02570694087403 %\n",
      "Epoch [763/1000], Step [100/137], Loss: 0.1493\n",
      "Accuracy of the model on the 17505 samples: 93.24193087689231 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [764/1000], Step [100/137], Loss: 0.1964\n",
      "Accuracy of the model on the 17505 samples: 93.51042559268781 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [765/1000], Step [100/137], Loss: 0.1902\n",
      "Accuracy of the model on the 17505 samples: 92.48214795772637 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [766/1000], Step [100/137], Loss: 0.1558\n",
      "Accuracy of the model on the 17505 samples: 92.69922879177378 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [767/1000], Step [100/137], Loss: 0.1610\n",
      "Accuracy of the model on the 17505 samples: 93.47614967152242 %\n",
      "Accuracy of the model on the 5835 samples: 89.94001713796058 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [768/1000], Step [100/137], Loss: 0.1584\n",
      "Accuracy of the model on the 17505 samples: 93.42473578977435 %\n",
      "Accuracy of the model on the 5835 samples: 90.00856898029134 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Epoch [769/1000], Step [100/137], Loss: 0.1462\n",
      "Accuracy of the model on the 17505 samples: 93.42473578977435 %\n",
      "Accuracy of the model on the 5835 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 90.0942587832048 %\n",
      "Epoch [770/1000], Step [100/137], Loss: 0.1693\n",
      "Accuracy of the model on the 17505 samples: 93.43616109682948 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [771/1000], Step [100/137], Loss: 0.3583\n",
      "Accuracy of the model on the 17505 samples: 88.20337046558126 %\n",
      "Accuracy of the model on the 5835 samples: 87.40359897172236 %\n",
      "Accuracy of the model on the 5835 samples: 87.48928877463582 %\n",
      "Epoch [772/1000], Step [100/137], Loss: 0.3317\n",
      "Accuracy of the model on the 17505 samples: 89.02027992002284 %\n",
      "Accuracy of the model on the 5835 samples: 87.62639245929735 %\n",
      "Accuracy of the model on the 5835 samples: 88.46615252784919 %\n",
      "Epoch [773/1000], Step [100/137], Loss: 0.2886\n",
      "Accuracy of the model on the 17505 samples: 89.6086832333619 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Epoch [774/1000], Step [100/137], Loss: 0.3001\n",
      "Accuracy of the model on the 17505 samples: 90.21422450728363 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [775/1000], Step [100/137], Loss: 0.2974\n",
      "Accuracy of the model on the 17505 samples: 90.68266209654385 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [776/1000], Step [100/137], Loss: 0.2227\n",
      "Accuracy of the model on the 17505 samples: 90.8311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [777/1000], Step [100/137], Loss: 0.2178\n",
      "Accuracy of the model on the 17505 samples: 91.30534133104827 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [778/1000], Step [100/137], Loss: 0.1964\n",
      "Accuracy of the model on the 17505 samples: 91.43101970865467 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [779/1000], Step [100/137], Loss: 0.2266\n",
      "Accuracy of the model on the 17505 samples: 92.20222793487575 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [780/1000], Step [100/137], Loss: 0.2076\n",
      "Accuracy of the model on the 17505 samples: 91.68808911739502 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Epoch [781/1000], Step [100/137], Loss: 0.2075\n",
      "Accuracy of the model on the 17505 samples: 93.167666381034 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [782/1000], Step [100/137], Loss: 0.1612\n",
      "Accuracy of the model on the 17505 samples: 93.27049414453013 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [783/1000], Step [100/137], Loss: 0.1746\n",
      "Accuracy of the model on the 17505 samples: 93.00199942873465 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [784/1000], Step [100/137], Loss: 0.1523\n",
      "Accuracy of the model on the 17505 samples: 93.32190802627821 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [785/1000], Step [100/137], Loss: 0.1749\n",
      "Accuracy of the model on the 17505 samples: 92.75064267352185 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Epoch [786/1000], Step [100/137], Loss: 0.1593\n",
      "Accuracy of the model on the 17505 samples: 92.75064267352185 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [787/1000], Step [100/137], Loss: 0.1636\n",
      "Accuracy of the model on the 17505 samples: 92.7277920594116 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [788/1000], Step [100/137], Loss: 0.1577\n",
      "Accuracy of the model on the 17505 samples: 92.8991716652385 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [789/1000], Step [100/137], Loss: 0.1969\n",
      "Accuracy of the model on the 17505 samples: 93.48757497857756 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Epoch [790/1000], Step [100/137], Loss: 0.1661\n",
      "Accuracy of the model on the 17505 samples: 93.3847472150814 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [791/1000], Step [100/137], Loss: 0.1522\n",
      "Accuracy of the model on the 17505 samples: 93.12196515281349 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [792/1000], Step [100/137], Loss: 0.1800\n",
      "Accuracy of the model on the 17505 samples: 92.76206798057697 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [793/1000], Step [100/137], Loss: 0.1489\n",
      "Accuracy of the model on the 17505 samples: 92.56783776063982 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [794/1000], Step [100/137], Loss: 0.1567\n",
      "Accuracy of the model on the 17505 samples: 93.37332190802628 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [795/1000], Step [100/137], Loss: 0.1813\n",
      "Accuracy of the model on the 17505 samples: 92.23079120251357 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Epoch [796/1000], Step [100/137], Loss: 0.2208\n",
      "Accuracy of the model on the 17505 samples: 93.11625249928592 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [797/1000], Step [100/137], Loss: 0.1496\n",
      "Accuracy of the model on the 17505 samples: 92.9163096258212 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Epoch [798/1000], Step [100/137], Loss: 0.1485\n",
      "Accuracy of the model on the 17505 samples: 92.96201085404171 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [799/1000], Step [100/137], Loss: 0.1790\n",
      "Accuracy of the model on the 17505 samples: 92.92773493287632 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [800/1000], Step [100/137], Loss: 0.1389\n",
      "Accuracy of the model on the 17505 samples: 93.0876892316481 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [801/1000], Step [100/137], Loss: 0.1534\n",
      "Accuracy of the model on the 17505 samples: 92.84204512996287 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [802/1000], Step [100/137], Loss: 0.1702\n",
      "Accuracy of the model on the 17505 samples: 92.16223936018281 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [803/1000], Step [100/137], Loss: 0.2868\n",
      "Accuracy of the model on the 17505 samples: 88.80319908597544 %\n",
      "Accuracy of the model on the 5835 samples: 87.78063410454156 %\n",
      "Accuracy of the model on the 5835 samples: 88.10625535561269 %\n",
      "Epoch [804/1000], Step [100/137], Loss: 0.2889\n",
      "Accuracy of the model on the 17505 samples: 89.44872893459012 %\n",
      "Accuracy of the model on the 5835 samples: 88.03770351328193 %\n",
      "Accuracy of the model on the 5835 samples: 88.6203941730934 %\n",
      "Epoch [805/1000], Step [100/137], Loss: 0.2972\n",
      "Accuracy of the model on the 17505 samples: 89.87717794915739 %\n",
      "Accuracy of the model on the 5835 samples: 88.4318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [806/1000], Step [100/137], Loss: 0.2951\n",
      "Accuracy of the model on the 17505 samples: 89.70579834333047 %\n",
      "Accuracy of the model on the 5835 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [807/1000], Step [100/137], Loss: 0.2403\n",
      "Accuracy of the model on the 17505 samples: 90.29991431019708 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Epoch [808/1000], Step [100/137], Loss: 0.2389\n",
      "Accuracy of the model on the 17505 samples: 90.5169951442445 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Epoch [809/1000], Step [100/137], Loss: 0.2348\n",
      "Accuracy of the model on the 17505 samples: 90.59125964010283 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Epoch [810/1000], Step [100/137], Loss: 0.2374\n",
      "Accuracy of the model on the 17505 samples: 90.48271922307912 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [811/1000], Step [100/137], Loss: 0.3102\n",
      "Accuracy of the model on the 17505 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 88.2604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.15766923736075 %\n",
      "Epoch [812/1000], Step [100/137], Loss: 0.2349\n",
      "Accuracy of the model on the 17505 samples: 90.85975435589832 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [813/1000], Step [100/137], Loss: 0.2337\n",
      "Accuracy of the model on the 17505 samples: 90.69980005712654 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Epoch [814/1000], Step [100/137], Loss: 0.2175\n",
      "Accuracy of the model on the 17505 samples: 90.83690374178806 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Epoch [815/1000], Step [100/137], Loss: 0.2253\n",
      "Accuracy of the model on the 17505 samples: 91.0197086546701 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [816/1000], Step [100/137], Loss: 0.2315\n",
      "Accuracy of the model on the 17505 samples: 91.11111111111111 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Accuracy of the model on the 5835 samples: 89.04884318766067 %\n",
      "Epoch [817/1000], Step [100/137], Loss: 0.2080\n",
      "Accuracy of the model on the 17505 samples: 91.02542130819766 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Epoch [818/1000], Step [100/137], Loss: 0.2092\n",
      "Accuracy of the model on the 17505 samples: 91.07683518994573 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [819/1000], Step [100/137], Loss: 0.2255\n",
      "Accuracy of the model on the 17505 samples: 90.9111682376464 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [820/1000], Step [100/137], Loss: 0.2213\n",
      "Accuracy of the model on the 17505 samples: 91.02542130819766 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [821/1000], Step [100/137], Loss: 0.2115\n",
      "Accuracy of the model on the 17505 samples: 90.89403027706369 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [822/1000], Step [100/137], Loss: 0.2054\n",
      "Accuracy of the model on the 17505 samples: 90.876892316481 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [823/1000], Step [100/137], Loss: 0.2396\n",
      "Accuracy of the model on the 17505 samples: 90.14567266495287 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [824/1000], Step [100/137], Loss: 0.2626\n",
      "Accuracy of the model on the 17505 samples: 90.7055127106541 %\n",
      "Accuracy of the model on the 5835 samples: 88.50042844901456 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Epoch [825/1000], Step [100/137], Loss: 0.2249\n",
      "Accuracy of the model on the 17505 samples: 91.08826049700086 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [826/1000], Step [100/137], Loss: 0.2165\n",
      "Accuracy of the model on the 17505 samples: 91.31105398457584 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [827/1000], Step [100/137], Loss: 0.2918\n",
      "Accuracy of the model on the 17505 samples: 90.31705227077978 %\n",
      "Accuracy of the model on the 5835 samples: 88.72322193658954 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [828/1000], Step [100/137], Loss: 0.2190\n",
      "Accuracy of the model on the 17505 samples: 91.0825478434733 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Epoch [829/1000], Step [100/137], Loss: 0.2360\n",
      "Accuracy of the model on the 17505 samples: 91.0825478434733 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [830/1000], Step [100/137], Loss: 0.2275\n",
      "Accuracy of the model on the 17505 samples: 91.37389317337903 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [831/1000], Step [100/137], Loss: 0.2783\n",
      "Accuracy of the model on the 17505 samples: 90.17994858611826 %\n",
      "Accuracy of the model on the 5835 samples: 88.55184233076264 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Epoch [832/1000], Step [100/137], Loss: 0.2532\n",
      "Accuracy of the model on the 17505 samples: 90.9111682376464 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [833/1000], Step [100/137], Loss: 0.2472\n",
      "Accuracy of the model on the 17505 samples: 90.49985718366182 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Epoch [834/1000], Step [100/137], Loss: 0.2055\n",
      "Accuracy of the model on the 17505 samples: 91.00828334761496 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Epoch [835/1000], Step [100/137], Loss: 0.2170\n",
      "Accuracy of the model on the 17505 samples: 91.2539274493002 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Accuracy of the model on the 5835 samples: 89.3401885175664 %\n",
      "Epoch [836/1000], Step [100/137], Loss: 0.2469\n",
      "Accuracy of the model on the 17505 samples: 90.74550128534705 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [837/1000], Step [100/137], Loss: 0.2317\n",
      "Accuracy of the model on the 17505 samples: 91.25964010282776 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [838/1000], Step [100/137], Loss: 0.1992\n",
      "Accuracy of the model on the 17505 samples: 91.60811196800914 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [839/1000], Step [100/137], Loss: 0.1980\n",
      "Accuracy of the model on the 17505 samples: 91.75664095972579 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [840/1000], Step [100/137], Loss: 0.1986\n",
      "Accuracy of the model on the 17505 samples: 91.75092830619823 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [841/1000], Step [100/137], Loss: 0.2043\n",
      "Accuracy of the model on the 17505 samples: 91.69380177092259 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 90.0771208226221 %\n",
      "Epoch [842/1000], Step [100/137], Loss: 0.2050\n",
      "Accuracy of the model on the 17505 samples: 91.61382462153671 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Epoch [843/1000], Step [100/137], Loss: 0.2102\n",
      "Accuracy of the model on the 17505 samples: 91.819480148529 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [844/1000], Step [100/137], Loss: 0.2299\n",
      "Accuracy of the model on the 17505 samples: 91.85946872322194 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [845/1000], Step [100/137], Loss: 0.3795\n",
      "Accuracy of the model on the 17505 samples: 89.72293630391317 %\n",
      "Accuracy of the model on the 5835 samples: 88.39760068551843 %\n",
      "Accuracy of the model on the 5835 samples: 88.41473864610111 %\n",
      "Epoch [846/1000], Step [100/137], Loss: 0.3001\n",
      "Accuracy of the model on the 17505 samples: 88.58040559840046 %\n",
      "Accuracy of the model on the 5835 samples: 87.60925449871465 %\n",
      "Accuracy of the model on the 5835 samples: 87.8834618680377 %\n",
      "Epoch [847/1000], Step [100/137], Loss: 0.2933\n",
      "Accuracy of the model on the 17505 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 88.70608397600685 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [848/1000], Step [100/137], Loss: 0.2802\n",
      "Accuracy of the model on the 17505 samples: 89.77435018566123 %\n",
      "Accuracy of the model on the 5835 samples: 88.56898029134533 %\n",
      "Accuracy of the model on the 5835 samples: 88.89460154241645 %\n",
      "Epoch [849/1000], Step [100/137], Loss: 0.2618\n",
      "Accuracy of the model on the 17505 samples: 90.1113967437875 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Accuracy of the model on the 5835 samples: 89.22022279348758 %\n",
      "Epoch [850/1000], Step [100/137], Loss: 0.2411\n",
      "Accuracy of the model on the 17505 samples: 89.94572979148815 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [851/1000], Step [100/137], Loss: 0.2403\n",
      "Accuracy of the model on the 17505 samples: 90.39702942016567 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.44301628106255 %\n",
      "Epoch [852/1000], Step [100/137], Loss: 0.2593\n",
      "Accuracy of the model on the 17505 samples: 90.40274207369323 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Epoch [853/1000], Step [100/137], Loss: 0.2541\n",
      "Accuracy of the model on the 17505 samples: 90.71122536418166 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Accuracy of the model on the 5835 samples: 89.47729220222793 %\n",
      "Epoch [854/1000], Step [100/137], Loss: 0.2413\n",
      "Accuracy of the model on the 17505 samples: 90.73978863181948 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [855/1000], Step [100/137], Loss: 0.2514\n",
      "Accuracy of the model on the 17505 samples: 90.58554698657527 %\n",
      "Accuracy of the model on the 5835 samples: 88.89460154241645 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [856/1000], Step [100/137], Loss: 0.2444\n",
      "Accuracy of the model on the 17505 samples: 90.8311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Epoch [857/1000], Step [100/137], Loss: 0.2376\n",
      "Accuracy of the model on the 17505 samples: 90.57412167952013 %\n",
      "Accuracy of the model on the 5835 samples: 89.528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Epoch [858/1000], Step [100/137], Loss: 0.2381\n",
      "Accuracy of the model on the 17505 samples: 91.39103113396173 %\n",
      "Accuracy of the model on the 5835 samples: 89.9228791773779 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [859/1000], Step [100/137], Loss: 0.2332\n",
      "Accuracy of the model on the 17505 samples: 90.89403027706369 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [860/1000], Step [100/137], Loss: 0.2321\n",
      "Accuracy of the model on the 17505 samples: 90.98543273350471 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [861/1000], Step [100/137], Loss: 0.2513\n",
      "Accuracy of the model on the 17505 samples: 90.93401885175665 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 89.51156812339332 %\n",
      "Epoch [862/1000], Step [100/137], Loss: 0.2310\n",
      "Accuracy of the model on the 17505 samples: 91.15109968580406 %\n",
      "Accuracy of the model on the 5835 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 89.61439588688945 %\n",
      "Epoch [863/1000], Step [100/137], Loss: 0.1953\n",
      "Accuracy of the model on the 17505 samples: 90.99114538703228 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [864/1000], Step [100/137], Loss: 0.2374\n",
      "Accuracy of the model on the 17505 samples: 91.05969722936304 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [865/1000], Step [100/137], Loss: 0.2142\n",
      "Accuracy of the model on the 17505 samples: 90.75692659240217 %\n",
      "Accuracy of the model on the 5835 samples: 89.25449871465295 %\n",
      "Accuracy of the model on the 5835 samples: 89.3573264781491 %\n",
      "Epoch [866/1000], Step [100/137], Loss: 0.2027\n",
      "Accuracy of the model on the 17505 samples: 91.07683518994573 %\n",
      "Accuracy of the model on the 5835 samples: 89.27163667523564 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [867/1000], Step [100/137], Loss: 0.2306\n",
      "Accuracy of the model on the 17505 samples: 91.50528420451299 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [868/1000], Step [100/137], Loss: 0.2074\n",
      "Accuracy of the model on the 17505 samples: 91.31105398457584 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Epoch [869/1000], Step [100/137], Loss: 0.1949\n",
      "Accuracy of the model on the 17505 samples: 91.38531848043417 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.58011996572408 %\n",
      "Epoch [870/1000], Step [100/137], Loss: 0.1994\n",
      "Accuracy of the model on the 17505 samples: 91.47672093687518 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.76863753213368 %\n",
      "Epoch [871/1000], Step [100/137], Loss: 0.1921\n",
      "Accuracy of the model on the 17505 samples: 91.59097400742645 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [872/1000], Step [100/137], Loss: 0.2077\n",
      "Accuracy of the model on the 17505 samples: 91.52242216509569 %\n",
      "Accuracy of the model on the 5835 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 89.70008568980292 %\n",
      "Epoch [873/1000], Step [100/137], Loss: 0.1895\n",
      "Accuracy of the model on the 17505 samples: 91.87089403027706 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [874/1000], Step [100/137], Loss: 0.1956\n",
      "Accuracy of the model on the 17505 samples: 91.75092830619823 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [875/1000], Step [100/137], Loss: 0.1918\n",
      "Accuracy of the model on the 17505 samples: 91.8480434161668 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [876/1000], Step [100/137], Loss: 0.1947\n",
      "Accuracy of the model on the 17505 samples: 91.85946872322194 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 90.14567266495287 %\n",
      "Epoch [877/1000], Step [100/137], Loss: 0.1979\n",
      "Accuracy of the model on the 17505 samples: 91.73379034561553 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [878/1000], Step [100/137], Loss: 0.2176\n",
      "Accuracy of the model on the 17505 samples: 91.22536418166239 %\n",
      "Accuracy of the model on the 5835 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 89.28877463581834 %\n",
      "Epoch [879/1000], Step [100/137], Loss: 0.2050\n",
      "Accuracy of the model on the 17505 samples: 91.64238788917451 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [880/1000], Step [100/137], Loss: 0.1881\n",
      "Accuracy of the model on the 17505 samples: 91.68237646386747 %\n",
      "Accuracy of the model on the 5835 samples: 89.9057412167952 %\n",
      "Accuracy of the model on the 5835 samples: 89.82005141388174 %\n",
      "Epoch [881/1000], Step [100/137], Loss: 0.2030\n",
      "Accuracy of the model on the 17505 samples: 91.65952584975722 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.8886032562125 %\n",
      "Epoch [882/1000], Step [100/137], Loss: 0.1881\n",
      "Accuracy of the model on the 17505 samples: 91.76806626678092 %\n",
      "Accuracy of the model on the 5835 samples: 89.95715509854327 %\n",
      "Accuracy of the model on the 5835 samples: 89.99143101970866 %\n",
      "Epoch [883/1000], Step [100/137], Loss: 0.2028\n",
      "Accuracy of the model on the 17505 samples: 92.07083690374179 %\n",
      "Accuracy of the model on the 5835 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [884/1000], Step [100/137], Loss: 0.2029\n",
      "Accuracy of the model on the 17505 samples: 91.57954870037132 %\n",
      "Accuracy of the model on the 5835 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Epoch [885/1000], Step [100/137], Loss: 0.1800\n",
      "Accuracy of the model on the 17505 samples: 92.33933161953728 %\n",
      "Accuracy of the model on the 5835 samples: 90.26563838903171 %\n",
      "Accuracy of the model on the 5835 samples: 89.83718937446444 %\n",
      "Epoch [886/1000], Step [100/137], Loss: 0.1812\n",
      "Accuracy of the model on the 17505 samples: 91.90516995144245 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Accuracy of the model on the 5835 samples: 89.32305055698372 %\n",
      "Epoch [887/1000], Step [100/137], Loss: 0.1896\n",
      "Accuracy of the model on the 17505 samples: 91.80805484147386 %\n",
      "Accuracy of the model on the 5835 samples: 89.63153384747216 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [888/1000], Step [100/137], Loss: 0.1999\n",
      "Accuracy of the model on the 17505 samples: 92.0365609825764 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Accuracy of the model on the 5835 samples: 89.85432733504713 %\n",
      "Epoch [889/1000], Step [100/137], Loss: 0.2089\n",
      "Accuracy of the model on the 17505 samples: 91.3796058269066 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Epoch [890/1000], Step [100/137], Loss: 0.1891\n",
      "Accuracy of the model on the 17505 samples: 91.6766638103399 %\n",
      "Accuracy of the model on the 5835 samples: 89.37446443873179 %\n",
      "Accuracy of the model on the 5835 samples: 89.54584404455869 %\n",
      "Epoch [891/1000], Step [100/137], Loss: 0.1718\n",
      "Accuracy of the model on the 17505 samples: 92.225078548986 %\n",
      "Accuracy of the model on the 5835 samples: 90.05998286203942 %\n",
      "Accuracy of the model on the 5835 samples: 90.04284490145673 %\n",
      "Epoch [892/1000], Step [100/137], Loss: 0.1995\n",
      "Accuracy of the model on the 17505 samples: 91.97372179377321 %\n",
      "Accuracy of the model on the 5835 samples: 89.7172236503856 %\n",
      "Accuracy of the model on the 5835 samples: 89.73436161096829 %\n",
      "Epoch [893/1000], Step [100/137], Loss: 0.1915\n",
      "Accuracy of the model on the 17505 samples: 92.08226221079691 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.78577549271637 %\n",
      "Epoch [894/1000], Step [100/137], Loss: 0.1975\n",
      "Accuracy of the model on the 17505 samples: 91.5681233933162 %\n",
      "Accuracy of the model on the 5835 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [895/1000], Step [100/137], Loss: 0.1813\n",
      "Accuracy of the model on the 17505 samples: 92.07654955726935 %\n",
      "Accuracy of the model on the 5835 samples: 90.19708654670094 %\n",
      "Accuracy of the model on the 5835 samples: 90.16281062553556 %\n",
      "Epoch [896/1000], Step [100/137], Loss: 0.1789\n",
      "Accuracy of the model on the 17505 samples: 91.23107683518995 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [897/1000], Step [100/137], Loss: 0.1769\n",
      "Accuracy of the model on the 17505 samples: 92.03084832904884 %\n",
      "Accuracy of the model on the 5835 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 89.66580976863753 %\n",
      "Epoch [898/1000], Step [100/137], Loss: 0.1943\n",
      "Accuracy of the model on the 17505 samples: 91.8480434161668 %\n",
      "Accuracy of the model on the 5835 samples: 89.68294772922022 %\n",
      "Accuracy of the model on the 5835 samples: 89.46015424164524 %\n",
      "Epoch [899/1000], Step [100/137], Loss: 0.1912\n",
      "Accuracy of the model on the 17505 samples: 91.83090545558412 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.87146529562982 %\n",
      "Epoch [900/1000], Step [100/137], Loss: 0.3530\n",
      "Accuracy of the model on the 17505 samples: 86.41530991145387 %\n",
      "Accuracy of the model on the 5835 samples: 85.6898029134533 %\n",
      "Accuracy of the model on the 5835 samples: 86.27249357326478 %\n",
      "Epoch [901/1000], Step [100/137], Loss: 0.3474\n",
      "Accuracy of the model on the 17505 samples: 87.00371322479292 %\n",
      "Accuracy of the model on the 5835 samples: 86.39245929734362 %\n",
      "Accuracy of the model on the 5835 samples: 86.11825192802057 %\n",
      "Epoch [902/1000], Step [100/137], Loss: 0.3877\n",
      "Accuracy of the model on the 17505 samples: 86.36960868323337 %\n",
      "Accuracy of the model on the 5835 samples: 86.20394173093402 %\n",
      "Accuracy of the model on the 5835 samples: 86.0325621251071 %\n",
      "Epoch [903/1000], Step [100/137], Loss: 0.3718\n",
      "Accuracy of the model on the 17505 samples: 86.26678091973722 %\n",
      "Accuracy of the model on the 5835 samples: 85.51842330762639 %\n",
      "Accuracy of the model on the 5835 samples: 86.426735218509 %\n",
      "Epoch [904/1000], Step [100/137], Loss: 0.3475\n",
      "Accuracy of the model on the 17505 samples: 86.49528706083976 %\n",
      "Accuracy of the model on the 5835 samples: 86.35818337617823 %\n",
      "Accuracy of the model on the 5835 samples: 86.88946015424165 %\n",
      "Epoch [905/1000], Step [100/137], Loss: 0.3065\n",
      "Accuracy of the model on the 17505 samples: 87.20365609825764 %\n",
      "Accuracy of the model on the 5835 samples: 87.30077120822622 %\n",
      "Accuracy of the model on the 5835 samples: 86.68380462724936 %\n",
      "Epoch [906/1000], Step [100/137], Loss: 0.2991\n",
      "Accuracy of the model on the 17505 samples: 88.21479577263639 %\n",
      "Accuracy of the model on the 5835 samples: 87.76349614395887 %\n",
      "Accuracy of the model on the 5835 samples: 87.5235646958012 %\n",
      "Epoch [907/1000], Step [100/137], Loss: 0.2977\n",
      "Accuracy of the model on the 17505 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 87.9005998286204 %\n",
      "Accuracy of the model on the 5835 samples: 87.95201371036846 %\n",
      "Epoch [908/1000], Step [100/137], Loss: 0.3164\n",
      "Accuracy of the model on the 17505 samples: 88.0091402456441 %\n",
      "Accuracy of the model on the 5835 samples: 87.98628963153385 %\n",
      "Accuracy of the model on the 5835 samples: 88.00342759211654 %\n",
      "Epoch [909/1000], Step [100/137], Loss: 0.3033\n",
      "Accuracy of the model on the 17505 samples: 88.38617537846329 %\n",
      "Accuracy of the model on the 5835 samples: 87.55784061696659 %\n",
      "Accuracy of the model on the 5835 samples: 87.95201371036846 %\n",
      "Epoch [910/1000], Step [100/137], Loss: 0.2985\n",
      "Accuracy of the model on the 17505 samples: 88.66609540131391 %\n",
      "Accuracy of the model on the 5835 samples: 87.9005998286204 %\n",
      "Accuracy of the model on the 5835 samples: 87.91773778920309 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [911/1000], Step [100/137], Loss: 0.2923\n",
      "Accuracy of the model on the 17505 samples: 88.77463581833761 %\n",
      "Accuracy of the model on the 5835 samples: 87.67780634104541 %\n",
      "Accuracy of the model on the 5835 samples: 88.24335904027421 %\n",
      "Epoch [912/1000], Step [100/137], Loss: 0.2912\n",
      "Accuracy of the model on the 17505 samples: 89.14024564410168 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n",
      "Accuracy of the model on the 5835 samples: 88.36332476435304 %\n",
      "Epoch [913/1000], Step [100/137], Loss: 0.2841\n",
      "Accuracy of the model on the 17505 samples: 89.22022279348758 %\n",
      "Accuracy of the model on the 5835 samples: 88.38046272493574 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Epoch [914/1000], Step [100/137], Loss: 0.2765\n",
      "Accuracy of the model on the 17505 samples: 89.26021136818052 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Accuracy of the model on the 5835 samples: 88.36332476435304 %\n",
      "Epoch [915/1000], Step [100/137], Loss: 0.2811\n",
      "Accuracy of the model on the 17505 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Epoch [916/1000], Step [100/137], Loss: 0.2742\n",
      "Accuracy of the model on the 17505 samples: 89.60297057983433 %\n",
      "Accuracy of the model on the 5835 samples: 89.10025706940874 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Epoch [917/1000], Step [100/137], Loss: 0.2987\n",
      "Accuracy of the model on the 17505 samples: 89.51728077692088 %\n",
      "Accuracy of the model on the 5835 samples: 88.72322193658954 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Epoch [918/1000], Step [100/137], Loss: 0.2882\n",
      "Accuracy of the model on the 17505 samples: 89.40302770636961 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [919/1000], Step [100/137], Loss: 0.2735\n",
      "Accuracy of the model on the 17505 samples: 89.81433876035419 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Epoch [920/1000], Step [100/137], Loss: 0.2879\n",
      "Accuracy of the model on the 17505 samples: 89.65438446158241 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Epoch [921/1000], Step [100/137], Loss: 0.2719\n",
      "Accuracy of the model on the 17505 samples: 89.3401885175664 %\n",
      "Accuracy of the model on the 5835 samples: 88.74035989717224 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Epoch [922/1000], Step [100/137], Loss: 0.2614\n",
      "Accuracy of the model on the 17505 samples: 89.64295915452728 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Epoch [923/1000], Step [100/137], Loss: 0.2726\n",
      "Accuracy of the model on the 17505 samples: 89.65438446158241 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [924/1000], Step [100/137], Loss: 0.2720\n",
      "Accuracy of the model on the 17505 samples: 89.31733790345615 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [925/1000], Step [100/137], Loss: 0.2899\n",
      "Accuracy of the model on the 17505 samples: 89.49443016281063 %\n",
      "Accuracy of the model on the 5835 samples: 88.6203941730934 %\n",
      "Accuracy of the model on the 5835 samples: 88.808911739503 %\n",
      "Epoch [926/1000], Step [100/137], Loss: 0.2610\n",
      "Accuracy of the model on the 17505 samples: 89.87146529562982 %\n",
      "Accuracy of the model on the 5835 samples: 89.03170522707798 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Epoch [927/1000], Step [100/137], Loss: 0.2520\n",
      "Accuracy of the model on the 17505 samples: 89.87717794915739 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [928/1000], Step [100/137], Loss: 0.2503\n",
      "Accuracy of the model on the 17505 samples: 89.87717794915739 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [929/1000], Step [100/137], Loss: 0.2482\n",
      "Accuracy of the model on the 17505 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Epoch [930/1000], Step [100/137], Loss: 0.2492\n",
      "Accuracy of the model on the 17505 samples: 89.82576406740931 %\n",
      "Accuracy of the model on the 5835 samples: 88.72322193658954 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [931/1000], Step [100/137], Loss: 0.2424\n",
      "Accuracy of the model on the 17505 samples: 89.99714367323622 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Epoch [932/1000], Step [100/137], Loss: 0.2399\n",
      "Accuracy of the model on the 17505 samples: 90.08854612967724 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Epoch [933/1000], Step [100/137], Loss: 0.2470\n",
      "Accuracy of the model on the 17505 samples: 90.20851185375606 %\n",
      "Accuracy of the model on the 5835 samples: 89.30591259640103 %\n",
      "Accuracy of the model on the 5835 samples: 89.1688089117395 %\n",
      "Epoch [934/1000], Step [100/137], Loss: 0.2586\n",
      "Accuracy of the model on the 17505 samples: 90.08283347614967 %\n",
      "Accuracy of the model on the 5835 samples: 89.23736075407027 %\n",
      "Accuracy of the model on the 5835 samples: 89.5629820051414 %\n",
      "Epoch [935/1000], Step [100/137], Loss: 0.2671\n",
      "Accuracy of the model on the 17505 samples: 89.16309625821194 %\n",
      "Accuracy of the model on the 5835 samples: 87.91773778920309 %\n",
      "Accuracy of the model on the 5835 samples: 88.14053127677806 %\n",
      "Epoch [936/1000], Step [100/137], Loss: 0.2515\n",
      "Accuracy of the model on the 17505 samples: 89.76863753213368 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 89.13453299057412 %\n",
      "Epoch [937/1000], Step [100/137], Loss: 0.2644\n",
      "Accuracy of the model on the 17505 samples: 89.07740645529849 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Epoch [938/1000], Step [100/137], Loss: 0.2735\n",
      "Accuracy of the model on the 17505 samples: 89.6086832333619 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Epoch [939/1000], Step [100/137], Loss: 0.2874\n",
      "Accuracy of the model on the 17505 samples: 89.55155669808626 %\n",
      "Accuracy of the model on the 5835 samples: 88.27763496143959 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Epoch [940/1000], Step [100/137], Loss: 0.2677\n",
      "Accuracy of the model on the 17505 samples: 88.5118537560697 %\n",
      "Accuracy of the model on the 5835 samples: 88.12339331619538 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Epoch [941/1000], Step [100/137], Loss: 0.2724\n",
      "Accuracy of the model on the 17505 samples: 89.11739502999143 %\n",
      "Accuracy of the model on the 5835 samples: 88.63753213367609 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [942/1000], Step [100/137], Loss: 0.2714\n",
      "Accuracy of the model on the 17505 samples: 89.44301628106255 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Accuracy of the model on the 5835 samples: 88.60325621251071 %\n",
      "Epoch [943/1000], Step [100/137], Loss: 0.3015\n",
      "Accuracy of the model on the 17505 samples: 88.85461296772351 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Accuracy of the model on the 5835 samples: 88.34618680377035 %\n",
      "Epoch [944/1000], Step [100/137], Loss: 0.3058\n",
      "Accuracy of the model on the 17505 samples: 87.9005998286204 %\n",
      "Accuracy of the model on the 5835 samples: 87.76349614395887 %\n",
      "Accuracy of the model on the 5835 samples: 87.64353041988004 %\n",
      "Epoch [945/1000], Step [100/137], Loss: 0.3106\n",
      "Accuracy of the model on the 17505 samples: 88.5118537560697 %\n",
      "Accuracy of the model on the 5835 samples: 88.32904884318766 %\n",
      "Accuracy of the model on the 5835 samples: 88.02056555269922 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [946/1000], Step [100/137], Loss: 0.2919\n",
      "Accuracy of the model on the 17505 samples: 88.9345901171094 %\n",
      "Accuracy of the model on the 5835 samples: 88.38046272493574 %\n",
      "Accuracy of the model on the 5835 samples: 88.4490145672665 %\n",
      "Epoch [947/1000], Step [100/137], Loss: 0.2923\n",
      "Accuracy of the model on the 17505 samples: 89.0431305341331 %\n",
      "Accuracy of the model on the 5835 samples: 88.38046272493574 %\n",
      "Accuracy of the model on the 5835 samples: 88.39760068551843 %\n",
      "Epoch [948/1000], Step [100/137], Loss: 0.2988\n",
      "Accuracy of the model on the 17505 samples: 88.86603827477863 %\n",
      "Accuracy of the model on the 5835 samples: 88.31191088260498 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n",
      "Epoch [949/1000], Step [100/137], Loss: 0.2938\n",
      "Accuracy of the model on the 17505 samples: 89.26592402170809 %\n",
      "Accuracy of the model on the 5835 samples: 88.31191088260498 %\n",
      "Accuracy of the model on the 5835 samples: 88.41473864610111 %\n",
      "Epoch [950/1000], Step [100/137], Loss: 0.2815\n",
      "Accuracy of the model on the 17505 samples: 89.35161382462154 %\n",
      "Accuracy of the model on the 5835 samples: 88.10625535561269 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Epoch [951/1000], Step [100/137], Loss: 0.2647\n",
      "Accuracy of the model on the 17505 samples: 89.2944872893459 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Epoch [952/1000], Step [100/137], Loss: 0.2758\n",
      "Accuracy of the model on the 17505 samples: 89.0431305341331 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Accuracy of the model on the 5835 samples: 88.65467009425878 %\n",
      "Epoch [953/1000], Step [100/137], Loss: 0.2710\n",
      "Accuracy of the model on the 17505 samples: 89.28306198229077 %\n",
      "Accuracy of the model on the 5835 samples: 88.46615252784919 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n",
      "Epoch [954/1000], Step [100/137], Loss: 0.2714\n",
      "Accuracy of the model on the 17505 samples: 89.15167095115682 %\n",
      "Accuracy of the model on the 5835 samples: 88.56898029134533 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Epoch [955/1000], Step [100/137], Loss: 0.2814\n",
      "Accuracy of the model on the 17505 samples: 89.31733790345615 %\n",
      "Accuracy of the model on the 5835 samples: 88.55184233076264 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Epoch [956/1000], Step [100/137], Loss: 0.2788\n",
      "Accuracy of the model on the 17505 samples: 89.5915452727792 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [957/1000], Step [100/137], Loss: 0.2685\n",
      "Accuracy of the model on the 17505 samples: 89.57440731219651 %\n",
      "Accuracy of the model on the 5835 samples: 88.72322193658954 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Epoch [958/1000], Step [100/137], Loss: 0.2617\n",
      "Accuracy of the model on the 17505 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 88.72322193658954 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Epoch [959/1000], Step [100/137], Loss: 0.2624\n",
      "Accuracy of the model on the 17505 samples: 89.74578691802343 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Epoch [960/1000], Step [100/137], Loss: 0.2800\n",
      "Accuracy of the model on the 17505 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 88.53470437017995 %\n",
      "Accuracy of the model on the 5835 samples: 88.82604970008569 %\n",
      "Epoch [961/1000], Step [100/137], Loss: 0.2801\n",
      "Accuracy of the model on the 17505 samples: 89.55726935161383 %\n",
      "Accuracy of the model on the 5835 samples: 89.11739502999143 %\n",
      "Accuracy of the model on the 5835 samples: 88.87746358183377 %\n",
      "Epoch [962/1000], Step [100/137], Loss: 0.2559\n",
      "Accuracy of the model on the 17505 samples: 89.90002856326764 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Epoch [963/1000], Step [100/137], Loss: 0.2510\n",
      "Accuracy of the model on the 17505 samples: 89.80291345329906 %\n",
      "Accuracy of the model on the 5835 samples: 88.7917737789203 %\n",
      "Accuracy of the model on the 5835 samples: 89.20308483290488 %\n",
      "Epoch [964/1000], Step [100/137], Loss: 0.2466\n",
      "Accuracy of the model on the 17505 samples: 89.75149957155098 %\n",
      "Accuracy of the model on the 5835 samples: 88.70608397600685 %\n",
      "Accuracy of the model on the 5835 samples: 89.06598114824335 %\n",
      "Epoch [965/1000], Step [100/137], Loss: 0.2506\n",
      "Accuracy of the model on the 17505 samples: 90.07140816909454 %\n",
      "Accuracy of the model on the 5835 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 88.808911739503 %\n",
      "Epoch [966/1000], Step [100/137], Loss: 0.2473\n",
      "Accuracy of the model on the 17505 samples: 89.97429305912597 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Accuracy of the model on the 5835 samples: 88.6203941730934 %\n",
      "Epoch [967/1000], Step [100/137], Loss: 0.2876\n",
      "Accuracy of the model on the 17505 samples: 89.19165952584976 %\n",
      "Accuracy of the model on the 5835 samples: 88.24335904027421 %\n",
      "Accuracy of the model on the 5835 samples: 88.67180805484148 %\n",
      "Epoch [968/1000], Step [100/137], Loss: 0.2629\n",
      "Accuracy of the model on the 17505 samples: 89.75721222507855 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Accuracy of the model on the 5835 samples: 88.94601542416453 %\n",
      "Epoch [969/1000], Step [100/137], Loss: 0.2525\n",
      "Accuracy of the model on the 17505 samples: 89.91716652385033 %\n",
      "Accuracy of the model on the 5835 samples: 89.18594687232219 %\n",
      "Accuracy of the model on the 5835 samples: 89.04884318766067 %\n",
      "Epoch [970/1000], Step [100/137], Loss: 0.2379\n",
      "Accuracy of the model on the 17505 samples: 90.08283347614967 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Accuracy of the model on the 5835 samples: 89.40874035989717 %\n",
      "Epoch [971/1000], Step [100/137], Loss: 0.2476\n",
      "Accuracy of the model on the 17505 samples: 89.71151099685804 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Accuracy of the model on the 5835 samples: 88.68894601542416 %\n",
      "Epoch [972/1000], Step [100/137], Loss: 0.2495\n",
      "Accuracy of the model on the 17505 samples: 90.10568409025993 %\n",
      "Accuracy of the model on the 5835 samples: 89.64867180805484 %\n",
      "Accuracy of the model on the 5835 samples: 89.39160239931448 %\n",
      "Epoch [973/1000], Step [100/137], Loss: 0.2500\n",
      "Accuracy of the model on the 17505 samples: 89.86575264210225 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Epoch [974/1000], Step [100/137], Loss: 0.2616\n",
      "Accuracy of the model on the 17505 samples: 89.96286775207084 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 88.91173950299914 %\n",
      "Epoch [975/1000], Step [100/137], Loss: 0.2716\n",
      "Accuracy of the model on the 17505 samples: 89.42587832047987 %\n",
      "Accuracy of the model on the 5835 samples: 88.4318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.808911739503 %\n",
      "Epoch [976/1000], Step [100/137], Loss: 0.3345\n",
      "Accuracy of the model on the 17505 samples: 87.4150242787775 %\n",
      "Accuracy of the model on the 5835 samples: 86.47814910025707 %\n",
      "Accuracy of the model on the 5835 samples: 86.75235646958012 %\n",
      "Epoch [977/1000], Step [100/137], Loss: 0.3086\n",
      "Accuracy of the model on the 17505 samples: 88.03199085975436 %\n",
      "Accuracy of the model on the 5835 samples: 87.36932305055699 %\n",
      "Accuracy of the model on the 5835 samples: 87.24935732647815 %\n",
      "Epoch [978/1000], Step [100/137], Loss: 0.2950\n",
      "Accuracy of the model on the 17505 samples: 88.10054270208512 %\n",
      "Accuracy of the model on the 5835 samples: 87.38646101113967 %\n",
      "Accuracy of the model on the 5835 samples: 87.36932305055699 %\n",
      "Epoch [979/1000], Step [100/137], Loss: 0.2843\n",
      "Accuracy of the model on the 17505 samples: 88.28906026849472 %\n",
      "Accuracy of the model on the 5835 samples: 87.38646101113967 %\n",
      "Accuracy of the model on the 5835 samples: 87.64353041988004 %\n",
      "Epoch [980/1000], Step [100/137], Loss: 0.2740\n",
      "Accuracy of the model on the 17505 samples: 88.46615252784919 %\n",
      "Accuracy of the model on the 5835 samples: 87.62639245929735 %\n",
      "Accuracy of the model on the 5835 samples: 87.78063410454156 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [981/1000], Step [100/137], Loss: 0.2693\n",
      "Accuracy of the model on the 17505 samples: 88.55184233076264 %\n",
      "Accuracy of the model on the 5835 samples: 87.5235646958012 %\n",
      "Accuracy of the model on the 5835 samples: 87.83204798628964 %\n",
      "Epoch [982/1000], Step [100/137], Loss: 0.2650\n",
      "Accuracy of the model on the 17505 samples: 88.92316481005427 %\n",
      "Accuracy of the model on the 5835 samples: 88.03770351328193 %\n",
      "Accuracy of the model on the 5835 samples: 88.17480719794345 %\n",
      "Epoch [983/1000], Step [100/137], Loss: 0.2629\n",
      "Accuracy of the model on the 17505 samples: 88.97457869180234 %\n",
      "Accuracy of the model on the 5835 samples: 88.15766923736075 %\n",
      "Accuracy of the model on the 5835 samples: 88.0719794344473 %\n",
      "Epoch [984/1000], Step [100/137], Loss: 0.2583\n",
      "Accuracy of the model on the 17505 samples: 89.01456726649529 %\n",
      "Accuracy of the model on the 5835 samples: 88.4318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.48329048843188 %\n",
      "Epoch [985/1000], Step [100/137], Loss: 0.2626\n",
      "Accuracy of the model on the 17505 samples: 89.11739502999143 %\n",
      "Accuracy of the model on the 5835 samples: 88.36332476435304 %\n",
      "Accuracy of the model on the 5835 samples: 88.58611825192801 %\n",
      "Epoch [986/1000], Step [100/137], Loss: 0.2594\n",
      "Accuracy of the model on the 17505 samples: 89.19165952584976 %\n",
      "Accuracy of the model on the 5835 samples: 88.39760068551843 %\n",
      "Accuracy of the model on the 5835 samples: 88.50042844901456 %\n",
      "Epoch [987/1000], Step [100/137], Loss: 0.2490\n",
      "Accuracy of the model on the 17505 samples: 89.3573264781491 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Accuracy of the model on the 5835 samples: 88.86032562125106 %\n",
      "Epoch [988/1000], Step [100/137], Loss: 0.2536\n",
      "Accuracy of the model on the 17505 samples: 89.4830048557555 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Accuracy of the model on the 5835 samples: 88.92887746358183 %\n",
      "Epoch [989/1000], Step [100/137], Loss: 0.2524\n",
      "Accuracy of the model on the 17505 samples: 89.54584404455869 %\n",
      "Accuracy of the model on the 5835 samples: 88.808911739503 %\n",
      "Accuracy of the model on the 5835 samples: 88.9802913453299 %\n",
      "Epoch [990/1000], Step [100/137], Loss: 0.2455\n",
      "Accuracy of the model on the 17505 samples: 89.18023421879462 %\n",
      "Accuracy of the model on the 5835 samples: 88.50042844901456 %\n",
      "Accuracy of the model on the 5835 samples: 88.50042844901456 %\n",
      "Epoch [991/1000], Step [100/137], Loss: 0.2409\n",
      "Accuracy of the model on the 17505 samples: 89.5629820051414 %\n",
      "Accuracy of the model on the 5835 samples: 89.08311910882605 %\n",
      "Accuracy of the model on the 5835 samples: 88.84318766066838 %\n",
      "Epoch [992/1000], Step [100/137], Loss: 0.2516\n",
      "Accuracy of the model on the 17505 samples: 89.65438446158241 %\n",
      "Accuracy of the model on the 5835 samples: 88.96315338474722 %\n",
      "Accuracy of the model on the 5835 samples: 88.75749785775493 %\n",
      "Epoch [993/1000], Step [100/137], Loss: 0.2490\n",
      "Accuracy of the model on the 17505 samples: 89.59725792630677 %\n",
      "Accuracy of the model on the 5835 samples: 88.77463581833761 %\n",
      "Accuracy of the model on the 5835 samples: 88.99742930591259 %\n",
      "Epoch [994/1000], Step [100/137], Loss: 0.3771\n",
      "Accuracy of the model on the 17505 samples: 87.32362182233648 %\n",
      "Accuracy of the model on the 5835 samples: 86.71808054841473 %\n",
      "Accuracy of the model on the 5835 samples: 86.30676949443016 %\n",
      "Epoch [995/1000], Step [100/137], Loss: 0.3198\n",
      "Accuracy of the model on the 17505 samples: 87.62067980576978 %\n",
      "Accuracy of the model on the 5835 samples: 86.71808054841473 %\n",
      "Accuracy of the model on the 5835 samples: 86.70094258783205 %\n",
      "Epoch [996/1000], Step [100/137], Loss: 0.2859\n",
      "Accuracy of the model on the 17505 samples: 88.30619822907741 %\n",
      "Accuracy of the model on the 5835 samples: 87.48928877463582 %\n",
      "Accuracy of the model on the 5835 samples: 87.5235646958012 %\n",
      "Epoch [997/1000], Step [100/137], Loss: 0.2715\n",
      "Accuracy of the model on the 17505 samples: 88.7460725506998 %\n",
      "Accuracy of the model on the 5835 samples: 88.05484147386461 %\n",
      "Accuracy of the model on the 5835 samples: 88.03770351328193 %\n",
      "Epoch [998/1000], Step [100/137], Loss: 0.2986\n",
      "Accuracy of the model on the 17505 samples: 88.43758926021137 %\n",
      "Accuracy of the model on the 5835 samples: 87.60925449871465 %\n",
      "Accuracy of the model on the 5835 samples: 87.76349614395887 %\n",
      "Epoch [999/1000], Step [100/137], Loss: 0.2971\n",
      "Accuracy of the model on the 17505 samples: 88.70608397600685 %\n",
      "Accuracy of the model on the 5835 samples: 87.38646101113967 %\n",
      "Accuracy of the model on the 5835 samples: 87.83204798628964 %\n",
      "Epoch [1000/1000], Step [100/137], Loss: 0.2922\n",
      "Accuracy of the model on the 17505 samples: 89.05455584118823 %\n",
      "Accuracy of the model on the 5835 samples: 88.36332476435304 %\n",
      "Accuracy of the model on the 5835 samples: 88.20908311910883 %\n"
     ]
    }
   ],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_iterator)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_iterator):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            epochs[epoch] = {\"loss\": loss.item(), \"train\": eval_rnn(model, train_iterator), \n",
    "                             \"val\": eval_rnn(model, val_iterator), \"test\": eval_rnn(model, test_iterator)} \n",
    "            \n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872c0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in val_iterator:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Val Accuracy of the model on the {len(y_val)} val images: {100 * correct / total} %') \n",
    "            \n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_iterator:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy of the model on the {len(y_test)} test images: {100 * correct / total} %') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "86048f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "353a852e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hwixley/Documents/4th-Year/Honours-Project/localhost-data-preprocessing/ml-models/polar-lag0/viz/lstm-20epochs-2layers-128hidden.png'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "dum_input = torch.ones(1,28,28).to(device)\n",
    "a = model(dum_input)\n",
    "make_dot(a, params=dict(model.named_parameters())).render(f\"{save_dir}viz/lstm-{num_epochs}epochs-{num_layers}layers-{hidden_size}hidden\",format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27d017c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rnn = []\n",
    "val_rnn = []\n",
    "test_rnn = []\n",
    "loss_rnn = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    ep = epochs[i]\n",
    "    train_rnn.append(ep[\"train\"][2])\n",
    "    val_rnn.append(ep[\"val\"][2])\n",
    "    test_rnn.append(ep[\"test\"][2])\n",
    "    loss_rnn.append(ep[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "865f4c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch #')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(0,num_epochs,1)\n",
    "\n",
    "plt.plot(epochs, train_rnn, c=\"r\", lw=0.7, label=\"Train\")\n",
    "plt.plot(epochs, val_rnn, c=\"b\", lw=0.7, label=\"Validation\")\n",
    "plt.plot(epochs, test_rnn, c=\"g\", lw=0.7, label=\"Test\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.title(\"LSTM RNN (polar features, lag 0) - Epoch vs. Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add2e58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "413a5b95",
   "metadata": {},
   "source": [
    "# Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "526cc84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e83a06344a44b4ab5079d75b5b87fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss 0.5500611662864685\n",
      "epoch 1: loss 0.4101846218109131\n",
      "epoch 2: loss 0.5257068276405334\n",
      "epoch 3: loss 0.6833233833312988\n",
      "epoch 4: loss 0.45541390776634216\n",
      "epoch 5: loss 0.3435070216655731\n",
      "epoch 6: loss 0.40591639280319214\n",
      "epoch 7: loss 0.38531774282455444\n",
      "epoch 8: loss 0.474290132522583\n",
      "epoch 9: loss 0.49090608954429626\n",
      "epoch 10: loss 0.4676029086112976\n",
      "epoch 11: loss 0.4214766025543213\n",
      "epoch 12: loss 0.38312554359436035\n",
      "epoch 13: loss 0.5094064474105835\n",
      "epoch 14: loss 0.4004752039909363\n",
      "epoch 15: loss 0.4089033007621765\n",
      "epoch 16: loss 0.37904679775238037\n",
      "epoch 17: loss 0.4463964104652405\n",
      "epoch 18: loss 0.486097514629364\n",
      "epoch 19: loss 0.3884008824825287\n",
      "epoch 20: loss 0.4015762507915497\n",
      "epoch 21: loss 0.3920731544494629\n",
      "epoch 22: loss 0.4037277102470398\n",
      "epoch 23: loss 0.45267534255981445\n",
      "epoch 24: loss 0.40353846549987793\n",
      "epoch 25: loss 0.46682465076446533\n",
      "epoch 26: loss 0.4019089639186859\n",
      "epoch 27: loss 0.4145364761352539\n",
      "epoch 28: loss 0.5167216062545776\n",
      "epoch 29: loss 0.36968329548835754\n",
      "epoch 30: loss 0.3848176896572113\n",
      "epoch 31: loss 0.4135960042476654\n",
      "epoch 32: loss 0.3977789282798767\n",
      "epoch 33: loss 0.34366029500961304\n",
      "epoch 34: loss 0.37479761242866516\n",
      "epoch 35: loss 0.34943702816963196\n",
      "epoch 36: loss 0.3588157296180725\n",
      "epoch 37: loss 0.4029639959335327\n",
      "epoch 38: loss 0.2813543677330017\n",
      "epoch 39: loss 0.4079987108707428\n",
      "epoch 40: loss 0.4512866735458374\n",
      "epoch 41: loss 0.37656837701797485\n",
      "epoch 42: loss 0.3721229135990143\n",
      "epoch 43: loss 0.4054657816886902\n",
      "epoch 44: loss 0.4118296802043915\n",
      "epoch 45: loss 0.3724376857280731\n",
      "epoch 46: loss 0.41029804944992065\n",
      "epoch 47: loss 0.36296725273132324\n",
      "epoch 48: loss 0.40426546335220337\n",
      "epoch 49: loss 0.365716814994812\n",
      "epoch 50: loss 0.30640292167663574\n",
      "epoch 51: loss 0.41883790493011475\n",
      "epoch 52: loss 0.37465932965278625\n",
      "epoch 53: loss 0.4375658333301544\n",
      "epoch 54: loss 0.3599379062652588\n",
      "epoch 55: loss 0.36987221240997314\n",
      "epoch 56: loss 0.3874998092651367\n",
      "epoch 57: loss 0.29794272780418396\n",
      "epoch 58: loss 0.36522284150123596\n",
      "epoch 59: loss 0.3216931223869324\n",
      "epoch 60: loss 0.38118141889572144\n",
      "epoch 61: loss 0.29365625977516174\n",
      "epoch 62: loss 0.31151294708251953\n",
      "epoch 63: loss 0.3925743103027344\n",
      "epoch 64: loss 0.3795377016067505\n",
      "epoch 65: loss 0.4355829656124115\n",
      "epoch 66: loss 0.3738066554069519\n",
      "epoch 67: loss 0.39335089921951294\n",
      "epoch 68: loss 0.4083312749862671\n",
      "epoch 69: loss 0.5024024248123169\n",
      "epoch 70: loss 0.4676268398761749\n",
      "epoch 71: loss 0.38431641459465027\n",
      "epoch 72: loss 0.3582901656627655\n",
      "epoch 73: loss 0.40514078736305237\n",
      "epoch 74: loss 0.37149935960769653\n",
      "epoch 75: loss 0.38350751996040344\n",
      "epoch 76: loss 0.335703045129776\n",
      "epoch 77: loss 0.4274689555168152\n",
      "epoch 78: loss 0.3709566593170166\n",
      "epoch 79: loss 0.42633557319641113\n",
      "epoch 80: loss 0.4328247904777527\n",
      "epoch 81: loss 0.36564022302627563\n",
      "epoch 82: loss 0.4002974033355713\n",
      "epoch 83: loss 0.4664193391799927\n",
      "epoch 84: loss 0.3617461323738098\n",
      "epoch 85: loss 0.35800325870513916\n",
      "epoch 86: loss 0.41656243801116943\n",
      "epoch 87: loss 0.346891313791275\n",
      "epoch 88: loss 0.32588648796081543\n",
      "epoch 89: loss 0.4428855776786804\n",
      "epoch 90: loss 0.32323306798934937\n",
      "epoch 91: loss 0.4424225091934204\n",
      "epoch 92: loss 0.40289127826690674\n",
      "epoch 93: loss 0.4039493501186371\n",
      "epoch 94: loss 0.34972986578941345\n",
      "epoch 95: loss 0.2752789556980133\n",
      "epoch 96: loss 0.32357072830200195\n",
      "epoch 97: loss 0.4313086271286011\n",
      "epoch 98: loss 0.40793490409851074\n",
      "epoch 99: loss 0.380817174911499\n",
      "epoch 100: loss 0.4810338020324707\n",
      "epoch 101: loss 0.459532767534256\n",
      "epoch 102: loss 0.3249705135822296\n",
      "epoch 103: loss 0.43061578273773193\n",
      "epoch 104: loss 0.3128044903278351\n",
      "epoch 105: loss 0.4158545732498169\n",
      "epoch 106: loss 0.3739381432533264\n",
      "epoch 107: loss 0.35869449377059937\n",
      "epoch 108: loss 0.34840720891952515\n",
      "epoch 109: loss 0.41590413451194763\n",
      "epoch 110: loss 0.37845492362976074\n",
      "epoch 111: loss 0.4328048825263977\n",
      "epoch 112: loss 0.43383970856666565\n",
      "epoch 113: loss 0.3394579291343689\n",
      "epoch 114: loss 0.21038368344306946\n",
      "epoch 115: loss 0.5187526941299438\n",
      "epoch 116: loss 0.3833526372909546\n",
      "epoch 117: loss 0.3348633944988251\n",
      "epoch 118: loss 0.3631499409675598\n",
      "epoch 119: loss 0.39397895336151123\n",
      "epoch 120: loss 0.35228073596954346\n",
      "epoch 121: loss 0.41512027382850647\n",
      "epoch 122: loss 0.3162965774536133\n",
      "epoch 123: loss 0.43849387764930725\n",
      "epoch 124: loss 0.3360416293144226\n",
      "epoch 125: loss 0.3276575207710266\n",
      "epoch 126: loss 0.41398125886917114\n",
      "epoch 127: loss 0.38604456186294556\n",
      "epoch 128: loss 0.33837583661079407\n",
      "epoch 129: loss 0.36511749029159546\n",
      "epoch 130: loss 0.42030441761016846\n",
      "epoch 131: loss 0.3524891138076782\n",
      "epoch 132: loss 0.3727301061153412\n",
      "epoch 133: loss 0.39543676376342773\n",
      "epoch 134: loss 0.33743423223495483\n",
      "epoch 135: loss 0.31471726298332214\n",
      "epoch 136: loss 0.26545897126197815\n",
      "epoch 0: loss 0.41562357544898987\n",
      "epoch 1: loss 0.3892553448677063\n",
      "epoch 2: loss 0.4413774013519287\n",
      "epoch 3: loss 0.4393501281738281\n",
      "epoch 4: loss 0.44035273790359497\n",
      "epoch 5: loss 0.340267539024353\n",
      "epoch 6: loss 0.3338242173194885\n",
      "epoch 7: loss 0.33047348260879517\n",
      "epoch 8: loss 0.5790836811065674\n",
      "epoch 9: loss 0.520073652267456\n",
      "epoch 10: loss 0.39335963129997253\n",
      "epoch 11: loss 0.4766322374343872\n",
      "epoch 12: loss 0.43465304374694824\n",
      "epoch 13: loss 0.4970962107181549\n",
      "epoch 14: loss 0.3923400044441223\n",
      "epoch 15: loss 0.4208395481109619\n",
      "epoch 16: loss 0.38020050525665283\n",
      "epoch 17: loss 0.4215856194496155\n",
      "epoch 18: loss 0.43706321716308594\n",
      "epoch 19: loss 0.39356523752212524\n",
      "epoch 20: loss 0.41818252205848694\n",
      "epoch 21: loss 0.38803571462631226\n",
      "epoch 22: loss 0.37611210346221924\n",
      "epoch 23: loss 0.44685429334640503\n",
      "epoch 24: loss 0.4043455719947815\n",
      "epoch 25: loss 0.4686537981033325\n",
      "epoch 26: loss 0.3756116032600403\n",
      "epoch 27: loss 0.3948824405670166\n",
      "epoch 28: loss 0.47018980979919434\n",
      "epoch 29: loss 0.38499346375465393\n",
      "epoch 30: loss 0.3727557063102722\n",
      "epoch 31: loss 0.38874977827072144\n",
      "epoch 32: loss 0.3714969754219055\n",
      "epoch 33: loss 0.336248517036438\n",
      "epoch 34: loss 0.37489956617355347\n",
      "epoch 35: loss 0.322875440120697\n",
      "epoch 36: loss 0.3547969460487366\n",
      "epoch 37: loss 0.3799179792404175\n",
      "epoch 38: loss 0.2569319009780884\n",
      "epoch 39: loss 0.375590980052948\n",
      "epoch 40: loss 0.39605075120925903\n",
      "epoch 41: loss 0.34458184242248535\n",
      "epoch 42: loss 0.34322303533554077\n",
      "epoch 43: loss 0.3612554967403412\n",
      "epoch 44: loss 0.3956194519996643\n",
      "epoch 45: loss 0.33796969056129456\n",
      "epoch 46: loss 0.3799287676811218\n",
      "epoch 47: loss 0.3244401812553406\n",
      "epoch 48: loss 0.36197012662887573\n",
      "epoch 49: loss 0.3419528603553772\n",
      "epoch 50: loss 0.3023948669433594\n",
      "epoch 51: loss 0.4107832908630371\n",
      "epoch 52: loss 0.3413596749305725\n",
      "epoch 53: loss 0.41059350967407227\n",
      "epoch 54: loss 0.3285835385322571\n",
      "epoch 55: loss 0.33444744348526\n",
      "epoch 56: loss 0.3558047413825989\n",
      "epoch 57: loss 0.2767162024974823\n",
      "epoch 58: loss 0.35299116373062134\n",
      "epoch 59: loss 0.27750468254089355\n",
      "epoch 60: loss 0.37686654925346375\n",
      "epoch 61: loss 0.2543303966522217\n",
      "epoch 62: loss 0.27385303378105164\n",
      "epoch 63: loss 0.3718896508216858\n",
      "epoch 64: loss 0.34326040744781494\n",
      "epoch 65: loss 0.41929396986961365\n",
      "epoch 66: loss 0.3583124577999115\n",
      "epoch 67: loss 0.3849397301673889\n",
      "epoch 68: loss 0.38846850395202637\n",
      "epoch 69: loss 0.4624715745449066\n",
      "epoch 70: loss 0.4187244176864624\n",
      "epoch 71: loss 0.402082622051239\n",
      "epoch 72: loss 0.29647600650787354\n",
      "epoch 73: loss 0.412761926651001\n",
      "epoch 74: loss 0.3589135408401489\n",
      "epoch 75: loss 0.3528202772140503\n",
      "epoch 76: loss 0.32003194093704224\n",
      "epoch 77: loss 0.3963201344013214\n",
      "epoch 78: loss 0.35785964131355286\n",
      "epoch 79: loss 0.3781532049179077\n",
      "epoch 80: loss 0.4087565541267395\n",
      "epoch 81: loss 0.306918740272522\n",
      "epoch 82: loss 0.3680616021156311\n",
      "epoch 83: loss 0.4068390429019928\n",
      "epoch 84: loss 0.3172417879104614\n",
      "epoch 85: loss 0.3307710587978363\n",
      "epoch 86: loss 0.3946518898010254\n",
      "epoch 87: loss 0.3382912874221802\n",
      "epoch 88: loss 0.2901988923549652\n",
      "epoch 89: loss 0.378291517496109\n",
      "epoch 90: loss 0.28944647312164307\n",
      "epoch 91: loss 0.3750566244125366\n",
      "epoch 92: loss 0.33384910225868225\n",
      "epoch 93: loss 0.33513402938842773\n",
      "epoch 94: loss 0.319179892539978\n",
      "epoch 95: loss 0.23267501592636108\n",
      "epoch 96: loss 0.26150548458099365\n",
      "epoch 97: loss 0.32479503750801086\n",
      "epoch 98: loss 0.3244009017944336\n",
      "epoch 99: loss 0.38096320629119873\n",
      "epoch 100: loss 0.4416350722312927\n",
      "epoch 101: loss 0.33313050866127014\n",
      "epoch 102: loss 0.28974881768226624\n",
      "epoch 103: loss 0.3689355254173279\n",
      "epoch 104: loss 0.2851436138153076\n",
      "epoch 105: loss 0.36528587341308594\n",
      "epoch 106: loss 0.3457059860229492\n",
      "epoch 107: loss 0.32837337255477905\n",
      "epoch 108: loss 0.3491736650466919\n",
      "epoch 109: loss 0.39666980504989624\n",
      "epoch 110: loss 0.3505210280418396\n",
      "epoch 111: loss 0.3871159553527832\n",
      "epoch 112: loss 0.3899097442626953\n",
      "epoch 113: loss 0.325374573469162\n",
      "epoch 114: loss 0.17745506763458252\n",
      "epoch 115: loss 0.6004148125648499\n",
      "epoch 116: loss 0.37682050466537476\n",
      "epoch 117: loss 0.3051897883415222\n",
      "epoch 118: loss 0.3685966730117798\n",
      "epoch 119: loss 0.3868662118911743\n",
      "epoch 120: loss 0.3080548644065857\n",
      "epoch 121: loss 0.4328780174255371\n",
      "epoch 122: loss 0.29475778341293335\n",
      "epoch 123: loss 0.437458872795105\n",
      "epoch 124: loss 0.32206666469573975\n",
      "epoch 125: loss 0.34678786993026733\n",
      "epoch 126: loss 0.382226824760437\n",
      "epoch 127: loss 0.3902853727340698\n",
      "epoch 128: loss 0.30376940965652466\n",
      "epoch 129: loss 0.35308146476745605\n",
      "epoch 130: loss 0.4213945269584656\n",
      "epoch 131: loss 0.3413420617580414\n",
      "epoch 132: loss 0.3511638045310974\n",
      "epoch 133: loss 0.3672167956829071\n",
      "epoch 134: loss 0.3347057104110718\n",
      "epoch 135: loss 0.28002989292144775\n",
      "epoch 136: loss 0.24134492874145508\n",
      "epoch 0: loss 0.43386751413345337\n",
      "epoch 1: loss 0.37709900736808777\n",
      "epoch 2: loss 0.4299114942550659\n",
      "epoch 3: loss 0.4252927899360657\n",
      "epoch 4: loss 0.407289981842041\n",
      "epoch 5: loss 0.29029208421707153\n",
      "epoch 6: loss 0.3280453681945801\n",
      "epoch 7: loss 0.3108798861503601\n",
      "epoch 8: loss 0.5348511934280396\n",
      "epoch 9: loss 0.4783448576927185\n",
      "epoch 10: loss 0.37440669536590576\n",
      "epoch 11: loss 0.417904794216156\n",
      "epoch 12: loss 0.42995116114616394\n",
      "epoch 13: loss 0.4577605128288269\n",
      "epoch 14: loss 0.3664497137069702\n",
      "epoch 15: loss 0.38348937034606934\n",
      "epoch 16: loss 0.3695452809333801\n",
      "epoch 17: loss 0.4095052480697632\n",
      "epoch 18: loss 0.42983725666999817\n",
      "epoch 19: loss 0.34520819783210754\n",
      "epoch 20: loss 0.38640066981315613\n",
      "epoch 21: loss 0.3835626542568207\n",
      "epoch 22: loss 0.358295738697052\n",
      "epoch 23: loss 0.413644015789032\n",
      "epoch 24: loss 0.3879024386405945\n",
      "epoch 25: loss 0.43078121542930603\n",
      "epoch 26: loss 0.3454280495643616\n",
      "epoch 27: loss 0.3779979944229126\n",
      "epoch 28: loss 0.431793749332428\n",
      "epoch 29: loss 0.36516278982162476\n",
      "epoch 30: loss 0.31245094537734985\n",
      "epoch 31: loss 0.36765262484550476\n",
      "epoch 32: loss 0.3607507050037384\n",
      "epoch 33: loss 0.3384028375148773\n",
      "epoch 34: loss 0.37302976846694946\n",
      "epoch 35: loss 0.27744340896606445\n",
      "epoch 36: loss 0.34168240427970886\n",
      "epoch 37: loss 0.3285309970378876\n",
      "epoch 38: loss 0.25290679931640625\n",
      "epoch 39: loss 0.3462119400501251\n",
      "epoch 40: loss 0.35084712505340576\n",
      "epoch 41: loss 0.28868991136550903\n",
      "epoch 42: loss 0.30141979455947876\n",
      "epoch 43: loss 0.3219616413116455\n",
      "epoch 44: loss 0.38219720125198364\n",
      "epoch 45: loss 0.29243791103363037\n",
      "epoch 46: loss 0.347064733505249\n",
      "epoch 47: loss 0.2965679168701172\n",
      "epoch 48: loss 0.3212299048900604\n",
      "epoch 49: loss 0.3156735301017761\n",
      "epoch 50: loss 0.2831960916519165\n",
      "epoch 51: loss 0.4314281940460205\n",
      "epoch 52: loss 0.3119754195213318\n",
      "epoch 53: loss 0.3847199082374573\n",
      "epoch 54: loss 0.3420693278312683\n",
      "epoch 55: loss 0.31468465924263\n",
      "epoch 56: loss 0.3462786078453064\n",
      "epoch 57: loss 0.25129586458206177\n",
      "epoch 58: loss 0.3766941428184509\n",
      "epoch 59: loss 0.24974319338798523\n",
      "epoch 60: loss 0.35974806547164917\n",
      "epoch 61: loss 0.24643179774284363\n",
      "epoch 62: loss 0.2614600956439972\n",
      "epoch 63: loss 0.3711397051811218\n",
      "epoch 64: loss 0.3189236521720886\n",
      "epoch 65: loss 0.4251621961593628\n",
      "epoch 66: loss 0.3603718876838684\n",
      "epoch 67: loss 0.3945130705833435\n",
      "epoch 68: loss 0.3797357976436615\n",
      "epoch 69: loss 0.4454071521759033\n",
      "epoch 70: loss 0.4061134457588196\n",
      "epoch 71: loss 0.36032113432884216\n",
      "epoch 72: loss 0.28574883937835693\n",
      "epoch 73: loss 0.38178086280822754\n",
      "epoch 74: loss 0.32007601857185364\n",
      "epoch 75: loss 0.32848334312438965\n",
      "epoch 76: loss 0.3152245283126831\n",
      "epoch 77: loss 0.3948216140270233\n",
      "epoch 78: loss 0.33063027262687683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 79: loss 0.36564406752586365\n",
      "epoch 80: loss 0.39578962326049805\n",
      "epoch 81: loss 0.2856045961380005\n",
      "epoch 82: loss 0.352457731962204\n",
      "epoch 83: loss 0.3973473310470581\n",
      "epoch 84: loss 0.28241315484046936\n",
      "epoch 85: loss 0.32330116629600525\n",
      "epoch 86: loss 0.3842965364456177\n",
      "epoch 87: loss 0.3445189595222473\n",
      "epoch 88: loss 0.2811815142631531\n",
      "epoch 89: loss 0.3566948175430298\n",
      "epoch 90: loss 0.2664194703102112\n",
      "epoch 91: loss 0.3694326877593994\n",
      "epoch 92: loss 0.307528018951416\n",
      "epoch 93: loss 0.3611753582954407\n",
      "epoch 94: loss 0.3142728805541992\n",
      "epoch 95: loss 0.21857795119285583\n",
      "epoch 96: loss 0.2647843658924103\n",
      "epoch 97: loss 0.3308212161064148\n",
      "epoch 98: loss 0.31871142983436584\n",
      "epoch 99: loss 0.38119810819625854\n",
      "epoch 100: loss 0.4576214551925659\n",
      "epoch 101: loss 0.3359769582748413\n",
      "epoch 102: loss 0.2869722843170166\n",
      "epoch 103: loss 0.3864065408706665\n",
      "epoch 104: loss 0.2880285382270813\n",
      "epoch 105: loss 0.3738902509212494\n",
      "epoch 106: loss 0.3408002257347107\n",
      "epoch 107: loss 0.3445669412612915\n",
      "epoch 108: loss 0.35944610834121704\n",
      "epoch 109: loss 0.3770411014556885\n",
      "epoch 110: loss 0.3550506830215454\n",
      "epoch 111: loss 0.415988028049469\n",
      "epoch 112: loss 0.3830938935279846\n",
      "epoch 113: loss 0.33501505851745605\n",
      "epoch 114: loss 0.2529577314853668\n",
      "epoch 115: loss 0.4435104429721832\n",
      "epoch 116: loss 0.336862713098526\n",
      "epoch 117: loss 0.30526548624038696\n",
      "epoch 118: loss 0.33227312564849854\n",
      "epoch 119: loss 0.36999064683914185\n",
      "epoch 120: loss 0.31019869446754456\n",
      "epoch 121: loss 0.3973316550254822\n",
      "epoch 122: loss 0.2902781665325165\n",
      "epoch 123: loss 0.4164966344833374\n",
      "epoch 124: loss 0.3134250044822693\n",
      "epoch 125: loss 0.32835036516189575\n",
      "epoch 126: loss 0.3712272047996521\n",
      "epoch 127: loss 0.394316166639328\n",
      "epoch 128: loss 0.3088925778865814\n",
      "epoch 129: loss 0.34793275594711304\n",
      "epoch 130: loss 0.4105274975299835\n",
      "epoch 131: loss 0.3358994126319885\n",
      "epoch 132: loss 0.3414222002029419\n",
      "epoch 133: loss 0.36358168721199036\n",
      "epoch 134: loss 0.3396339416503906\n",
      "epoch 135: loss 0.2774578630924225\n",
      "epoch 136: loss 0.24015355110168457\n",
      "epoch 0: loss 0.4252716898918152\n",
      "epoch 1: loss 0.38168177008628845\n",
      "epoch 2: loss 0.45132869482040405\n",
      "epoch 3: loss 0.39970940351486206\n",
      "epoch 4: loss 0.4033079743385315\n",
      "epoch 5: loss 0.3202618360519409\n",
      "epoch 6: loss 0.30199795961380005\n",
      "epoch 7: loss 0.275555819272995\n",
      "epoch 8: loss 0.5120306015014648\n",
      "epoch 9: loss 0.498076468706131\n",
      "epoch 10: loss 0.3752133846282959\n",
      "epoch 11: loss 0.34500378370285034\n",
      "epoch 12: loss 0.4279076159000397\n",
      "epoch 13: loss 0.44964495301246643\n",
      "epoch 14: loss 0.3584677577018738\n",
      "epoch 15: loss 0.3470803499221802\n",
      "epoch 16: loss 0.3588736057281494\n",
      "epoch 17: loss 0.4058326482772827\n",
      "epoch 18: loss 0.4441126883029938\n",
      "epoch 19: loss 0.3078930974006653\n",
      "epoch 20: loss 0.35003215074539185\n",
      "epoch 21: loss 0.3802761435508728\n",
      "epoch 22: loss 0.3490177094936371\n",
      "epoch 23: loss 0.3936687111854553\n",
      "epoch 24: loss 0.37550845742225647\n",
      "epoch 25: loss 0.41297030448913574\n",
      "epoch 26: loss 0.3349568843841553\n",
      "epoch 27: loss 0.36770570278167725\n",
      "epoch 28: loss 0.41433703899383545\n",
      "epoch 29: loss 0.3581849932670593\n",
      "epoch 30: loss 0.2983052730560303\n",
      "epoch 31: loss 0.35176271200180054\n",
      "epoch 32: loss 0.34629887342453003\n",
      "epoch 33: loss 0.34362858533859253\n",
      "epoch 34: loss 0.3784097135066986\n",
      "epoch 35: loss 0.2681295871734619\n",
      "epoch 36: loss 0.3407239317893982\n",
      "epoch 37: loss 0.324091374874115\n",
      "epoch 38: loss 0.248769149184227\n",
      "epoch 39: loss 0.3384273052215576\n",
      "epoch 40: loss 0.3301311433315277\n",
      "epoch 41: loss 0.28056544065475464\n",
      "epoch 42: loss 0.28807348012924194\n",
      "epoch 43: loss 0.31150853633880615\n",
      "epoch 44: loss 0.38696539402008057\n",
      "epoch 45: loss 0.28364068269729614\n",
      "epoch 46: loss 0.34280699491500854\n",
      "epoch 47: loss 0.29576873779296875\n",
      "epoch 48: loss 0.3064221739768982\n",
      "epoch 49: loss 0.3142876923084259\n",
      "epoch 50: loss 0.2832571864128113\n",
      "epoch 51: loss 0.4380830228328705\n",
      "epoch 52: loss 0.3104875087738037\n",
      "epoch 53: loss 0.3827779293060303\n",
      "epoch 54: loss 0.3480585515499115\n",
      "epoch 55: loss 0.31077125668525696\n",
      "epoch 56: loss 0.34516242146492004\n",
      "epoch 57: loss 0.24662670493125916\n",
      "epoch 58: loss 0.37234562635421753\n",
      "epoch 59: loss 0.25685593485832214\n",
      "epoch 60: loss 0.36184385418891907\n",
      "epoch 61: loss 0.2399880290031433\n",
      "epoch 62: loss 0.26361411809921265\n",
      "epoch 63: loss 0.3659607768058777\n",
      "epoch 64: loss 0.321707546710968\n",
      "epoch 65: loss 0.4172872304916382\n",
      "epoch 66: loss 0.3535836338996887\n",
      "epoch 67: loss 0.3866170048713684\n",
      "epoch 68: loss 0.3780438303947449\n",
      "epoch 69: loss 0.43730294704437256\n",
      "epoch 70: loss 0.4047107696533203\n",
      "epoch 71: loss 0.36744144558906555\n",
      "epoch 72: loss 0.28097856044769287\n",
      "epoch 73: loss 0.3781047463417053\n",
      "epoch 74: loss 0.3210141062736511\n",
      "epoch 75: loss 0.33243030309677124\n",
      "epoch 76: loss 0.31280553340911865\n",
      "epoch 77: loss 0.3964616060256958\n",
      "epoch 78: loss 0.3320706784725189\n",
      "epoch 79: loss 0.35897886753082275\n",
      "epoch 80: loss 0.39216575026512146\n",
      "epoch 81: loss 0.28140783309936523\n",
      "epoch 82: loss 0.3513776659965515\n",
      "epoch 83: loss 0.3955671191215515\n",
      "epoch 84: loss 0.27896901965141296\n",
      "epoch 85: loss 0.32369476556777954\n",
      "epoch 86: loss 0.37099945545196533\n",
      "epoch 87: loss 0.34351998567581177\n",
      "epoch 88: loss 0.2764926254749298\n",
      "epoch 89: loss 0.3563900589942932\n",
      "epoch 90: loss 0.2628313899040222\n",
      "epoch 91: loss 0.35142281651496887\n",
      "epoch 92: loss 0.30407050251960754\n",
      "epoch 93: loss 0.34604597091674805\n",
      "epoch 94: loss 0.30838483572006226\n",
      "epoch 95: loss 0.21152323484420776\n",
      "epoch 96: loss 0.2585983872413635\n",
      "epoch 97: loss 0.3230053782463074\n",
      "epoch 98: loss 0.3212204575538635\n",
      "epoch 99: loss 0.36586183309555054\n",
      "epoch 100: loss 0.47126108407974243\n",
      "epoch 101: loss 0.3622305393218994\n",
      "epoch 102: loss 0.27889761328697205\n",
      "epoch 103: loss 0.382912278175354\n",
      "epoch 104: loss 0.2943938374519348\n",
      "epoch 105: loss 0.4002191424369812\n",
      "epoch 106: loss 0.3452041745185852\n",
      "epoch 107: loss 0.3182906210422516\n",
      "epoch 108: loss 0.3733949065208435\n",
      "epoch 109: loss 0.3743045926094055\n",
      "epoch 110: loss 0.34903645515441895\n",
      "epoch 111: loss 0.4026413559913635\n",
      "epoch 112: loss 0.3996215760707855\n",
      "epoch 113: loss 0.31014081835746765\n",
      "epoch 114: loss 0.2004554569721222\n",
      "epoch 115: loss 0.4474363923072815\n",
      "epoch 116: loss 0.32753434777259827\n",
      "epoch 117: loss 0.30351704359054565\n",
      "epoch 118: loss 0.33153194189071655\n",
      "epoch 119: loss 0.3695600628852844\n",
      "epoch 120: loss 0.2955651581287384\n",
      "epoch 121: loss 0.40641695261001587\n",
      "epoch 122: loss 0.2797744870185852\n",
      "epoch 123: loss 0.4111226499080658\n",
      "epoch 124: loss 0.315932035446167\n",
      "epoch 125: loss 0.33524757623672485\n",
      "epoch 126: loss 0.36618101596832275\n",
      "epoch 127: loss 0.3991889953613281\n",
      "epoch 128: loss 0.29047656059265137\n",
      "epoch 129: loss 0.3466532230377197\n",
      "epoch 130: loss 0.40255171060562134\n",
      "epoch 131: loss 0.3371550738811493\n",
      "epoch 132: loss 0.33605509996414185\n",
      "epoch 133: loss 0.3617308735847473\n",
      "epoch 134: loss 0.3278439939022064\n",
      "epoch 135: loss 0.26567745208740234\n",
      "epoch 136: loss 0.2356809824705124\n",
      "epoch 0: loss 0.42352262139320374\n",
      "epoch 1: loss 0.36554786562919617\n",
      "epoch 2: loss 0.4247745871543884\n",
      "epoch 3: loss 0.42065900564193726\n",
      "epoch 4: loss 0.39652636647224426\n",
      "epoch 5: loss 0.279571533203125\n",
      "epoch 6: loss 0.31886520981788635\n",
      "epoch 7: loss 0.29071348905563354\n",
      "epoch 8: loss 0.5211343765258789\n",
      "epoch 9: loss 0.4674528241157532\n",
      "epoch 10: loss 0.36587709188461304\n",
      "epoch 11: loss 0.4246823191642761\n",
      "epoch 12: loss 0.3743531107902527\n",
      "epoch 13: loss 0.4716842770576477\n",
      "epoch 14: loss 0.3741481900215149\n",
      "epoch 15: loss 0.375842809677124\n",
      "epoch 16: loss 0.36224400997161865\n",
      "epoch 17: loss 0.38587233424186707\n",
      "epoch 18: loss 0.4094094932079315\n",
      "epoch 19: loss 0.3551265001296997\n",
      "epoch 20: loss 0.37532752752304077\n",
      "epoch 21: loss 0.37009117007255554\n",
      "epoch 22: loss 0.35252994298934937\n",
      "epoch 23: loss 0.4175427556037903\n",
      "epoch 24: loss 0.38940849900245667\n",
      "epoch 25: loss 0.42066890001296997\n",
      "epoch 26: loss 0.3379606604576111\n",
      "epoch 27: loss 0.37437838315963745\n",
      "epoch 28: loss 0.4236934185028076\n",
      "epoch 29: loss 0.35740602016448975\n",
      "epoch 30: loss 0.3087427616119385\n",
      "epoch 31: loss 0.3575837016105652\n",
      "epoch 32: loss 0.35061872005462646\n",
      "epoch 33: loss 0.33935967087745667\n",
      "epoch 34: loss 0.37529483437538147\n",
      "epoch 35: loss 0.2729669213294983\n",
      "epoch 36: loss 0.3389819860458374\n",
      "epoch 37: loss 0.32496050000190735\n",
      "epoch 38: loss 0.24754659831523895\n",
      "epoch 39: loss 0.3396114706993103\n",
      "epoch 40: loss 0.3353099524974823\n",
      "epoch 41: loss 0.28401538729667664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42: loss 0.2921815514564514\n",
      "epoch 43: loss 0.31520894169807434\n",
      "epoch 44: loss 0.38653764128685\n",
      "epoch 45: loss 0.28789716958999634\n",
      "epoch 46: loss 0.3416782021522522\n",
      "epoch 47: loss 0.29677248001098633\n",
      "epoch 48: loss 0.3119189739227295\n",
      "epoch 49: loss 0.31638723611831665\n",
      "epoch 50: loss 0.2827293276786804\n",
      "epoch 51: loss 0.4402240812778473\n",
      "epoch 52: loss 0.3160759210586548\n",
      "epoch 53: loss 0.3820888102054596\n",
      "epoch 54: loss 0.3428497910499573\n",
      "epoch 55: loss 0.31732887029647827\n",
      "epoch 56: loss 0.34542116522789\n",
      "epoch 57: loss 0.24890020489692688\n",
      "epoch 58: loss 0.37610310316085815\n",
      "epoch 59: loss 0.25484123826026917\n",
      "epoch 60: loss 0.3724595904350281\n",
      "epoch 61: loss 0.22840319573879242\n",
      "epoch 62: loss 0.2545928657054901\n",
      "epoch 63: loss 0.3641064763069153\n",
      "epoch 64: loss 0.32462894916534424\n",
      "epoch 65: loss 0.4136999845504761\n",
      "epoch 66: loss 0.3519056439399719\n",
      "epoch 67: loss 0.38571780920028687\n",
      "epoch 68: loss 0.37929767370224\n",
      "epoch 69: loss 0.44690877199172974\n",
      "epoch 70: loss 0.4054431915283203\n",
      "epoch 71: loss 0.3677211403846741\n",
      "epoch 72: loss 0.3023197650909424\n",
      "epoch 73: loss 0.3640386462211609\n",
      "epoch 74: loss 0.3157793879508972\n",
      "epoch 75: loss 0.3340185284614563\n",
      "epoch 76: loss 0.31343865394592285\n",
      "epoch 77: loss 0.41092008352279663\n",
      "epoch 78: loss 0.31710851192474365\n",
      "epoch 79: loss 0.36380288004875183\n",
      "epoch 80: loss 0.39258283376693726\n",
      "epoch 81: loss 0.30458468198776245\n",
      "epoch 82: loss 0.34964045882225037\n",
      "epoch 83: loss 0.406272292137146\n",
      "epoch 84: loss 0.2668452560901642\n",
      "epoch 85: loss 0.32104170322418213\n",
      "epoch 86: loss 0.3636159300804138\n",
      "epoch 87: loss 0.34360271692276\n",
      "epoch 88: loss 0.2827540636062622\n",
      "epoch 89: loss 0.3525146245956421\n",
      "epoch 90: loss 0.26164495944976807\n",
      "epoch 91: loss 0.3727276027202606\n",
      "epoch 92: loss 0.30278271436691284\n",
      "epoch 93: loss 0.3677799701690674\n",
      "epoch 94: loss 0.3107350468635559\n",
      "epoch 95: loss 0.2081950306892395\n",
      "epoch 96: loss 0.2611822485923767\n",
      "epoch 97: loss 0.34368789196014404\n",
      "epoch 98: loss 0.34031665325164795\n",
      "epoch 99: loss 0.3599049150943756\n",
      "epoch 100: loss 0.4738554358482361\n",
      "epoch 101: loss 0.4185073971748352\n",
      "epoch 102: loss 0.27701371908187866\n",
      "epoch 103: loss 0.37844419479370117\n",
      "epoch 104: loss 0.30304741859436035\n",
      "epoch 105: loss 0.42889440059661865\n",
      "epoch 106: loss 0.3717401623725891\n",
      "epoch 107: loss 0.32075464725494385\n",
      "epoch 108: loss 0.3445327877998352\n",
      "epoch 109: loss 0.3852893114089966\n",
      "epoch 110: loss 0.3772781491279602\n",
      "epoch 111: loss 0.40855562686920166\n",
      "epoch 112: loss 0.3973098397254944\n",
      "epoch 113: loss 0.3206418752670288\n",
      "epoch 114: loss 0.19863444566726685\n",
      "epoch 115: loss 0.48257189989089966\n",
      "epoch 116: loss 0.3548515737056732\n",
      "epoch 117: loss 0.3114740252494812\n",
      "epoch 118: loss 0.33556121587753296\n",
      "epoch 119: loss 0.37122365832328796\n",
      "epoch 120: loss 0.3130381405353546\n",
      "epoch 121: loss 0.39706483483314514\n",
      "epoch 122: loss 0.3033137321472168\n",
      "epoch 123: loss 0.4038122296333313\n",
      "epoch 124: loss 0.3145475387573242\n",
      "epoch 125: loss 0.32438698410987854\n",
      "epoch 126: loss 0.38487252593040466\n",
      "epoch 127: loss 0.40429365634918213\n",
      "epoch 128: loss 0.29404520988464355\n",
      "epoch 129: loss 0.3508693277835846\n",
      "epoch 130: loss 0.39339950680732727\n",
      "epoch 131: loss 0.34344807267189026\n",
      "epoch 132: loss 0.3365139365196228\n",
      "epoch 133: loss 0.36571595072746277\n",
      "epoch 134: loss 0.31985652446746826\n",
      "epoch 135: loss 0.27048882842063904\n",
      "epoch 136: loss 0.23838907480239868\n",
      "epoch 0: loss 0.40136727690696716\n",
      "epoch 1: loss 0.35271239280700684\n",
      "epoch 2: loss 0.4314271807670593\n",
      "epoch 3: loss 0.396912544965744\n",
      "epoch 4: loss 0.37543410062789917\n",
      "epoch 5: loss 0.3021150231361389\n",
      "epoch 6: loss 0.29732435941696167\n",
      "epoch 7: loss 0.26356011629104614\n",
      "epoch 8: loss 0.48522475361824036\n",
      "epoch 9: loss 0.48889097571372986\n",
      "epoch 10: loss 0.35406312346458435\n",
      "epoch 11: loss 0.33290669322013855\n",
      "epoch 12: loss 0.413582980632782\n",
      "epoch 13: loss 0.44445982575416565\n",
      "epoch 14: loss 0.3536584973335266\n",
      "epoch 15: loss 0.3507603108882904\n",
      "epoch 16: loss 0.3603490889072418\n",
      "epoch 17: loss 0.3931656777858734\n",
      "epoch 18: loss 0.4218766689300537\n",
      "epoch 19: loss 0.310411274433136\n",
      "epoch 20: loss 0.3515430688858032\n",
      "epoch 21: loss 0.37851303815841675\n",
      "epoch 22: loss 0.34996867179870605\n",
      "epoch 23: loss 0.3920046389102936\n",
      "epoch 24: loss 0.36949601769447327\n",
      "epoch 25: loss 0.4068644940853119\n",
      "epoch 26: loss 0.33470606803894043\n",
      "epoch 27: loss 0.3709753155708313\n",
      "epoch 28: loss 0.4255659580230713\n",
      "epoch 29: loss 0.3339490294456482\n",
      "epoch 30: loss 0.3036726117134094\n",
      "epoch 31: loss 0.3517770767211914\n",
      "epoch 32: loss 0.34360471367836\n",
      "epoch 33: loss 0.34266397356987\n",
      "epoch 34: loss 0.377340704202652\n",
      "epoch 35: loss 0.278817355632782\n",
      "epoch 36: loss 0.35110166668891907\n",
      "epoch 37: loss 0.3492140769958496\n",
      "epoch 38: loss 0.23601987957954407\n",
      "epoch 39: loss 0.33317995071411133\n",
      "epoch 40: loss 0.33562126755714417\n",
      "epoch 41: loss 0.3035147786140442\n",
      "epoch 42: loss 0.2896583378314972\n",
      "epoch 43: loss 0.3146606385707855\n",
      "epoch 44: loss 0.39585191011428833\n",
      "epoch 45: loss 0.29878610372543335\n",
      "epoch 46: loss 0.34691527485847473\n",
      "epoch 47: loss 0.2885228991508484\n",
      "epoch 48: loss 0.31036657094955444\n",
      "epoch 49: loss 0.3431876003742218\n",
      "epoch 50: loss 0.29186734557151794\n",
      "epoch 51: loss 0.4322578012943268\n",
      "epoch 52: loss 0.33761221170425415\n",
      "epoch 53: loss 0.4017361104488373\n",
      "epoch 54: loss 0.3214881122112274\n",
      "epoch 55: loss 0.3100837171077728\n",
      "epoch 56: loss 0.34825071692466736\n",
      "epoch 57: loss 0.2834138572216034\n",
      "epoch 58: loss 0.3432347774505615\n",
      "epoch 59: loss 0.24987700581550598\n",
      "epoch 60: loss 0.3634989261627197\n",
      "epoch 61: loss 0.2396743893623352\n",
      "epoch 62: loss 0.2590208649635315\n",
      "epoch 63: loss 0.38480299711227417\n",
      "epoch 64: loss 0.32147589325904846\n",
      "epoch 65: loss 0.4122087359428406\n",
      "epoch 66: loss 0.3695598244667053\n",
      "epoch 67: loss 0.39955151081085205\n",
      "epoch 68: loss 0.378828763961792\n",
      "epoch 69: loss 0.43694472312927246\n",
      "epoch 70: loss 0.4180212616920471\n",
      "epoch 71: loss 0.320115327835083\n",
      "epoch 72: loss 0.27824029326438904\n",
      "epoch 73: loss 0.3604215383529663\n",
      "epoch 74: loss 0.308668851852417\n",
      "epoch 75: loss 0.32531946897506714\n",
      "epoch 76: loss 0.3127657175064087\n",
      "epoch 77: loss 0.39950451254844666\n",
      "epoch 78: loss 0.31469494104385376\n",
      "epoch 79: loss 0.364309698343277\n",
      "epoch 80: loss 0.3919578194618225\n",
      "epoch 81: loss 0.2864801287651062\n",
      "epoch 82: loss 0.3464190363883972\n",
      "epoch 83: loss 0.39336076378822327\n",
      "epoch 84: loss 0.26415345072746277\n",
      "epoch 85: loss 0.3204604387283325\n",
      "epoch 86: loss 0.37224051356315613\n",
      "epoch 87: loss 0.3445615768432617\n",
      "epoch 88: loss 0.2767975926399231\n",
      "epoch 89: loss 0.34572097659111023\n",
      "epoch 90: loss 0.26231905817985535\n",
      "epoch 91: loss 0.3516751527786255\n",
      "epoch 92: loss 0.2957882285118103\n",
      "epoch 93: loss 0.3331313133239746\n",
      "epoch 94: loss 0.3067128658294678\n",
      "epoch 95: loss 0.2092709243297577\n",
      "epoch 96: loss 0.2360641360282898\n",
      "epoch 97: loss 0.31539982557296753\n",
      "epoch 98: loss 0.3335112929344177\n",
      "epoch 99: loss 0.370095431804657\n",
      "epoch 100: loss 0.4486424922943115\n",
      "epoch 101: loss 0.42115122079849243\n",
      "epoch 102: loss 0.2895023226737976\n",
      "epoch 103: loss 0.340140700340271\n",
      "epoch 104: loss 0.28891780972480774\n",
      "epoch 105: loss 0.4190366864204407\n",
      "epoch 106: loss 0.3781317174434662\n",
      "epoch 107: loss 0.3250035047531128\n",
      "epoch 108: loss 0.3412206768989563\n",
      "epoch 109: loss 0.37461617588996887\n",
      "epoch 110: loss 0.3711428642272949\n",
      "epoch 111: loss 0.4119911789894104\n",
      "epoch 112: loss 0.39445751905441284\n",
      "epoch 113: loss 0.31267833709716797\n",
      "epoch 114: loss 0.1888129711151123\n",
      "epoch 115: loss 0.48857802152633667\n",
      "epoch 116: loss 0.35029202699661255\n",
      "epoch 117: loss 0.3080179691314697\n",
      "epoch 118: loss 0.3336043953895569\n",
      "epoch 119: loss 0.3725759983062744\n",
      "epoch 120: loss 0.315060555934906\n",
      "epoch 121: loss 0.39448875188827515\n",
      "epoch 122: loss 0.3009682893753052\n",
      "epoch 123: loss 0.40318208932876587\n",
      "epoch 124: loss 0.312976598739624\n",
      "epoch 125: loss 0.32600903511047363\n",
      "epoch 126: loss 0.38071468472480774\n",
      "epoch 127: loss 0.40755021572113037\n",
      "epoch 128: loss 0.2888433337211609\n",
      "epoch 129: loss 0.34973084926605225\n",
      "epoch 130: loss 0.3909268379211426\n",
      "epoch 131: loss 0.3438660502433777\n",
      "epoch 132: loss 0.334622323513031\n",
      "epoch 133: loss 0.36269611120224\n",
      "epoch 134: loss 0.3200855851173401\n",
      "epoch 135: loss 0.2672881484031677\n",
      "epoch 136: loss 0.2358759045600891\n",
      "epoch 0: loss 0.4072984457015991\n",
      "epoch 1: loss 0.35300391912460327\n",
      "epoch 2: loss 0.4336701035499573\n",
      "epoch 3: loss 0.39109569787979126\n",
      "epoch 4: loss 0.3687576949596405\n",
      "epoch 5: loss 0.3089524507522583\n",
      "epoch 6: loss 0.2949192523956299\n",
      "epoch 7: loss 0.25343382358551025\n",
      "epoch 8: loss 0.4714272618293762\n",
      "epoch 9: loss 0.492131769657135\n",
      "epoch 10: loss 0.36287611722946167\n",
      "epoch 11: loss 0.2995623052120209\n",
      "epoch 12: loss 0.40551722049713135\n",
      "epoch 13: loss 0.4404660761356354\n",
      "epoch 14: loss 0.35110148787498474\n",
      "epoch 15: loss 0.32701021432876587\n",
      "epoch 16: loss 0.3515397310256958\n",
      "epoch 17: loss 0.3823072612285614\n",
      "epoch 18: loss 0.42605745792388916\n",
      "epoch 19: loss 0.29628118872642517\n",
      "epoch 20: loss 0.3277096152305603\n",
      "epoch 21: loss 0.3673909306526184\n",
      "epoch 22: loss 0.349215030670166\n",
      "epoch 23: loss 0.39013999700546265\n",
      "epoch 24: loss 0.36762285232543945\n",
      "epoch 25: loss 0.40495628118515015\n",
      "epoch 26: loss 0.331099271774292\n",
      "epoch 27: loss 0.3705516755580902\n",
      "epoch 28: loss 0.43118613958358765\n",
      "epoch 29: loss 0.3213980197906494\n",
      "epoch 30: loss 0.28997930884361267\n",
      "epoch 31: loss 0.3456556797027588\n",
      "epoch 32: loss 0.34661075472831726\n",
      "epoch 33: loss 0.34875184297561646\n",
      "epoch 34: loss 0.3853069543838501\n",
      "epoch 35: loss 0.2609567642211914\n",
      "epoch 36: loss 0.34792208671569824\n",
      "epoch 37: loss 0.3417796790599823\n",
      "epoch 38: loss 0.226681649684906\n",
      "epoch 39: loss 0.3319931924343109\n",
      "epoch 40: loss 0.32435405254364014\n",
      "epoch 41: loss 0.30608537793159485\n",
      "epoch 42: loss 0.30214762687683105\n",
      "epoch 43: loss 0.31290650367736816\n",
      "epoch 44: loss 0.38890835642814636\n",
      "epoch 45: loss 0.30470961332321167\n",
      "epoch 46: loss 0.35920634865760803\n",
      "epoch 47: loss 0.2893519699573517\n",
      "epoch 48: loss 0.29806965589523315\n",
      "epoch 49: loss 0.34714990854263306\n",
      "epoch 50: loss 0.31072092056274414\n",
      "epoch 51: loss 0.42061394453048706\n",
      "epoch 52: loss 0.312576025724411\n",
      "epoch 53: loss 0.3889199197292328\n",
      "epoch 54: loss 0.32407599687576294\n",
      "epoch 55: loss 0.30889320373535156\n",
      "epoch 56: loss 0.34113171696662903\n",
      "epoch 57: loss 0.2584267556667328\n",
      "epoch 58: loss 0.34381893277168274\n",
      "epoch 59: loss 0.2496553510427475\n",
      "epoch 60: loss 0.3584839701652527\n",
      "epoch 61: loss 0.22942155599594116\n",
      "epoch 62: loss 0.24726612865924835\n",
      "epoch 63: loss 0.3734705448150635\n",
      "epoch 64: loss 0.31733816862106323\n",
      "epoch 65: loss 0.4133511483669281\n",
      "epoch 66: loss 0.35606691241264343\n",
      "epoch 67: loss 0.3936629891395569\n",
      "epoch 68: loss 0.37972491979599\n",
      "epoch 69: loss 0.42571133375167847\n",
      "epoch 70: loss 0.40238481760025024\n",
      "epoch 71: loss 0.3292502462863922\n",
      "epoch 72: loss 0.27381157875061035\n",
      "epoch 73: loss 0.3555416762828827\n",
      "epoch 74: loss 0.30048608779907227\n",
      "epoch 75: loss 0.3256242275238037\n",
      "epoch 76: loss 0.3090083599090576\n",
      "epoch 77: loss 0.4019281268119812\n",
      "epoch 78: loss 0.3136546015739441\n",
      "epoch 79: loss 0.3555746078491211\n",
      "epoch 80: loss 0.3868328332901001\n",
      "epoch 81: loss 0.28736066818237305\n",
      "epoch 82: loss 0.34216034412384033\n",
      "epoch 83: loss 0.3919718265533447\n",
      "epoch 84: loss 0.2558233439922333\n",
      "epoch 85: loss 0.3203808069229126\n",
      "epoch 86: loss 0.3624282777309418\n",
      "epoch 87: loss 0.3456325829029083\n",
      "epoch 88: loss 0.2748146653175354\n",
      "epoch 89: loss 0.33864104747772217\n",
      "epoch 90: loss 0.2571832537651062\n",
      "epoch 91: loss 0.3467924892902374\n",
      "epoch 92: loss 0.289387583732605\n",
      "epoch 93: loss 0.3351677656173706\n",
      "epoch 94: loss 0.3033379018306732\n",
      "epoch 95: loss 0.20288318395614624\n",
      "epoch 96: loss 0.23440027236938477\n",
      "epoch 97: loss 0.31720301508903503\n",
      "epoch 98: loss 0.33760499954223633\n",
      "epoch 99: loss 0.3711205720901489\n",
      "epoch 100: loss 0.449554979801178\n",
      "epoch 101: loss 0.43137291073799133\n",
      "epoch 102: loss 0.2905835807323456\n",
      "epoch 103: loss 0.33726438879966736\n",
      "epoch 104: loss 0.2883833050727844\n",
      "epoch 105: loss 0.4208552837371826\n",
      "epoch 106: loss 0.38274627923965454\n",
      "epoch 107: loss 0.3316499888896942\n",
      "epoch 108: loss 0.34072035551071167\n",
      "epoch 109: loss 0.37053656578063965\n",
      "epoch 110: loss 0.3642103970050812\n",
      "epoch 111: loss 0.41134193539619446\n",
      "epoch 112: loss 0.4033103883266449\n",
      "epoch 113: loss 0.33210474252700806\n",
      "epoch 114: loss 0.21438905596733093\n",
      "epoch 115: loss 0.4639374613761902\n",
      "epoch 116: loss 0.34667855501174927\n",
      "epoch 117: loss 0.3104911744594574\n",
      "epoch 118: loss 0.3401535153388977\n",
      "epoch 119: loss 0.37512528896331787\n",
      "epoch 120: loss 0.3027823567390442\n",
      "epoch 121: loss 0.3930226266384125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 122: loss 0.30957692861557007\n",
      "epoch 123: loss 0.4008687138557434\n",
      "epoch 124: loss 0.31477364897727966\n",
      "epoch 125: loss 0.326911985874176\n",
      "epoch 126: loss 0.38186103105545044\n",
      "epoch 127: loss 0.41021907329559326\n",
      "epoch 128: loss 0.2874417304992676\n",
      "epoch 129: loss 0.3450600802898407\n",
      "epoch 130: loss 0.3907572627067566\n",
      "epoch 131: loss 0.34756436944007874\n",
      "epoch 132: loss 0.3377028703689575\n",
      "epoch 133: loss 0.3597000241279602\n",
      "epoch 134: loss 0.32160767912864685\n",
      "epoch 135: loss 0.26750439405441284\n",
      "epoch 136: loss 0.23527756333351135\n",
      "epoch 0: loss 0.4096202254295349\n",
      "epoch 1: loss 0.3590545058250427\n",
      "epoch 2: loss 0.4396924674510956\n",
      "epoch 3: loss 0.3897557258605957\n",
      "epoch 4: loss 0.37598875164985657\n",
      "epoch 5: loss 0.3158378005027771\n",
      "epoch 6: loss 0.2967967391014099\n",
      "epoch 7: loss 0.2528761029243469\n",
      "epoch 8: loss 0.4773944616317749\n",
      "epoch 9: loss 0.49180179834365845\n",
      "epoch 10: loss 0.3627208471298218\n",
      "epoch 11: loss 0.30076172947883606\n",
      "epoch 12: loss 0.39949530363082886\n",
      "epoch 13: loss 0.4454289972782135\n",
      "epoch 14: loss 0.35538214445114136\n",
      "epoch 15: loss 0.3248410224914551\n",
      "epoch 16: loss 0.3493691086769104\n",
      "epoch 17: loss 0.3811081051826477\n",
      "epoch 18: loss 0.42881110310554504\n",
      "epoch 19: loss 0.2916456460952759\n",
      "epoch 20: loss 0.32203954458236694\n",
      "epoch 21: loss 0.3620828092098236\n",
      "epoch 22: loss 0.34770843386650085\n",
      "epoch 23: loss 0.3878532946109772\n",
      "epoch 24: loss 0.36806029081344604\n",
      "epoch 25: loss 0.4048197865486145\n",
      "epoch 26: loss 0.33013540506362915\n",
      "epoch 27: loss 0.3659224510192871\n",
      "epoch 28: loss 0.4205775260925293\n",
      "epoch 29: loss 0.3242456316947937\n",
      "epoch 30: loss 0.28923463821411133\n",
      "epoch 31: loss 0.3416834771633148\n",
      "epoch 32: loss 0.3445655107498169\n",
      "epoch 33: loss 0.34608566761016846\n",
      "epoch 34: loss 0.38675132393836975\n",
      "epoch 35: loss 0.259415864944458\n",
      "epoch 36: loss 0.34228891134262085\n",
      "epoch 37: loss 0.3271068334579468\n",
      "epoch 38: loss 0.22353461384773254\n",
      "epoch 39: loss 0.32721737027168274\n",
      "epoch 40: loss 0.32478106021881104\n",
      "epoch 41: loss 0.295904278755188\n",
      "epoch 42: loss 0.29363498091697693\n",
      "epoch 43: loss 0.3129170536994934\n",
      "epoch 44: loss 0.3814617395401001\n",
      "epoch 45: loss 0.29271677136421204\n",
      "epoch 46: loss 0.3421180248260498\n",
      "epoch 47: loss 0.28489062190055847\n",
      "epoch 48: loss 0.2941894829273224\n",
      "epoch 49: loss 0.34149980545043945\n",
      "epoch 50: loss 0.29712313413619995\n",
      "epoch 51: loss 0.4288759231567383\n",
      "epoch 52: loss 0.3148987293243408\n",
      "epoch 53: loss 0.38678717613220215\n",
      "epoch 54: loss 0.3300190269947052\n",
      "epoch 55: loss 0.30642449855804443\n",
      "epoch 56: loss 0.3441762328147888\n",
      "epoch 57: loss 0.2660929560661316\n",
      "epoch 58: loss 0.3424031138420105\n",
      "epoch 59: loss 0.24764606356620789\n",
      "epoch 60: loss 0.35772275924682617\n",
      "epoch 61: loss 0.2272091954946518\n",
      "epoch 62: loss 0.24386660754680634\n",
      "epoch 63: loss 0.37374377250671387\n",
      "epoch 64: loss 0.31659233570098877\n",
      "epoch 65: loss 0.4096220135688782\n",
      "epoch 66: loss 0.3591850697994232\n",
      "epoch 67: loss 0.38990122079849243\n",
      "epoch 68: loss 0.37449911236763\n",
      "epoch 69: loss 0.43352171778678894\n",
      "epoch 70: loss 0.4085109233856201\n",
      "epoch 71: loss 0.32106032967567444\n",
      "epoch 72: loss 0.273306280374527\n",
      "epoch 73: loss 0.35425251722335815\n",
      "epoch 74: loss 0.30010542273521423\n",
      "epoch 75: loss 0.3251645565032959\n",
      "epoch 76: loss 0.3081231117248535\n",
      "epoch 77: loss 0.4019486904144287\n",
      "epoch 78: loss 0.31301021575927734\n",
      "epoch 79: loss 0.35321757197380066\n",
      "epoch 80: loss 0.3853268623352051\n",
      "epoch 81: loss 0.28357335925102234\n",
      "epoch 82: loss 0.33891358971595764\n",
      "epoch 83: loss 0.3897908329963684\n",
      "epoch 84: loss 0.25505363941192627\n",
      "epoch 85: loss 0.3197981119155884\n",
      "epoch 86: loss 0.3635505735874176\n",
      "epoch 87: loss 0.3489190340042114\n",
      "epoch 88: loss 0.27404817938804626\n",
      "epoch 89: loss 0.33358728885650635\n",
      "epoch 90: loss 0.2582038640975952\n",
      "epoch 91: loss 0.34154391288757324\n",
      "epoch 92: loss 0.2858843207359314\n",
      "epoch 93: loss 0.3308759927749634\n",
      "epoch 94: loss 0.3005765676498413\n",
      "epoch 95: loss 0.20487122237682343\n",
      "epoch 96: loss 0.2247501164674759\n",
      "epoch 97: loss 0.31304866075515747\n",
      "epoch 98: loss 0.32964277267456055\n",
      "epoch 99: loss 0.37133753299713135\n",
      "epoch 100: loss 0.4503026306629181\n",
      "epoch 101: loss 0.43157899379730225\n",
      "epoch 102: loss 0.28242477774620056\n",
      "epoch 103: loss 0.3386259377002716\n",
      "epoch 104: loss 0.2888067364692688\n",
      "epoch 105: loss 0.42581284046173096\n",
      "epoch 106: loss 0.38217663764953613\n",
      "epoch 107: loss 0.31993016600608826\n",
      "epoch 108: loss 0.34640833735466003\n",
      "epoch 109: loss 0.3657470941543579\n",
      "epoch 110: loss 0.37061017751693726\n",
      "epoch 111: loss 0.40351322293281555\n",
      "epoch 112: loss 0.38896238803863525\n",
      "epoch 113: loss 0.3161102831363678\n",
      "epoch 114: loss 0.20082655549049377\n",
      "epoch 115: loss 0.46487152576446533\n",
      "epoch 116: loss 0.34260064363479614\n",
      "epoch 117: loss 0.3071039915084839\n",
      "epoch 118: loss 0.3352314233779907\n",
      "epoch 119: loss 0.3716275691986084\n",
      "epoch 120: loss 0.30036503076553345\n",
      "epoch 121: loss 0.39103034138679504\n",
      "epoch 122: loss 0.30200260877609253\n",
      "epoch 123: loss 0.3978131413459778\n",
      "epoch 124: loss 0.31652355194091797\n",
      "epoch 125: loss 0.3252631425857544\n",
      "epoch 126: loss 0.3748471438884735\n",
      "epoch 127: loss 0.40647563338279724\n",
      "epoch 128: loss 0.2844042181968689\n",
      "epoch 129: loss 0.34438055753707886\n",
      "epoch 130: loss 0.38721805810928345\n",
      "epoch 131: loss 0.34004083275794983\n",
      "epoch 132: loss 0.33409926295280457\n",
      "epoch 133: loss 0.3591717481613159\n",
      "epoch 134: loss 0.3209364414215088\n",
      "epoch 135: loss 0.2653944790363312\n",
      "epoch 136: loss 0.23578935861587524\n",
      "epoch 0: loss 0.4078735411167145\n",
      "epoch 1: loss 0.35789626836776733\n",
      "epoch 2: loss 0.44030824303627014\n",
      "epoch 3: loss 0.38573622703552246\n",
      "epoch 4: loss 0.36711370944976807\n",
      "epoch 5: loss 0.3088451027870178\n",
      "epoch 6: loss 0.2952462434768677\n",
      "epoch 7: loss 0.24744285643100739\n",
      "epoch 8: loss 0.4665309190750122\n",
      "epoch 9: loss 0.4882912039756775\n",
      "epoch 10: loss 0.36152610182762146\n",
      "epoch 11: loss 0.2966822385787964\n",
      "epoch 12: loss 0.3923869729042053\n",
      "epoch 13: loss 0.4403860569000244\n",
      "epoch 14: loss 0.35403960943222046\n",
      "epoch 15: loss 0.3198620676994324\n",
      "epoch 16: loss 0.34506911039352417\n",
      "epoch 17: loss 0.3751058876514435\n",
      "epoch 18: loss 0.4206761121749878\n",
      "epoch 19: loss 0.28710705041885376\n",
      "epoch 20: loss 0.3175975978374481\n",
      "epoch 21: loss 0.3596246838569641\n",
      "epoch 22: loss 0.34233415126800537\n",
      "epoch 23: loss 0.3832547068595886\n",
      "epoch 24: loss 0.36466121673583984\n",
      "epoch 25: loss 0.4025772511959076\n",
      "epoch 26: loss 0.3258249759674072\n",
      "epoch 27: loss 0.3621290326118469\n",
      "epoch 28: loss 0.4111247658729553\n",
      "epoch 29: loss 0.32589396834373474\n",
      "epoch 30: loss 0.28777194023132324\n",
      "epoch 31: loss 0.34041765332221985\n",
      "epoch 32: loss 0.3430591821670532\n",
      "epoch 33: loss 0.3469077944755554\n",
      "epoch 34: loss 0.3888821005821228\n",
      "epoch 35: loss 0.259125679731369\n",
      "epoch 36: loss 0.3389504849910736\n",
      "epoch 37: loss 0.32386982440948486\n",
      "epoch 38: loss 0.22177079319953918\n",
      "epoch 39: loss 0.32945433259010315\n",
      "epoch 40: loss 0.3235155940055847\n",
      "epoch 41: loss 0.292764812707901\n",
      "epoch 42: loss 0.29924342036247253\n",
      "epoch 43: loss 0.31750160455703735\n",
      "epoch 44: loss 0.3737975060939789\n",
      "epoch 45: loss 0.2941230535507202\n",
      "epoch 46: loss 0.34742629528045654\n",
      "epoch 47: loss 0.2864205241203308\n",
      "epoch 48: loss 0.2876738905906677\n",
      "epoch 49: loss 0.3403729200363159\n",
      "epoch 50: loss 0.3022793233394623\n",
      "epoch 51: loss 0.42006391286849976\n",
      "epoch 52: loss 0.3084295094013214\n",
      "epoch 53: loss 0.38354629278182983\n",
      "epoch 54: loss 0.33045250177383423\n",
      "epoch 55: loss 0.30595263838768005\n",
      "epoch 56: loss 0.3423575758934021\n",
      "epoch 57: loss 0.2577558159828186\n",
      "epoch 58: loss 0.3425525426864624\n",
      "epoch 59: loss 0.24870291352272034\n",
      "epoch 60: loss 0.3537297248840332\n",
      "epoch 61: loss 0.2269204556941986\n",
      "epoch 62: loss 0.24271732568740845\n",
      "epoch 63: loss 0.36769628524780273\n",
      "epoch 64: loss 0.31377363204956055\n",
      "epoch 65: loss 0.40837642550468445\n",
      "epoch 66: loss 0.35179051756858826\n",
      "epoch 67: loss 0.387174129486084\n",
      "epoch 68: loss 0.37200725078582764\n",
      "epoch 69: loss 0.42841029167175293\n",
      "epoch 70: loss 0.39997154474258423\n",
      "epoch 71: loss 0.32726535201072693\n",
      "epoch 72: loss 0.27280738949775696\n",
      "epoch 73: loss 0.3513530492782593\n",
      "epoch 74: loss 0.29486191272735596\n",
      "epoch 75: loss 0.3270646929740906\n",
      "epoch 76: loss 0.30660587549209595\n",
      "epoch 77: loss 0.4045957922935486\n",
      "epoch 78: loss 0.31254929304122925\n",
      "epoch 79: loss 0.34773528575897217\n",
      "epoch 80: loss 0.3827298581600189\n",
      "epoch 81: loss 0.28451836109161377\n",
      "epoch 82: loss 0.33657318353652954\n",
      "epoch 83: loss 0.3896976709365845\n",
      "epoch 84: loss 0.25213655829429626\n",
      "epoch 85: loss 0.3188871145248413\n",
      "epoch 86: loss 0.35643821954727173\n",
      "epoch 87: loss 0.3457225561141968\n",
      "epoch 88: loss 0.27298247814178467\n",
      "epoch 89: loss 0.3310800790786743\n",
      "epoch 90: loss 0.25596168637275696\n",
      "epoch 91: loss 0.3372895419597626\n",
      "epoch 92: loss 0.28351718187332153\n",
      "epoch 93: loss 0.3302304744720459\n",
      "epoch 94: loss 0.2974136471748352\n",
      "epoch 95: loss 0.19805827736854553\n",
      "epoch 96: loss 0.22766372561454773\n",
      "epoch 97: loss 0.31359928846359253\n",
      "epoch 98: loss 0.3313649594783783\n",
      "epoch 99: loss 0.3669583201408386\n",
      "epoch 100: loss 0.45232680439949036\n",
      "epoch 101: loss 0.4420042634010315\n",
      "epoch 102: loss 0.27198028564453125\n",
      "epoch 103: loss 0.35145244002342224\n",
      "epoch 104: loss 0.2904157042503357\n",
      "epoch 105: loss 0.4277520179748535\n",
      "epoch 106: loss 0.3822020888328552\n",
      "epoch 107: loss 0.3249644637107849\n",
      "epoch 108: loss 0.3416402339935303\n",
      "epoch 109: loss 0.36659500002861023\n",
      "epoch 110: loss 0.3676503300666809\n",
      "epoch 111: loss 0.40569886565208435\n",
      "epoch 112: loss 0.3949052095413208\n",
      "epoch 113: loss 0.3276199698448181\n",
      "epoch 114: loss 0.2154269516468048\n",
      "epoch 115: loss 0.45093661546707153\n",
      "epoch 116: loss 0.33822566270828247\n",
      "epoch 117: loss 0.3068641424179077\n",
      "epoch 118: loss 0.33707886934280396\n",
      "epoch 119: loss 0.37373921275138855\n",
      "epoch 120: loss 0.29655182361602783\n",
      "epoch 121: loss 0.39134347438812256\n",
      "epoch 122: loss 0.30076178908348083\n",
      "epoch 123: loss 0.3969598412513733\n",
      "epoch 124: loss 0.3189173936843872\n",
      "epoch 125: loss 0.32528674602508545\n",
      "epoch 126: loss 0.3713163137435913\n",
      "epoch 127: loss 0.4095116853713989\n",
      "epoch 128: loss 0.28584742546081543\n",
      "epoch 129: loss 0.34239494800567627\n",
      "epoch 130: loss 0.39232802391052246\n",
      "epoch 131: loss 0.3373571038246155\n",
      "epoch 132: loss 0.33318400382995605\n",
      "epoch 133: loss 0.35750943422317505\n",
      "epoch 134: loss 0.32651135325431824\n",
      "epoch 135: loss 0.2675536274909973\n",
      "epoch 136: loss 0.2357690930366516\n",
      "epoch 0: loss 0.40629690885543823\n",
      "epoch 1: loss 0.36553236842155457\n",
      "epoch 2: loss 0.4563937783241272\n",
      "epoch 3: loss 0.3865239918231964\n",
      "epoch 4: loss 0.34711185097694397\n",
      "epoch 5: loss 0.3262980878353119\n",
      "epoch 6: loss 0.3009788691997528\n",
      "epoch 7: loss 0.2427923083305359\n",
      "epoch 8: loss 0.4436470866203308\n",
      "epoch 9: loss 0.4870428740978241\n",
      "epoch 10: loss 0.37900251150131226\n",
      "epoch 11: loss 0.295890212059021\n",
      "epoch 12: loss 0.3426966965198517\n",
      "epoch 13: loss 0.4404836595058441\n",
      "epoch 14: loss 0.38254913687705994\n",
      "epoch 15: loss 0.32372820377349854\n",
      "epoch 16: loss 0.3385005593299866\n",
      "epoch 17: loss 0.3642028868198395\n",
      "epoch 18: loss 0.42292141914367676\n",
      "epoch 19: loss 0.2917540967464447\n",
      "epoch 20: loss 0.31081920862197876\n",
      "epoch 21: loss 0.3549863398075104\n",
      "epoch 22: loss 0.33738094568252563\n",
      "epoch 23: loss 0.38301950693130493\n",
      "epoch 24: loss 0.36988043785095215\n",
      "epoch 25: loss 0.4023669362068176\n",
      "epoch 26: loss 0.32871317863464355\n",
      "epoch 27: loss 0.36253729462623596\n",
      "epoch 28: loss 0.41901615262031555\n",
      "epoch 29: loss 0.32039740681648254\n",
      "epoch 30: loss 0.2837695777416229\n",
      "epoch 31: loss 0.3384600877761841\n",
      "epoch 32: loss 0.34340518712997437\n",
      "epoch 33: loss 0.34976983070373535\n",
      "epoch 34: loss 0.3891088366508484\n",
      "epoch 35: loss 0.26516982913017273\n",
      "epoch 36: loss 0.33978480100631714\n",
      "epoch 37: loss 0.314456045627594\n",
      "epoch 38: loss 0.21709874272346497\n",
      "epoch 39: loss 0.3326318562030792\n",
      "epoch 40: loss 0.3286573588848114\n",
      "epoch 41: loss 0.2859624922275543\n",
      "epoch 42: loss 0.2947317957878113\n",
      "epoch 43: loss 0.32047128677368164\n",
      "epoch 44: loss 0.367524653673172\n",
      "epoch 45: loss 0.2896023392677307\n",
      "epoch 46: loss 0.33047908544540405\n",
      "epoch 47: loss 0.28334954380989075\n",
      "epoch 48: loss 0.28460878133773804\n",
      "epoch 49: loss 0.3349744379520416\n",
      "epoch 50: loss 0.2911849021911621\n",
      "epoch 51: loss 0.424166202545166\n",
      "epoch 52: loss 0.30566221475601196\n",
      "epoch 53: loss 0.38275063037872314\n",
      "epoch 54: loss 0.33465635776519775\n",
      "epoch 55: loss 0.305858850479126\n",
      "epoch 56: loss 0.34564319252967834\n",
      "epoch 57: loss 0.2608107626438141\n",
      "epoch 58: loss 0.3451738953590393\n",
      "epoch 59: loss 0.24647456407546997\n",
      "epoch 60: loss 0.3602908253669739\n",
      "epoch 61: loss 0.22417965531349182\n",
      "epoch 62: loss 0.23975956439971924\n",
      "epoch 63: loss 0.36525070667266846\n",
      "epoch 64: loss 0.30950626730918884\n",
      "epoch 65: loss 0.4068951904773712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 66: loss 0.35878047347068787\n",
      "epoch 67: loss 0.383865088224411\n",
      "epoch 68: loss 0.36917734146118164\n",
      "epoch 69: loss 0.43634361028671265\n",
      "epoch 70: loss 0.4026485085487366\n",
      "epoch 71: loss 0.3279463052749634\n",
      "epoch 72: loss 0.2807009816169739\n",
      "epoch 73: loss 0.34938398003578186\n",
      "epoch 74: loss 0.2949312627315521\n",
      "epoch 75: loss 0.32730820775032043\n",
      "epoch 76: loss 0.30506432056427\n",
      "epoch 77: loss 0.40783166885375977\n",
      "epoch 78: loss 0.3141530752182007\n",
      "epoch 79: loss 0.3510892391204834\n",
      "epoch 80: loss 0.3814520239830017\n",
      "epoch 81: loss 0.2918451726436615\n",
      "epoch 82: loss 0.3371497690677643\n",
      "epoch 83: loss 0.3890513479709625\n",
      "epoch 84: loss 0.25096702575683594\n",
      "epoch 85: loss 0.31883615255355835\n",
      "epoch 86: loss 0.35447049140930176\n",
      "epoch 87: loss 0.3448483347892761\n",
      "epoch 88: loss 0.2713581323623657\n",
      "epoch 89: loss 0.33416905999183655\n",
      "epoch 90: loss 0.2576962113380432\n",
      "epoch 91: loss 0.3346683382987976\n",
      "epoch 92: loss 0.28420376777648926\n",
      "epoch 93: loss 0.33894237875938416\n",
      "epoch 94: loss 0.2988416254520416\n",
      "epoch 95: loss 0.19691075384616852\n",
      "epoch 96: loss 0.2301817089319229\n",
      "epoch 97: loss 0.317892849445343\n",
      "epoch 98: loss 0.3369224965572357\n",
      "epoch 99: loss 0.3747596740722656\n",
      "epoch 100: loss 0.4561197757720947\n",
      "epoch 101: loss 0.43415567278862\n",
      "epoch 102: loss 0.28223520517349243\n",
      "epoch 103: loss 0.3318639397621155\n",
      "epoch 104: loss 0.2833269536495209\n",
      "epoch 105: loss 0.417560875415802\n",
      "epoch 106: loss 0.3820980191230774\n",
      "epoch 107: loss 0.32209503650665283\n",
      "epoch 108: loss 0.3452104330062866\n",
      "epoch 109: loss 0.3620935082435608\n",
      "epoch 110: loss 0.3607370853424072\n",
      "epoch 111: loss 0.40639403462409973\n",
      "epoch 112: loss 0.39313799142837524\n",
      "epoch 113: loss 0.3168342411518097\n",
      "epoch 114: loss 0.20382985472679138\n",
      "epoch 115: loss 0.45775577425956726\n",
      "epoch 116: loss 0.3371833264827728\n",
      "epoch 117: loss 0.3033663034439087\n",
      "epoch 118: loss 0.33329010009765625\n",
      "epoch 119: loss 0.36943310499191284\n",
      "epoch 120: loss 0.2987734377384186\n",
      "epoch 121: loss 0.3916146457195282\n",
      "epoch 122: loss 0.30028948187828064\n",
      "epoch 123: loss 0.3934350907802582\n",
      "epoch 124: loss 0.31309089064598083\n",
      "epoch 125: loss 0.32550686597824097\n",
      "epoch 126: loss 0.3766227662563324\n",
      "epoch 127: loss 0.40710151195526123\n",
      "epoch 128: loss 0.2820098400115967\n",
      "epoch 129: loss 0.342803031206131\n",
      "epoch 130: loss 0.38543111085891724\n",
      "epoch 131: loss 0.3378239870071411\n",
      "epoch 132: loss 0.33224958181381226\n",
      "epoch 133: loss 0.35607200860977173\n",
      "epoch 134: loss 0.32319873571395874\n",
      "epoch 135: loss 0.26499831676483154\n",
      "epoch 136: loss 0.23871073126792908\n",
      "epoch 0: loss 0.40608084201812744\n",
      "epoch 1: loss 0.3584831953048706\n",
      "epoch 2: loss 0.450059711933136\n",
      "epoch 3: loss 0.38240885734558105\n",
      "epoch 4: loss 0.3553316593170166\n",
      "epoch 5: loss 0.31522244215011597\n",
      "epoch 6: loss 0.2994658946990967\n",
      "epoch 7: loss 0.2446117103099823\n",
      "epoch 8: loss 0.45005467534065247\n",
      "epoch 9: loss 0.48236018419265747\n",
      "epoch 10: loss 0.3704679012298584\n",
      "epoch 11: loss 0.2948704659938812\n",
      "epoch 12: loss 0.35026800632476807\n",
      "epoch 13: loss 0.4403265714645386\n",
      "epoch 14: loss 0.3722328841686249\n",
      "epoch 15: loss 0.32009056210517883\n",
      "epoch 16: loss 0.3364163637161255\n",
      "epoch 17: loss 0.3650516867637634\n",
      "epoch 18: loss 0.42023372650146484\n",
      "epoch 19: loss 0.28407180309295654\n",
      "epoch 20: loss 0.3110027313232422\n",
      "epoch 21: loss 0.35218173265457153\n",
      "epoch 22: loss 0.3349980115890503\n",
      "epoch 23: loss 0.38236308097839355\n",
      "epoch 24: loss 0.3643004894256592\n",
      "epoch 25: loss 0.40209442377090454\n",
      "epoch 26: loss 0.32419976592063904\n",
      "epoch 27: loss 0.36014091968536377\n",
      "epoch 28: loss 0.4102824926376343\n",
      "epoch 29: loss 0.3217877447605133\n",
      "epoch 30: loss 0.2819637656211853\n",
      "epoch 31: loss 0.34081026911735535\n",
      "epoch 32: loss 0.34324783086776733\n",
      "epoch 33: loss 0.3481295704841614\n",
      "epoch 34: loss 0.39185646176338196\n",
      "epoch 35: loss 0.2608492374420166\n",
      "epoch 36: loss 0.3393646776676178\n",
      "epoch 37: loss 0.314667671918869\n",
      "epoch 38: loss 0.21697531640529633\n",
      "epoch 39: loss 0.324867844581604\n",
      "epoch 40: loss 0.32040125131607056\n",
      "epoch 41: loss 0.2892425060272217\n",
      "epoch 42: loss 0.2907242774963379\n",
      "epoch 43: loss 0.31675106287002563\n",
      "epoch 44: loss 0.37109798192977905\n",
      "epoch 45: loss 0.28821274638175964\n",
      "epoch 46: loss 0.33208757638931274\n",
      "epoch 47: loss 0.28416281938552856\n",
      "epoch 48: loss 0.2828674912452698\n",
      "epoch 49: loss 0.3369882106781006\n",
      "epoch 50: loss 0.2898451089859009\n",
      "epoch 51: loss 0.42745211720466614\n",
      "epoch 52: loss 0.3086780607700348\n",
      "epoch 53: loss 0.3802594542503357\n",
      "epoch 54: loss 0.3345242440700531\n",
      "epoch 55: loss 0.3045814037322998\n",
      "epoch 56: loss 0.3467095196247101\n",
      "epoch 57: loss 0.2616386115550995\n",
      "epoch 58: loss 0.34301942586898804\n",
      "epoch 59: loss 0.246144101023674\n",
      "epoch 60: loss 0.35767292976379395\n",
      "epoch 61: loss 0.224267840385437\n",
      "epoch 62: loss 0.2392045557498932\n",
      "epoch 63: loss 0.3658062219619751\n",
      "epoch 64: loss 0.30798670649528503\n",
      "epoch 65: loss 0.40482527017593384\n",
      "epoch 66: loss 0.3541610836982727\n",
      "epoch 67: loss 0.3821474611759186\n",
      "epoch 68: loss 0.3696575164794922\n",
      "epoch 69: loss 0.43265217542648315\n",
      "epoch 70: loss 0.40116816759109497\n",
      "epoch 71: loss 0.3257748484611511\n",
      "epoch 72: loss 0.2775169014930725\n",
      "epoch 73: loss 0.34793204069137573\n",
      "epoch 74: loss 0.2935341000556946\n",
      "epoch 75: loss 0.32751768827438354\n",
      "epoch 76: loss 0.30420297384262085\n",
      "epoch 77: loss 0.40724360942840576\n",
      "epoch 78: loss 0.31461936235427856\n",
      "epoch 79: loss 0.35302650928497314\n",
      "epoch 80: loss 0.3802797496318817\n",
      "epoch 81: loss 0.29540568590164185\n",
      "epoch 82: loss 0.33677542209625244\n",
      "epoch 83: loss 0.38839805126190186\n",
      "epoch 84: loss 0.25030964612960815\n",
      "epoch 85: loss 0.3168545961380005\n",
      "epoch 86: loss 0.355593740940094\n",
      "epoch 87: loss 0.3430623412132263\n",
      "epoch 88: loss 0.27220505475997925\n",
      "epoch 89: loss 0.3292613625526428\n",
      "epoch 90: loss 0.25882747769355774\n",
      "epoch 91: loss 0.33499956130981445\n",
      "epoch 92: loss 0.28269150853157043\n",
      "epoch 93: loss 0.32879337668418884\n",
      "epoch 94: loss 0.2956346869468689\n",
      "epoch 95: loss 0.19790327548980713\n",
      "epoch 96: loss 0.22685550153255463\n",
      "epoch 97: loss 0.31250476837158203\n",
      "epoch 98: loss 0.3330558240413666\n",
      "epoch 99: loss 0.3708447217941284\n",
      "epoch 100: loss 0.45731425285339355\n",
      "epoch 101: loss 0.41727158427238464\n",
      "epoch 102: loss 0.28251782059669495\n",
      "epoch 103: loss 0.33045923709869385\n",
      "epoch 104: loss 0.2799071669578552\n",
      "epoch 105: loss 0.4203376770019531\n",
      "epoch 106: loss 0.3846534192562103\n",
      "epoch 107: loss 0.3200223743915558\n",
      "epoch 108: loss 0.3530179262161255\n",
      "epoch 109: loss 0.3586350381374359\n",
      "epoch 110: loss 0.3639577031135559\n",
      "epoch 111: loss 0.3996396064758301\n",
      "epoch 112: loss 0.3809202313423157\n",
      "epoch 113: loss 0.31209149956703186\n",
      "epoch 114: loss 0.20385462045669556\n",
      "epoch 115: loss 0.44460856914520264\n",
      "epoch 116: loss 0.32930707931518555\n",
      "epoch 117: loss 0.29932552576065063\n",
      "epoch 118: loss 0.33005279302597046\n",
      "epoch 119: loss 0.3696960210800171\n",
      "epoch 120: loss 0.2949220538139343\n",
      "epoch 121: loss 0.38913702964782715\n",
      "epoch 122: loss 0.29378262162208557\n",
      "epoch 123: loss 0.3966521620750427\n",
      "epoch 124: loss 0.3169727623462677\n",
      "epoch 125: loss 0.3255569338798523\n",
      "epoch 126: loss 0.36851710081100464\n",
      "epoch 127: loss 0.4053398370742798\n",
      "epoch 128: loss 0.28268733620643616\n",
      "epoch 129: loss 0.34223806858062744\n",
      "epoch 130: loss 0.39031118154525757\n",
      "epoch 131: loss 0.3300236761569977\n",
      "epoch 132: loss 0.32852715253829956\n",
      "epoch 133: loss 0.3551669418811798\n",
      "epoch 134: loss 0.3294979929924011\n",
      "epoch 135: loss 0.2658703923225403\n",
      "epoch 136: loss 0.23850306868553162\n",
      "epoch 0: loss 0.3977721929550171\n",
      "epoch 1: loss 0.3590785264968872\n",
      "epoch 2: loss 0.4531712532043457\n",
      "epoch 3: loss 0.381222128868103\n",
      "epoch 4: loss 0.3384893834590912\n",
      "epoch 5: loss 0.3173842430114746\n",
      "epoch 6: loss 0.2999052405357361\n",
      "epoch 7: loss 0.24018245935440063\n",
      "epoch 8: loss 0.4347834587097168\n",
      "epoch 9: loss 0.4786342978477478\n",
      "epoch 10: loss 0.37033993005752563\n",
      "epoch 11: loss 0.29087698459625244\n",
      "epoch 12: loss 0.33655571937561035\n",
      "epoch 13: loss 0.4364258050918579\n",
      "epoch 14: loss 0.3836252689361572\n",
      "epoch 15: loss 0.3159542679786682\n",
      "epoch 16: loss 0.33690401911735535\n",
      "epoch 17: loss 0.3671030104160309\n",
      "epoch 18: loss 0.42558610439300537\n",
      "epoch 19: loss 0.2835842967033386\n",
      "epoch 20: loss 0.3107661306858063\n",
      "epoch 21: loss 0.3539388179779053\n",
      "epoch 22: loss 0.337086021900177\n",
      "epoch 23: loss 0.38579514622688293\n",
      "epoch 24: loss 0.36855506896972656\n",
      "epoch 25: loss 0.40539026260375977\n",
      "epoch 26: loss 0.32522842288017273\n",
      "epoch 27: loss 0.362100213766098\n",
      "epoch 28: loss 0.4135643243789673\n",
      "epoch 29: loss 0.31793537735939026\n",
      "epoch 30: loss 0.2805677652359009\n",
      "epoch 31: loss 0.33973294496536255\n",
      "epoch 32: loss 0.3427056670188904\n",
      "epoch 33: loss 0.3491233289241791\n",
      "epoch 34: loss 0.3905485272407532\n",
      "epoch 35: loss 0.2639268636703491\n",
      "epoch 36: loss 0.3369487524032593\n",
      "epoch 37: loss 0.3123684823513031\n",
      "epoch 38: loss 0.21666790544986725\n",
      "epoch 39: loss 0.3316424489021301\n",
      "epoch 40: loss 0.32475942373275757\n",
      "epoch 41: loss 0.28518015146255493\n",
      "epoch 42: loss 0.29276853799819946\n",
      "epoch 43: loss 0.32183271646499634\n",
      "epoch 44: loss 0.36352911591529846\n",
      "epoch 45: loss 0.28815221786499023\n",
      "epoch 46: loss 0.3219295144081116\n",
      "epoch 47: loss 0.28378626704216003\n",
      "epoch 48: loss 0.2814006209373474\n",
      "epoch 49: loss 0.330089807510376\n",
      "epoch 50: loss 0.28431978821754456\n",
      "epoch 51: loss 0.42207056283950806\n",
      "epoch 52: loss 0.30531594157218933\n",
      "epoch 53: loss 0.37813591957092285\n",
      "epoch 54: loss 0.33627432584762573\n",
      "epoch 55: loss 0.30409932136535645\n",
      "epoch 56: loss 0.34747031331062317\n",
      "epoch 57: loss 0.26247894763946533\n",
      "epoch 58: loss 0.34578683972358704\n",
      "epoch 59: loss 0.24307329952716827\n",
      "epoch 60: loss 0.35689228773117065\n",
      "epoch 61: loss 0.21723537147045135\n",
      "epoch 62: loss 0.2345213145017624\n",
      "epoch 63: loss 0.3592509627342224\n",
      "epoch 64: loss 0.30290400981903076\n",
      "epoch 65: loss 0.4040480852127075\n",
      "epoch 66: loss 0.3484795093536377\n",
      "epoch 67: loss 0.38178250193595886\n",
      "epoch 68: loss 0.3686854839324951\n",
      "epoch 69: loss 0.43199780583381653\n",
      "epoch 70: loss 0.38864028453826904\n",
      "epoch 71: loss 0.3407158851623535\n",
      "epoch 72: loss 0.2736999988555908\n",
      "epoch 73: loss 0.35172295570373535\n",
      "epoch 74: loss 0.29656147956848145\n",
      "epoch 75: loss 0.3317001461982727\n",
      "epoch 76: loss 0.3031148314476013\n",
      "epoch 77: loss 0.4040999412536621\n",
      "epoch 78: loss 0.31427687406539917\n",
      "epoch 79: loss 0.345624178647995\n",
      "epoch 80: loss 0.37800133228302\n",
      "epoch 81: loss 0.2857155203819275\n",
      "epoch 82: loss 0.33525970578193665\n",
      "epoch 83: loss 0.38570109009742737\n",
      "epoch 84: loss 0.25287163257598877\n",
      "epoch 85: loss 0.31537172198295593\n",
      "epoch 86: loss 0.3539191782474518\n",
      "epoch 87: loss 0.3457579016685486\n",
      "epoch 88: loss 0.26851385831832886\n",
      "epoch 89: loss 0.330105185508728\n",
      "epoch 90: loss 0.2574999928474426\n",
      "epoch 91: loss 0.33351364731788635\n",
      "epoch 92: loss 0.28448057174682617\n",
      "epoch 93: loss 0.3288918137550354\n",
      "epoch 94: loss 0.2951749563217163\n",
      "epoch 95: loss 0.1943131983280182\n",
      "epoch 96: loss 0.22926604747772217\n",
      "epoch 97: loss 0.31474021077156067\n",
      "epoch 98: loss 0.3346940279006958\n",
      "epoch 99: loss 0.36707162857055664\n",
      "epoch 100: loss 0.46107402443885803\n",
      "epoch 101: loss 0.4308601915836334\n",
      "epoch 102: loss 0.28240132331848145\n",
      "epoch 103: loss 0.3279569745063782\n",
      "epoch 104: loss 0.27692359685897827\n",
      "epoch 105: loss 0.41600513458251953\n",
      "epoch 106: loss 0.3826378583908081\n",
      "epoch 107: loss 0.3268212080001831\n",
      "epoch 108: loss 0.34670668840408325\n",
      "epoch 109: loss 0.3631966710090637\n",
      "epoch 110: loss 0.3586719334125519\n",
      "epoch 111: loss 0.3953906297683716\n",
      "epoch 112: loss 0.39139434695243835\n",
      "epoch 113: loss 0.3262518644332886\n",
      "epoch 114: loss 0.21847409009933472\n",
      "epoch 115: loss 0.43550050258636475\n",
      "epoch 116: loss 0.3269290626049042\n",
      "epoch 117: loss 0.29858699440956116\n",
      "epoch 118: loss 0.33288809657096863\n",
      "epoch 119: loss 0.3726900815963745\n",
      "epoch 120: loss 0.29285305738449097\n",
      "epoch 121: loss 0.38746219873428345\n",
      "epoch 122: loss 0.29037031531333923\n",
      "epoch 123: loss 0.39703479409217834\n",
      "epoch 124: loss 0.32095620036125183\n",
      "epoch 125: loss 0.3247699737548828\n",
      "epoch 126: loss 0.36384284496307373\n",
      "epoch 127: loss 0.4050925374031067\n",
      "epoch 128: loss 0.28106483817100525\n",
      "epoch 129: loss 0.3413635492324829\n",
      "epoch 130: loss 0.38945937156677246\n",
      "epoch 131: loss 0.3322073221206665\n",
      "epoch 132: loss 0.32826077938079834\n",
      "epoch 133: loss 0.3543846011161804\n",
      "epoch 134: loss 0.3248845338821411\n",
      "epoch 135: loss 0.26284322142601013\n",
      "epoch 136: loss 0.2340393364429474\n",
      "epoch 0: loss 0.3979458212852478\n",
      "epoch 1: loss 0.35981374979019165\n",
      "epoch 2: loss 0.4485519826412201\n",
      "epoch 3: loss 0.3802328109741211\n",
      "epoch 4: loss 0.3452739715576172\n",
      "epoch 5: loss 0.30840998888015747\n",
      "epoch 6: loss 0.29387766122817993\n",
      "epoch 7: loss 0.24028989672660828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: loss 0.4445115327835083\n",
      "epoch 9: loss 0.4810005724430084\n",
      "epoch 10: loss 0.3670188784599304\n",
      "epoch 11: loss 0.28895318508148193\n",
      "epoch 12: loss 0.351140558719635\n",
      "epoch 13: loss 0.4336097538471222\n",
      "epoch 14: loss 0.36995428800582886\n",
      "epoch 15: loss 0.31237104535102844\n",
      "epoch 16: loss 0.33608192205429077\n",
      "epoch 17: loss 0.36572685837745667\n",
      "epoch 18: loss 0.42215844988822937\n",
      "epoch 19: loss 0.2804291844367981\n",
      "epoch 20: loss 0.30933836102485657\n",
      "epoch 21: loss 0.3514232039451599\n",
      "epoch 22: loss 0.3348492383956909\n",
      "epoch 23: loss 0.38103047013282776\n",
      "epoch 24: loss 0.3623407483100891\n",
      "epoch 25: loss 0.3994485139846802\n",
      "epoch 26: loss 0.3219192326068878\n",
      "epoch 27: loss 0.35781627893447876\n",
      "epoch 28: loss 0.4104622006416321\n",
      "epoch 29: loss 0.3188140392303467\n",
      "epoch 30: loss 0.2801325023174286\n",
      "epoch 31: loss 0.33996719121932983\n",
      "epoch 32: loss 0.34034180641174316\n",
      "epoch 33: loss 0.3499835729598999\n",
      "epoch 34: loss 0.3902769684791565\n",
      "epoch 35: loss 0.2614765465259552\n",
      "epoch 36: loss 0.338255912065506\n",
      "epoch 37: loss 0.3111318349838257\n",
      "epoch 38: loss 0.21380515396595\n",
      "epoch 39: loss 0.3266679048538208\n",
      "epoch 40: loss 0.318774938583374\n",
      "epoch 41: loss 0.28406718373298645\n",
      "epoch 42: loss 0.2908993363380432\n",
      "epoch 43: loss 0.3168819546699524\n",
      "epoch 44: loss 0.36930257081985474\n",
      "epoch 45: loss 0.2894037067890167\n",
      "epoch 46: loss 0.3289010226726532\n",
      "epoch 47: loss 0.2839111089706421\n",
      "epoch 48: loss 0.2792438864707947\n",
      "epoch 49: loss 0.33574244379997253\n",
      "epoch 50: loss 0.2894694209098816\n",
      "epoch 51: loss 0.4189888834953308\n",
      "epoch 52: loss 0.3053121566772461\n",
      "epoch 53: loss 0.37695035338401794\n",
      "epoch 54: loss 0.3361319601535797\n",
      "epoch 55: loss 0.3038744330406189\n",
      "epoch 56: loss 0.3471708297729492\n",
      "epoch 57: loss 0.25973427295684814\n",
      "epoch 58: loss 0.34357064962387085\n",
      "epoch 59: loss 0.24359582364559174\n",
      "epoch 60: loss 0.3553328812122345\n",
      "epoch 61: loss 0.21927808225154877\n",
      "epoch 62: loss 0.23489320278167725\n",
      "epoch 63: loss 0.3598877787590027\n",
      "epoch 64: loss 0.3022101819515228\n",
      "epoch 65: loss 0.40338990092277527\n",
      "epoch 66: loss 0.3517000079154968\n",
      "epoch 67: loss 0.3805493712425232\n",
      "epoch 68: loss 0.3670281171798706\n",
      "epoch 69: loss 0.4337976276874542\n",
      "epoch 70: loss 0.39213764667510986\n",
      "epoch 71: loss 0.33354952931404114\n",
      "epoch 72: loss 0.2769748270511627\n",
      "epoch 73: loss 0.3465646207332611\n",
      "epoch 74: loss 0.2924138009548187\n",
      "epoch 75: loss 0.32954737544059753\n",
      "epoch 76: loss 0.30236440896987915\n",
      "epoch 77: loss 0.4050357937812805\n",
      "epoch 78: loss 0.3137330710887909\n",
      "epoch 79: loss 0.34587669372558594\n",
      "epoch 80: loss 0.37813666462898254\n",
      "epoch 81: loss 0.2890579402446747\n",
      "epoch 82: loss 0.3363719582557678\n",
      "epoch 83: loss 0.3853745460510254\n",
      "epoch 84: loss 0.25235268473625183\n",
      "epoch 85: loss 0.3152623474597931\n",
      "epoch 86: loss 0.35197120904922485\n",
      "epoch 87: loss 0.3425535559654236\n",
      "epoch 88: loss 0.26754480600357056\n",
      "epoch 89: loss 0.32894808053970337\n",
      "epoch 90: loss 0.2590775787830353\n",
      "epoch 91: loss 0.32987362146377563\n",
      "epoch 92: loss 0.28481802344322205\n",
      "epoch 93: loss 0.3367875814437866\n",
      "epoch 94: loss 0.2963706851005554\n",
      "epoch 95: loss 0.1919647753238678\n",
      "epoch 96: loss 0.2352323979139328\n",
      "epoch 97: loss 0.31990212202072144\n",
      "epoch 98: loss 0.3404430150985718\n",
      "epoch 99: loss 0.36770862340927124\n",
      "epoch 100: loss 0.45876437425613403\n",
      "epoch 101: loss 0.42859306931495667\n",
      "epoch 102: loss 0.28109899163246155\n",
      "epoch 103: loss 0.32581135630607605\n",
      "epoch 104: loss 0.27613765001296997\n",
      "epoch 105: loss 0.41474562883377075\n",
      "epoch 106: loss 0.38240525126457214\n",
      "epoch 107: loss 0.3261576294898987\n",
      "epoch 108: loss 0.34779056906700134\n",
      "epoch 109: loss 0.36296629905700684\n",
      "epoch 110: loss 0.35592058300971985\n",
      "epoch 111: loss 0.3959839344024658\n",
      "epoch 112: loss 0.39259299635887146\n",
      "epoch 113: loss 0.325558066368103\n",
      "epoch 114: loss 0.22201623022556305\n",
      "epoch 115: loss 0.43032440543174744\n",
      "epoch 116: loss 0.3241417407989502\n",
      "epoch 117: loss 0.29448750615119934\n",
      "epoch 118: loss 0.32988545298576355\n",
      "epoch 119: loss 0.3712616562843323\n",
      "epoch 120: loss 0.2882038950920105\n",
      "epoch 121: loss 0.38861826062202454\n",
      "epoch 122: loss 0.28833574056625366\n",
      "epoch 123: loss 0.3940279185771942\n",
      "epoch 124: loss 0.31923526525497437\n",
      "epoch 125: loss 0.3282167315483093\n",
      "epoch 126: loss 0.367880254983902\n",
      "epoch 127: loss 0.4112231433391571\n",
      "epoch 128: loss 0.28013306856155396\n",
      "epoch 129: loss 0.34369656443595886\n",
      "epoch 130: loss 0.3833281397819519\n",
      "epoch 131: loss 0.33115488290786743\n",
      "epoch 132: loss 0.3301559090614319\n",
      "epoch 133: loss 0.35467126965522766\n",
      "epoch 134: loss 0.3228653073310852\n",
      "epoch 135: loss 0.26162058115005493\n",
      "epoch 136: loss 0.23644761741161346\n",
      "epoch 0: loss 0.40003132820129395\n",
      "epoch 1: loss 0.35620588064193726\n",
      "epoch 2: loss 0.4471629559993744\n",
      "epoch 3: loss 0.3787284195423126\n",
      "epoch 4: loss 0.3513054847717285\n",
      "epoch 5: loss 0.30223503708839417\n",
      "epoch 6: loss 0.2954912483692169\n",
      "epoch 7: loss 0.24217313528060913\n",
      "epoch 8: loss 0.45021700859069824\n",
      "epoch 9: loss 0.48230624198913574\n",
      "epoch 10: loss 0.36041754484176636\n",
      "epoch 11: loss 0.28933247923851013\n",
      "epoch 12: loss 0.35439231991767883\n",
      "epoch 13: loss 0.4349960684776306\n",
      "epoch 14: loss 0.3662216067314148\n",
      "epoch 15: loss 0.31350177526474\n",
      "epoch 16: loss 0.33707505464553833\n",
      "epoch 17: loss 0.368446946144104\n",
      "epoch 18: loss 0.4210996925830841\n",
      "epoch 19: loss 0.28041067719459534\n",
      "epoch 20: loss 0.3104214072227478\n",
      "epoch 21: loss 0.35082101821899414\n",
      "epoch 22: loss 0.336652547121048\n",
      "epoch 23: loss 0.3828972578048706\n",
      "epoch 24: loss 0.3628789782524109\n",
      "epoch 25: loss 0.40268632769584656\n",
      "epoch 26: loss 0.32210153341293335\n",
      "epoch 27: loss 0.3600209653377533\n",
      "epoch 28: loss 0.40768295526504517\n",
      "epoch 29: loss 0.32034793496131897\n",
      "epoch 30: loss 0.2795445919036865\n",
      "epoch 31: loss 0.34048107266426086\n",
      "epoch 32: loss 0.3422940969467163\n",
      "epoch 33: loss 0.35132041573524475\n",
      "epoch 34: loss 0.39278194308280945\n",
      "epoch 35: loss 0.2589638829231262\n",
      "epoch 36: loss 0.335530549287796\n",
      "epoch 37: loss 0.3154044449329376\n",
      "epoch 38: loss 0.2183043360710144\n",
      "epoch 39: loss 0.3280431628227234\n",
      "epoch 40: loss 0.3179491460323334\n",
      "epoch 41: loss 0.28690066933631897\n",
      "epoch 42: loss 0.2928553521633148\n",
      "epoch 43: loss 0.31780022382736206\n",
      "epoch 44: loss 0.36792826652526855\n",
      "epoch 45: loss 0.29042333364486694\n",
      "epoch 46: loss 0.3349798917770386\n",
      "epoch 47: loss 0.28709107637405396\n",
      "epoch 48: loss 0.27946937084198\n",
      "epoch 49: loss 0.33931565284729004\n",
      "epoch 50: loss 0.290718138217926\n",
      "epoch 51: loss 0.41861191391944885\n",
      "epoch 52: loss 0.30597108602523804\n",
      "epoch 53: loss 0.3781283497810364\n",
      "epoch 54: loss 0.3347472548484802\n",
      "epoch 55: loss 0.3023653030395508\n",
      "epoch 56: loss 0.34619981050491333\n",
      "epoch 57: loss 0.25781798362731934\n",
      "epoch 58: loss 0.3415360152721405\n",
      "epoch 59: loss 0.24631643295288086\n",
      "epoch 60: loss 0.3470609784126282\n",
      "epoch 61: loss 0.2205718606710434\n",
      "epoch 62: loss 0.23576195538043976\n",
      "epoch 63: loss 0.3606233596801758\n",
      "epoch 64: loss 0.3042738735675812\n",
      "epoch 65: loss 0.4036887586116791\n",
      "epoch 66: loss 0.3452327847480774\n",
      "epoch 67: loss 0.37948960065841675\n",
      "epoch 68: loss 0.3677801191806793\n",
      "epoch 69: loss 0.4273768663406372\n",
      "epoch 70: loss 0.39100420475006104\n",
      "epoch 71: loss 0.3311901092529297\n",
      "epoch 72: loss 0.27294155955314636\n",
      "epoch 73: loss 0.3491811156272888\n",
      "epoch 74: loss 0.28988468647003174\n",
      "epoch 75: loss 0.3308335542678833\n",
      "epoch 76: loss 0.30511677265167236\n",
      "epoch 77: loss 0.40343526005744934\n",
      "epoch 78: loss 0.3147200047969818\n",
      "epoch 79: loss 0.34154704213142395\n",
      "epoch 80: loss 0.3750779628753662\n",
      "epoch 81: loss 0.28374478220939636\n",
      "epoch 82: loss 0.3308737874031067\n",
      "epoch 83: loss 0.38582587242126465\n",
      "epoch 84: loss 0.24729350209236145\n",
      "epoch 85: loss 0.31524544954299927\n",
      "epoch 86: loss 0.34887629747390747\n",
      "epoch 87: loss 0.34583580493927\n",
      "epoch 88: loss 0.27098533511161804\n",
      "epoch 89: loss 0.323077529668808\n",
      "epoch 90: loss 0.2580338716506958\n",
      "epoch 91: loss 0.33343619108200073\n",
      "epoch 92: loss 0.279241681098938\n",
      "epoch 93: loss 0.32686924934387207\n",
      "epoch 94: loss 0.29552048444747925\n",
      "epoch 95: loss 0.19578170776367188\n",
      "epoch 96: loss 0.2192634642124176\n",
      "epoch 97: loss 0.30965176224708557\n",
      "epoch 98: loss 0.3299601972103119\n",
      "epoch 99: loss 0.36397093534469604\n",
      "epoch 100: loss 0.45525041222572327\n",
      "epoch 101: loss 0.4373805522918701\n",
      "epoch 102: loss 0.2684754729270935\n",
      "epoch 103: loss 0.33915039896965027\n",
      "epoch 104: loss 0.28110983967781067\n",
      "epoch 105: loss 0.4200659990310669\n",
      "epoch 106: loss 0.38146376609802246\n",
      "epoch 107: loss 0.32051882147789\n",
      "epoch 108: loss 0.34502848982810974\n",
      "epoch 109: loss 0.36082446575164795\n",
      "epoch 110: loss 0.3617318570613861\n",
      "epoch 111: loss 0.40106189250946045\n",
      "epoch 112: loss 0.3901001513004303\n",
      "epoch 113: loss 0.31979265809059143\n",
      "epoch 114: loss 0.20859628915786743\n",
      "epoch 115: loss 0.4431838095188141\n",
      "epoch 116: loss 0.32576242089271545\n",
      "epoch 117: loss 0.2890581488609314\n",
      "epoch 118: loss 0.3274451494216919\n",
      "epoch 119: loss 0.36661314964294434\n",
      "epoch 120: loss 0.28616541624069214\n",
      "epoch 121: loss 0.38377612829208374\n",
      "epoch 122: loss 0.2904180884361267\n",
      "epoch 123: loss 0.39438092708587646\n",
      "epoch 124: loss 0.310091108083725\n",
      "epoch 125: loss 0.3279324769973755\n",
      "epoch 126: loss 0.37025874853134155\n",
      "epoch 127: loss 0.40969520807266235\n",
      "epoch 128: loss 0.27281758189201355\n",
      "epoch 129: loss 0.3435150980949402\n",
      "epoch 130: loss 0.3745824992656708\n",
      "epoch 131: loss 0.34100645780563354\n",
      "epoch 132: loss 0.3242054879665375\n",
      "epoch 133: loss 0.35727453231811523\n",
      "epoch 134: loss 0.3192364573478699\n",
      "epoch 135: loss 0.25763070583343506\n",
      "epoch 136: loss 0.23458771407604218\n",
      "epoch 0: loss 0.3860684931278229\n",
      "epoch 1: loss 0.34176671504974365\n",
      "epoch 2: loss 0.41668176651000977\n",
      "epoch 3: loss 0.38739192485809326\n",
      "epoch 4: loss 0.3595922887325287\n",
      "epoch 5: loss 0.2762864828109741\n",
      "epoch 6: loss 0.2944817543029785\n",
      "epoch 7: loss 0.250879168510437\n",
      "epoch 8: loss 0.46857643127441406\n",
      "epoch 9: loss 0.4779807925224304\n",
      "epoch 10: loss 0.3429597020149231\n",
      "epoch 11: loss 0.298918753862381\n",
      "epoch 12: loss 0.40592116117477417\n",
      "epoch 13: loss 0.43942585587501526\n",
      "epoch 14: loss 0.3482295870780945\n",
      "epoch 15: loss 0.32301050424575806\n",
      "epoch 16: loss 0.3580171763896942\n",
      "epoch 17: loss 0.38486596941947937\n",
      "epoch 18: loss 0.41055506467819214\n",
      "epoch 19: loss 0.289070725440979\n",
      "epoch 20: loss 0.329791784286499\n",
      "epoch 21: loss 0.36640602350234985\n",
      "epoch 22: loss 0.3511773943901062\n",
      "epoch 23: loss 0.38393643498420715\n",
      "epoch 24: loss 0.36447423696517944\n",
      "epoch 25: loss 0.39745578169822693\n",
      "epoch 26: loss 0.3233549892902374\n",
      "epoch 27: loss 0.36345794796943665\n",
      "epoch 28: loss 0.4118807315826416\n",
      "epoch 29: loss 0.3174794912338257\n",
      "epoch 30: loss 0.2828570604324341\n",
      "epoch 31: loss 0.34478673338890076\n",
      "epoch 32: loss 0.3378381133079529\n",
      "epoch 33: loss 0.35094010829925537\n",
      "epoch 34: loss 0.38962334394454956\n",
      "epoch 35: loss 0.2570434510707855\n",
      "epoch 36: loss 0.3376057744026184\n",
      "epoch 37: loss 0.32381781935691833\n",
      "epoch 38: loss 0.21803082525730133\n",
      "epoch 39: loss 0.3258393406867981\n",
      "epoch 40: loss 0.31376492977142334\n",
      "epoch 41: loss 0.28998786211013794\n",
      "epoch 42: loss 0.295107364654541\n",
      "epoch 43: loss 0.3151021897792816\n",
      "epoch 44: loss 0.3735831379890442\n",
      "epoch 45: loss 0.2910776436328888\n",
      "epoch 46: loss 0.33685553073883057\n",
      "epoch 47: loss 0.2878151535987854\n",
      "epoch 48: loss 0.27737075090408325\n",
      "epoch 49: loss 0.34037524461746216\n",
      "epoch 50: loss 0.2915763258934021\n",
      "epoch 51: loss 0.4172074794769287\n",
      "epoch 52: loss 0.305492103099823\n",
      "epoch 53: loss 0.3781679570674896\n",
      "epoch 54: loss 0.3327453136444092\n",
      "epoch 55: loss 0.30410486459732056\n",
      "epoch 56: loss 0.3455634117126465\n",
      "epoch 57: loss 0.25632548332214355\n",
      "epoch 58: loss 0.3434901833534241\n",
      "epoch 59: loss 0.24531590938568115\n",
      "epoch 60: loss 0.35033118724823\n",
      "epoch 61: loss 0.22240731120109558\n",
      "epoch 62: loss 0.2371358871459961\n",
      "epoch 63: loss 0.35766565799713135\n",
      "epoch 64: loss 0.30120229721069336\n",
      "epoch 65: loss 0.40317070484161377\n",
      "epoch 66: loss 0.3453028202056885\n",
      "epoch 67: loss 0.37967824935913086\n",
      "epoch 68: loss 0.3673872649669647\n",
      "epoch 69: loss 0.4262385070323944\n",
      "epoch 70: loss 0.3893350660800934\n",
      "epoch 71: loss 0.3309146761894226\n",
      "epoch 72: loss 0.274474173784256\n",
      "epoch 73: loss 0.3469119071960449\n",
      "epoch 74: loss 0.28857070207595825\n",
      "epoch 75: loss 0.3296352028846741\n",
      "epoch 76: loss 0.30263829231262207\n",
      "epoch 77: loss 0.4055477976799011\n",
      "epoch 78: loss 0.3132912814617157\n",
      "epoch 79: loss 0.34437984228134155\n",
      "epoch 80: loss 0.3770346939563751\n",
      "epoch 81: loss 0.28601229190826416\n",
      "epoch 82: loss 0.33318376541137695\n",
      "epoch 83: loss 0.38364502787590027\n",
      "epoch 84: loss 0.2479015737771988\n",
      "epoch 85: loss 0.3151940703392029\n",
      "epoch 86: loss 0.3507809042930603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 87: loss 0.34567925333976746\n",
      "epoch 88: loss 0.2673581838607788\n",
      "epoch 89: loss 0.3278135359287262\n",
      "epoch 90: loss 0.25941431522369385\n",
      "epoch 91: loss 0.3291699290275574\n",
      "epoch 92: loss 0.28328847885131836\n",
      "epoch 93: loss 0.3344647288322449\n",
      "epoch 94: loss 0.29377153515815735\n",
      "epoch 95: loss 0.19288620352745056\n",
      "epoch 96: loss 0.23204563558101654\n",
      "epoch 97: loss 0.3169374167919159\n",
      "epoch 98: loss 0.33805811405181885\n",
      "epoch 99: loss 0.36979299783706665\n",
      "epoch 100: loss 0.46400195360183716\n",
      "epoch 101: loss 0.39490655064582825\n",
      "epoch 102: loss 0.28264981508255005\n",
      "epoch 103: loss 0.3159555196762085\n",
      "epoch 104: loss 0.26979291439056396\n",
      "epoch 105: loss 0.39391815662384033\n",
      "epoch 106: loss 0.37200799584388733\n",
      "epoch 107: loss 0.3169001340866089\n",
      "epoch 108: loss 0.35039204359054565\n",
      "epoch 109: loss 0.3673100769519806\n",
      "epoch 110: loss 0.35470709204673767\n",
      "epoch 111: loss 0.38411325216293335\n",
      "epoch 112: loss 0.38238024711608887\n",
      "epoch 113: loss 0.32737112045288086\n",
      "epoch 114: loss 0.2335130274295807\n",
      "epoch 115: loss 0.4150638282299042\n",
      "epoch 116: loss 0.3170086145401001\n",
      "epoch 117: loss 0.28978583216667175\n",
      "epoch 118: loss 0.330679714679718\n",
      "epoch 119: loss 0.3746972382068634\n",
      "epoch 120: loss 0.290283203125\n",
      "epoch 121: loss 0.3953678607940674\n",
      "epoch 122: loss 0.2769620716571808\n",
      "epoch 123: loss 0.39355480670928955\n",
      "epoch 124: loss 0.32260841131210327\n",
      "epoch 125: loss 0.3310668170452118\n",
      "epoch 126: loss 0.3610968589782715\n",
      "epoch 127: loss 0.39915362000465393\n",
      "epoch 128: loss 0.2758127450942993\n",
      "epoch 129: loss 0.34266066551208496\n",
      "epoch 130: loss 0.38414037227630615\n",
      "epoch 131: loss 0.32850587368011475\n",
      "epoch 132: loss 0.3241026997566223\n",
      "epoch 133: loss 0.3533518314361572\n",
      "epoch 134: loss 0.3253907859325409\n",
      "epoch 135: loss 0.26007312536239624\n",
      "epoch 136: loss 0.2349054366350174\n",
      "epoch 0: loss 0.3977258801460266\n",
      "epoch 1: loss 0.3536369204521179\n",
      "epoch 2: loss 0.4459038972854614\n",
      "epoch 3: loss 0.37463873624801636\n",
      "epoch 4: loss 0.35827934741973877\n",
      "epoch 5: loss 0.2902083694934845\n",
      "epoch 6: loss 0.2916245758533478\n",
      "epoch 7: loss 0.2401759922504425\n",
      "epoch 8: loss 0.45661628246307373\n",
      "epoch 9: loss 0.4845118522644043\n",
      "epoch 10: loss 0.35462528467178345\n",
      "epoch 11: loss 0.28404784202575684\n",
      "epoch 12: loss 0.3888091444969177\n",
      "epoch 13: loss 0.42973989248275757\n",
      "epoch 14: loss 0.3486100435256958\n",
      "epoch 15: loss 0.30835357308387756\n",
      "epoch 16: loss 0.3376651704311371\n",
      "epoch 17: loss 0.36594605445861816\n",
      "epoch 18: loss 0.4079129099845886\n",
      "epoch 19: loss 0.2779664397239685\n",
      "epoch 20: loss 0.31261077523231506\n",
      "epoch 21: loss 0.3520966172218323\n",
      "epoch 22: loss 0.3336515426635742\n",
      "epoch 23: loss 0.37852713465690613\n",
      "epoch 24: loss 0.35854387283325195\n",
      "epoch 25: loss 0.39658039808273315\n",
      "epoch 26: loss 0.3210707902908325\n",
      "epoch 27: loss 0.3546086549758911\n",
      "epoch 28: loss 0.4064224362373352\n",
      "epoch 29: loss 0.3177655339241028\n",
      "epoch 30: loss 0.28070759773254395\n",
      "epoch 31: loss 0.3409811854362488\n",
      "epoch 32: loss 0.33776748180389404\n",
      "epoch 33: loss 0.3476802110671997\n",
      "epoch 34: loss 0.389771431684494\n",
      "epoch 35: loss 0.2592770755290985\n",
      "epoch 36: loss 0.3355180323123932\n",
      "epoch 37: loss 0.31338468194007874\n",
      "epoch 38: loss 0.21551448106765747\n",
      "epoch 39: loss 0.32351812720298767\n",
      "epoch 40: loss 0.3165169358253479\n",
      "epoch 41: loss 0.2827981114387512\n",
      "epoch 42: loss 0.28653261065483093\n",
      "epoch 43: loss 0.31893157958984375\n",
      "epoch 44: loss 0.3631647825241089\n",
      "epoch 45: loss 0.28437361121177673\n",
      "epoch 46: loss 0.32667669653892517\n",
      "epoch 47: loss 0.28467607498168945\n",
      "epoch 48: loss 0.27452874183654785\n",
      "epoch 49: loss 0.33939898014068604\n",
      "epoch 50: loss 0.291044682264328\n",
      "epoch 51: loss 0.4226427376270294\n",
      "epoch 52: loss 0.30477580428123474\n",
      "epoch 53: loss 0.3754171133041382\n",
      "epoch 54: loss 0.3367234766483307\n",
      "epoch 55: loss 0.30280593037605286\n",
      "epoch 56: loss 0.3487889766693115\n",
      "epoch 57: loss 0.2554815411567688\n",
      "epoch 58: loss 0.33833807706832886\n",
      "epoch 59: loss 0.2463572472333908\n",
      "epoch 60: loss 0.34466615319252014\n",
      "epoch 61: loss 0.22016729414463043\n",
      "epoch 62: loss 0.23779374361038208\n",
      "epoch 63: loss 0.3565722107887268\n",
      "epoch 64: loss 0.29804113507270813\n",
      "epoch 65: loss 0.4020519256591797\n",
      "epoch 66: loss 0.3430309295654297\n",
      "epoch 67: loss 0.376924604177475\n",
      "epoch 68: loss 0.36571115255355835\n",
      "epoch 69: loss 0.42646321654319763\n",
      "epoch 70: loss 0.3851783871650696\n",
      "epoch 71: loss 0.3317883014678955\n",
      "epoch 72: loss 0.2674841582775116\n",
      "epoch 73: loss 0.3494873344898224\n",
      "epoch 74: loss 0.2898922562599182\n",
      "epoch 75: loss 0.3312070965766907\n",
      "epoch 76: loss 0.30253055691719055\n",
      "epoch 77: loss 0.40275001525878906\n",
      "epoch 78: loss 0.3132667541503906\n",
      "epoch 79: loss 0.3415488600730896\n",
      "epoch 80: loss 0.3776741623878479\n",
      "epoch 81: loss 0.2770978808403015\n",
      "epoch 82: loss 0.33289843797683716\n",
      "epoch 83: loss 0.38232535123825073\n",
      "epoch 84: loss 0.25421297550201416\n",
      "epoch 85: loss 0.3143216371536255\n",
      "epoch 86: loss 0.35117772221565247\n",
      "epoch 87: loss 0.3463025689125061\n",
      "epoch 88: loss 0.26698631048202515\n",
      "epoch 89: loss 0.3315902352333069\n",
      "epoch 90: loss 0.26139768958091736\n",
      "epoch 91: loss 0.3280497193336487\n",
      "epoch 92: loss 0.29016149044036865\n",
      "epoch 93: loss 0.3327324688434601\n",
      "epoch 94: loss 0.29340195655822754\n",
      "epoch 95: loss 0.19119659066200256\n",
      "epoch 96: loss 0.24305245280265808\n",
      "epoch 97: loss 0.3216251730918884\n",
      "epoch 98: loss 0.34577375650405884\n",
      "epoch 99: loss 0.371982216835022\n",
      "epoch 100: loss 0.4703691303730011\n",
      "epoch 101: loss 0.3535541296005249\n",
      "epoch 102: loss 0.2973736524581909\n",
      "epoch 103: loss 0.3128306567668915\n",
      "epoch 104: loss 0.27171385288238525\n",
      "epoch 105: loss 0.3756924867630005\n",
      "epoch 106: loss 0.3624045252799988\n",
      "epoch 107: loss 0.31594836711883545\n",
      "epoch 108: loss 0.34765228629112244\n",
      "epoch 109: loss 0.3771827220916748\n",
      "epoch 110: loss 0.3568469285964966\n",
      "epoch 111: loss 0.3818361163139343\n",
      "epoch 112: loss 0.3765183091163635\n",
      "epoch 113: loss 0.32321321964263916\n",
      "epoch 114: loss 0.24615108966827393\n",
      "epoch 115: loss 0.40449023246765137\n",
      "epoch 116: loss 0.31234562397003174\n",
      "epoch 117: loss 0.2849894165992737\n",
      "epoch 118: loss 0.32866954803466797\n",
      "epoch 119: loss 0.3749757409095764\n",
      "epoch 120: loss 0.2909504771232605\n",
      "epoch 121: loss 0.4016472399234772\n",
      "epoch 122: loss 0.2710842192173004\n",
      "epoch 123: loss 0.3939204812049866\n",
      "epoch 124: loss 0.31768858432769775\n",
      "epoch 125: loss 0.3333480954170227\n",
      "epoch 126: loss 0.3593783378601074\n",
      "epoch 127: loss 0.3915553689002991\n",
      "epoch 128: loss 0.27580589056015015\n",
      "epoch 129: loss 0.34248173236846924\n",
      "epoch 130: loss 0.3814713954925537\n",
      "epoch 131: loss 0.3266890048980713\n",
      "epoch 132: loss 0.3226661682128906\n",
      "epoch 133: loss 0.3539295196533203\n",
      "epoch 134: loss 0.32424408197402954\n",
      "epoch 135: loss 0.2613055408000946\n",
      "epoch 136: loss 0.23480172455310822\n",
      "epoch 0: loss 0.3933212161064148\n",
      "epoch 1: loss 0.35183167457580566\n",
      "epoch 2: loss 0.44097384810447693\n",
      "epoch 3: loss 0.37489670515060425\n",
      "epoch 4: loss 0.36214348673820496\n",
      "epoch 5: loss 0.29206138849258423\n",
      "epoch 6: loss 0.2891867756843567\n",
      "epoch 7: loss 0.24278029799461365\n",
      "epoch 8: loss 0.4698109030723572\n",
      "epoch 9: loss 0.49364203214645386\n",
      "epoch 10: loss 0.3594541549682617\n",
      "epoch 11: loss 0.2846619486808777\n",
      "epoch 12: loss 0.3902391195297241\n",
      "epoch 13: loss 0.4290830194950104\n",
      "epoch 14: loss 0.34788021445274353\n",
      "epoch 15: loss 0.3079869747161865\n",
      "epoch 16: loss 0.3382672667503357\n",
      "epoch 17: loss 0.366771399974823\n",
      "epoch 18: loss 0.4088004231452942\n",
      "epoch 19: loss 0.27846378087997437\n",
      "epoch 20: loss 0.31231391429901123\n",
      "epoch 21: loss 0.3498271703720093\n",
      "epoch 22: loss 0.33433860540390015\n",
      "epoch 23: loss 0.3780301511287689\n",
      "epoch 24: loss 0.35900408029556274\n",
      "epoch 25: loss 0.39692068099975586\n",
      "epoch 26: loss 0.321372389793396\n",
      "epoch 27: loss 0.35409286618232727\n",
      "epoch 28: loss 0.4033820927143097\n",
      "epoch 29: loss 0.3182947635650635\n",
      "epoch 30: loss 0.2785602807998657\n",
      "epoch 31: loss 0.3410605490207672\n",
      "epoch 32: loss 0.3383738398551941\n",
      "epoch 33: loss 0.3479319214820862\n",
      "epoch 34: loss 0.39165568351745605\n",
      "epoch 35: loss 0.2575763165950775\n",
      "epoch 36: loss 0.33488309383392334\n",
      "epoch 37: loss 0.31371888518333435\n",
      "epoch 38: loss 0.21586668491363525\n",
      "epoch 39: loss 0.3215287923812866\n",
      "epoch 40: loss 0.3132783770561218\n",
      "epoch 41: loss 0.28276997804641724\n",
      "epoch 42: loss 0.28727394342422485\n",
      "epoch 43: loss 0.31926244497299194\n",
      "epoch 44: loss 0.36124101281166077\n",
      "epoch 45: loss 0.2847028374671936\n",
      "epoch 46: loss 0.3271690607070923\n",
      "epoch 47: loss 0.2857797145843506\n",
      "epoch 48: loss 0.2751237750053406\n",
      "epoch 49: loss 0.3356112241744995\n",
      "epoch 50: loss 0.2915353775024414\n",
      "epoch 51: loss 0.41609063744544983\n",
      "epoch 52: loss 0.3040043115615845\n",
      "epoch 53: loss 0.372958242893219\n",
      "epoch 54: loss 0.33472731709480286\n",
      "epoch 55: loss 0.30310454964637756\n",
      "epoch 56: loss 0.3487653136253357\n",
      "epoch 57: loss 0.2539430260658264\n",
      "epoch 58: loss 0.3416956663131714\n",
      "epoch 59: loss 0.24592770636081696\n",
      "epoch 60: loss 0.3461434245109558\n",
      "epoch 61: loss 0.2198788821697235\n",
      "epoch 62: loss 0.2382316291332245\n",
      "epoch 63: loss 0.35381078720092773\n",
      "epoch 64: loss 0.2961709499359131\n",
      "epoch 65: loss 0.400212824344635\n",
      "epoch 66: loss 0.34146976470947266\n",
      "epoch 67: loss 0.37849485874176025\n",
      "epoch 68: loss 0.36463662981987\n",
      "epoch 69: loss 0.42561429738998413\n",
      "epoch 70: loss 0.38205015659332275\n",
      "epoch 71: loss 0.33633244037628174\n",
      "epoch 72: loss 0.26799044013023376\n",
      "epoch 73: loss 0.3497655391693115\n",
      "epoch 74: loss 0.28925228118896484\n",
      "epoch 75: loss 0.33349645137786865\n",
      "epoch 76: loss 0.30375802516937256\n",
      "epoch 77: loss 0.4036468267440796\n",
      "epoch 78: loss 0.313356876373291\n",
      "epoch 79: loss 0.3378991484642029\n",
      "epoch 80: loss 0.3738904595375061\n",
      "epoch 81: loss 0.28242987394332886\n",
      "epoch 82: loss 0.3317731022834778\n",
      "epoch 83: loss 0.38194581866264343\n",
      "epoch 84: loss 0.24975037574768066\n",
      "epoch 85: loss 0.3149039149284363\n",
      "epoch 86: loss 0.34699952602386475\n",
      "epoch 87: loss 0.34467414021492004\n",
      "epoch 88: loss 0.2680264115333557\n",
      "epoch 89: loss 0.32664623856544495\n",
      "epoch 90: loss 0.261582612991333\n",
      "epoch 91: loss 0.32757049798965454\n",
      "epoch 92: loss 0.2790057063102722\n",
      "epoch 93: loss 0.3379741311073303\n",
      "epoch 94: loss 0.2946135997772217\n",
      "epoch 95: loss 0.19228841364383698\n",
      "epoch 96: loss 0.22951659560203552\n",
      "epoch 97: loss 0.3178115487098694\n",
      "epoch 98: loss 0.3424045741558075\n",
      "epoch 99: loss 0.37325623631477356\n",
      "epoch 100: loss 0.470860093832016\n",
      "epoch 101: loss 0.35109490156173706\n",
      "epoch 102: loss 0.28770971298217773\n",
      "epoch 103: loss 0.30766937136650085\n",
      "epoch 104: loss 0.2673901915550232\n",
      "epoch 105: loss 0.37880104780197144\n",
      "epoch 106: loss 0.36413949728012085\n",
      "epoch 107: loss 0.3106171786785126\n",
      "epoch 108: loss 0.34842202067375183\n",
      "epoch 109: loss 0.3736298680305481\n",
      "epoch 110: loss 0.35692644119262695\n",
      "epoch 111: loss 0.38145512342453003\n",
      "epoch 112: loss 0.3727130889892578\n",
      "epoch 113: loss 0.32060566544532776\n",
      "epoch 114: loss 0.2444581389427185\n",
      "epoch 115: loss 0.40232083201408386\n",
      "epoch 116: loss 0.31006520986557007\n",
      "epoch 117: loss 0.2826648950576782\n",
      "epoch 118: loss 0.32729360461235046\n",
      "epoch 119: loss 0.3708447217941284\n",
      "epoch 120: loss 0.28784793615341187\n",
      "epoch 121: loss 0.3998427987098694\n",
      "epoch 122: loss 0.2692839801311493\n",
      "epoch 123: loss 0.3927868604660034\n",
      "epoch 124: loss 0.3188316822052002\n",
      "epoch 125: loss 0.3347967267036438\n",
      "epoch 126: loss 0.35759103298187256\n",
      "epoch 127: loss 0.3970819115638733\n",
      "epoch 128: loss 0.27281084656715393\n",
      "epoch 129: loss 0.3418053388595581\n",
      "epoch 130: loss 0.37672585248947144\n",
      "epoch 131: loss 0.32896023988723755\n",
      "epoch 132: loss 0.32219621539115906\n",
      "epoch 133: loss 0.35445916652679443\n",
      "epoch 134: loss 0.3205908238887787\n",
      "epoch 135: loss 0.2591584026813507\n",
      "epoch 136: loss 0.23306426405906677\n",
      "epoch 0: loss 0.39236438274383545\n",
      "epoch 1: loss 0.3509312570095062\n",
      "epoch 2: loss 0.4358421862125397\n",
      "epoch 3: loss 0.3775274157524109\n",
      "epoch 4: loss 0.3727809488773346\n",
      "epoch 5: loss 0.2866545617580414\n",
      "epoch 6: loss 0.2916482388973236\n",
      "epoch 7: loss 0.24800008535385132\n",
      "epoch 8: loss 0.48012566566467285\n",
      "epoch 9: loss 0.49260419607162476\n",
      "epoch 10: loss 0.35533004999160767\n",
      "epoch 11: loss 0.2898314595222473\n",
      "epoch 12: loss 0.4070751667022705\n",
      "epoch 13: loss 0.428767591714859\n",
      "epoch 14: loss 0.3443232774734497\n",
      "epoch 15: loss 0.30815011262893677\n",
      "epoch 16: loss 0.3451513648033142\n",
      "epoch 17: loss 0.3716111481189728\n",
      "epoch 18: loss 0.4064249098300934\n",
      "epoch 19: loss 0.2780661880970001\n",
      "epoch 20: loss 0.3155803680419922\n",
      "epoch 21: loss 0.35172832012176514\n",
      "epoch 22: loss 0.33490270376205444\n",
      "epoch 23: loss 0.37620723247528076\n",
      "epoch 24: loss 0.36045485734939575\n",
      "epoch 25: loss 0.3947330415248871\n",
      "epoch 26: loss 0.3190224766731262\n",
      "epoch 27: loss 0.3549099564552307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: loss 0.39779698848724365\n",
      "epoch 29: loss 0.32037100195884705\n",
      "epoch 30: loss 0.27810409665107727\n",
      "epoch 31: loss 0.3408336043357849\n",
      "epoch 32: loss 0.337309330701828\n",
      "epoch 33: loss 0.3489130437374115\n",
      "epoch 34: loss 0.3911672532558441\n",
      "epoch 35: loss 0.25483036041259766\n",
      "epoch 36: loss 0.33383822441101074\n",
      "epoch 37: loss 0.32158565521240234\n",
      "epoch 38: loss 0.21717675030231476\n",
      "epoch 39: loss 0.3200816512107849\n",
      "epoch 40: loss 0.3095448613166809\n",
      "epoch 41: loss 0.28923892974853516\n",
      "epoch 42: loss 0.2897073030471802\n",
      "epoch 43: loss 0.31436794996261597\n",
      "epoch 44: loss 0.36768415570259094\n",
      "epoch 45: loss 0.289997398853302\n",
      "epoch 46: loss 0.3427899479866028\n",
      "epoch 47: loss 0.29012542963027954\n",
      "epoch 48: loss 0.26996538043022156\n",
      "epoch 49: loss 0.3441578149795532\n",
      "epoch 50: loss 0.29834678769111633\n",
      "epoch 51: loss 0.4164915978908539\n",
      "epoch 52: loss 0.3034587502479553\n",
      "epoch 53: loss 0.3713076412677765\n",
      "epoch 54: loss 0.33282721042633057\n",
      "epoch 55: loss 0.30338814854621887\n",
      "epoch 56: loss 0.34713536500930786\n",
      "epoch 57: loss 0.2515931725502014\n",
      "epoch 58: loss 0.3393012285232544\n",
      "epoch 59: loss 0.24795803427696228\n",
      "epoch 60: loss 0.3425423502922058\n",
      "epoch 61: loss 0.22305701673030853\n",
      "epoch 62: loss 0.24094143509864807\n",
      "epoch 63: loss 0.35123974084854126\n",
      "epoch 64: loss 0.29588329792022705\n",
      "epoch 65: loss 0.4001201391220093\n",
      "epoch 66: loss 0.33796802163124084\n",
      "epoch 67: loss 0.37643563747406006\n",
      "epoch 68: loss 0.364420086145401\n",
      "epoch 69: loss 0.423275887966156\n",
      "epoch 70: loss 0.3805631101131439\n",
      "epoch 71: loss 0.3389643430709839\n",
      "epoch 72: loss 0.2645632028579712\n",
      "epoch 73: loss 0.34811481833457947\n",
      "epoch 74: loss 0.2903570830821991\n",
      "epoch 75: loss 0.3333393335342407\n",
      "epoch 76: loss 0.301531046628952\n",
      "epoch 77: loss 0.4061400592327118\n",
      "epoch 78: loss 0.31083881855010986\n",
      "epoch 79: loss 0.3381127119064331\n",
      "epoch 80: loss 0.37502092123031616\n",
      "epoch 81: loss 0.2800927758216858\n",
      "epoch 82: loss 0.3316597044467926\n",
      "epoch 83: loss 0.3795511722564697\n",
      "epoch 84: loss 0.2507697343826294\n",
      "epoch 85: loss 0.3131431043148041\n",
      "epoch 86: loss 0.34521299600601196\n",
      "epoch 87: loss 0.344386488199234\n",
      "epoch 88: loss 0.26541054248809814\n",
      "epoch 89: loss 0.32461950182914734\n",
      "epoch 90: loss 0.2643797993659973\n",
      "epoch 91: loss 0.32467466592788696\n",
      "epoch 92: loss 0.28019940853118896\n",
      "epoch 93: loss 0.33798083662986755\n",
      "epoch 94: loss 0.2908931374549866\n",
      "epoch 95: loss 0.1899401694536209\n",
      "epoch 96: loss 0.2369927614927292\n",
      "epoch 97: loss 0.3205818235874176\n",
      "epoch 98: loss 0.33892732858657837\n",
      "epoch 99: loss 0.3657089173793793\n",
      "epoch 100: loss 0.4629260003566742\n",
      "epoch 101: loss 0.397163450717926\n",
      "epoch 102: loss 0.2740771174430847\n",
      "epoch 103: loss 0.313025563955307\n",
      "epoch 104: loss 0.2668676972389221\n",
      "epoch 105: loss 0.3883552849292755\n",
      "epoch 106: loss 0.3687501549720764\n",
      "epoch 107: loss 0.31245237588882446\n",
      "epoch 108: loss 0.34925737977027893\n",
      "epoch 109: loss 0.3698984980583191\n",
      "epoch 110: loss 0.3517434000968933\n",
      "epoch 111: loss 0.3832905888557434\n",
      "epoch 112: loss 0.37858760356903076\n",
      "epoch 113: loss 0.3266542851924896\n",
      "epoch 114: loss 0.22691531479358673\n",
      "epoch 115: loss 0.4102879762649536\n",
      "epoch 116: loss 0.3125542998313904\n",
      "epoch 117: loss 0.28383535146713257\n",
      "epoch 118: loss 0.327807754278183\n",
      "epoch 119: loss 0.37173694372177124\n",
      "epoch 120: loss 0.28728538751602173\n",
      "epoch 121: loss 0.3926631212234497\n",
      "epoch 122: loss 0.2754685878753662\n",
      "epoch 123: loss 0.39471107721328735\n",
      "epoch 124: loss 0.321679949760437\n",
      "epoch 125: loss 0.32932645082473755\n",
      "epoch 126: loss 0.36044248938560486\n",
      "epoch 127: loss 0.4045431315898895\n",
      "epoch 128: loss 0.2751438617706299\n",
      "epoch 129: loss 0.34198999404907227\n",
      "epoch 130: loss 0.38629549741744995\n",
      "epoch 131: loss 0.3254191279411316\n",
      "epoch 132: loss 0.31911158561706543\n",
      "epoch 133: loss 0.3500036597251892\n",
      "epoch 134: loss 0.33237189054489136\n",
      "epoch 135: loss 0.26473569869995117\n",
      "epoch 136: loss 0.23870176076889038\n",
      "epoch 0: loss 0.39109283685684204\n",
      "epoch 1: loss 0.3509153127670288\n",
      "epoch 2: loss 0.4532945454120636\n",
      "epoch 3: loss 0.37449410557746887\n",
      "epoch 4: loss 0.33285772800445557\n",
      "epoch 5: loss 0.3056725859642029\n",
      "epoch 6: loss 0.2920624911785126\n",
      "epoch 7: loss 0.23426371812820435\n",
      "epoch 8: loss 0.43391355872154236\n",
      "epoch 9: loss 0.4691845774650574\n",
      "epoch 10: loss 0.3601951003074646\n",
      "epoch 11: loss 0.2826704680919647\n",
      "epoch 12: loss 0.3398324251174927\n",
      "epoch 13: loss 0.42689836025238037\n",
      "epoch 14: loss 0.36563652753829956\n",
      "epoch 15: loss 0.3001129627227783\n",
      "epoch 16: loss 0.3345516622066498\n",
      "epoch 17: loss 0.36021509766578674\n",
      "epoch 18: loss 0.41375577449798584\n",
      "epoch 19: loss 0.2736261487007141\n",
      "epoch 20: loss 0.3073194622993469\n",
      "epoch 21: loss 0.3436086177825928\n",
      "epoch 22: loss 0.3270964026451111\n",
      "epoch 23: loss 0.3767120838165283\n",
      "epoch 24: loss 0.35676848888397217\n",
      "epoch 25: loss 0.3939698338508606\n",
      "epoch 26: loss 0.31929415464401245\n",
      "epoch 27: loss 0.35001128911972046\n",
      "epoch 28: loss 0.4050177037715912\n",
      "epoch 29: loss 0.31603485345840454\n",
      "epoch 30: loss 0.2778264582157135\n",
      "epoch 31: loss 0.3403383791446686\n",
      "epoch 32: loss 0.3364264965057373\n",
      "epoch 33: loss 0.3498840630054474\n",
      "epoch 34: loss 0.38727283477783203\n",
      "epoch 35: loss 0.26000505685806274\n",
      "epoch 36: loss 0.3374682068824768\n",
      "epoch 37: loss 0.3086804151535034\n",
      "epoch 38: loss 0.2146795243024826\n",
      "epoch 39: loss 0.3226322531700134\n",
      "epoch 40: loss 0.32101279497146606\n",
      "epoch 41: loss 0.2780459225177765\n",
      "epoch 42: loss 0.27591466903686523\n",
      "epoch 43: loss 0.3225887417793274\n",
      "epoch 44: loss 0.3522605299949646\n",
      "epoch 45: loss 0.27809178829193115\n",
      "epoch 46: loss 0.30756932497024536\n",
      "epoch 47: loss 0.2809998691082001\n",
      "epoch 48: loss 0.2695911228656769\n",
      "epoch 49: loss 0.32847410440444946\n",
      "epoch 50: loss 0.28216153383255005\n",
      "epoch 51: loss 0.4212894141674042\n",
      "epoch 52: loss 0.3006897270679474\n",
      "epoch 53: loss 0.3727380633354187\n",
      "epoch 54: loss 0.342704713344574\n",
      "epoch 55: loss 0.30121272802352905\n",
      "epoch 56: loss 0.3501865267753601\n",
      "epoch 57: loss 0.2522791028022766\n",
      "epoch 58: loss 0.33938151597976685\n",
      "epoch 59: loss 0.2458438277244568\n",
      "epoch 60: loss 0.34394368529319763\n",
      "epoch 61: loss 0.21701371669769287\n",
      "epoch 62: loss 0.23807059228420258\n",
      "epoch 63: loss 0.34873145818710327\n",
      "epoch 64: loss 0.2996830940246582\n",
      "epoch 65: loss 0.3982030153274536\n",
      "epoch 66: loss 0.3371098041534424\n",
      "epoch 67: loss 0.3730334937572479\n",
      "epoch 68: loss 0.36346185207366943\n",
      "epoch 69: loss 0.4253900945186615\n",
      "epoch 70: loss 0.3825506865978241\n",
      "epoch 71: loss 0.3421722650527954\n",
      "epoch 72: loss 0.2581891119480133\n",
      "epoch 73: loss 0.36165329813957214\n",
      "epoch 74: loss 0.29919612407684326\n",
      "epoch 75: loss 0.33575475215911865\n",
      "epoch 76: loss 0.3009752333164215\n",
      "epoch 77: loss 0.4018017053604126\n",
      "epoch 78: loss 0.31262296438217163\n",
      "epoch 79: loss 0.33536458015441895\n",
      "epoch 80: loss 0.37474697828292847\n",
      "epoch 81: loss 0.27599695324897766\n",
      "epoch 82: loss 0.33268576860427856\n",
      "epoch 83: loss 0.3799639046192169\n",
      "epoch 84: loss 0.2570188343524933\n",
      "epoch 85: loss 0.3121402859687805\n",
      "epoch 86: loss 0.34473997354507446\n",
      "epoch 87: loss 0.3453129827976227\n",
      "epoch 88: loss 0.2660647928714752\n",
      "epoch 89: loss 0.3358054757118225\n",
      "epoch 90: loss 0.27309536933898926\n",
      "epoch 91: loss 0.33003246784210205\n",
      "epoch 92: loss 0.2725035548210144\n",
      "epoch 93: loss 0.35670560598373413\n",
      "epoch 94: loss 0.2936754822731018\n",
      "epoch 95: loss 0.18994702398777008\n",
      "epoch 96: loss 0.24127250909805298\n",
      "epoch 97: loss 0.32673031091690063\n",
      "epoch 98: loss 0.3574880361557007\n",
      "epoch 99: loss 0.37702202796936035\n",
      "epoch 100: loss 0.474418580532074\n",
      "epoch 101: loss 0.34113094210624695\n",
      "epoch 102: loss 0.3030392825603485\n",
      "epoch 103: loss 0.30841660499572754\n",
      "epoch 104: loss 0.26861101388931274\n",
      "epoch 105: loss 0.3739261031150818\n",
      "epoch 106: loss 0.36011645197868347\n",
      "epoch 107: loss 0.31665515899658203\n",
      "epoch 108: loss 0.3447248637676239\n",
      "epoch 109: loss 0.38144558668136597\n",
      "epoch 110: loss 0.35726237297058105\n",
      "epoch 111: loss 0.3839564919471741\n",
      "epoch 112: loss 0.37569594383239746\n",
      "epoch 113: loss 0.32135796546936035\n",
      "epoch 114: loss 0.2524576187133789\n",
      "epoch 115: loss 0.40212613344192505\n",
      "epoch 116: loss 0.3162113130092621\n",
      "epoch 117: loss 0.2862318754196167\n",
      "epoch 118: loss 0.32737255096435547\n",
      "epoch 119: loss 0.37269076704978943\n",
      "epoch 120: loss 0.29170963168144226\n",
      "epoch 121: loss 0.401857852935791\n",
      "epoch 122: loss 0.26878583431243896\n",
      "epoch 123: loss 0.39869457483291626\n",
      "epoch 124: loss 0.30881553888320923\n",
      "epoch 125: loss 0.32978731393814087\n",
      "epoch 126: loss 0.35970741510391235\n",
      "epoch 127: loss 0.384769469499588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 128: loss 0.2829509377479553\n",
      "epoch 129: loss 0.3435678482055664\n",
      "epoch 130: loss 0.37691086530685425\n",
      "epoch 131: loss 0.32390499114990234\n",
      "epoch 132: loss 0.32012510299682617\n",
      "epoch 133: loss 0.3572903275489807\n",
      "epoch 134: loss 0.3211645483970642\n",
      "epoch 135: loss 0.26276931166648865\n",
      "epoch 136: loss 0.237234428524971\n",
      "epoch 0: loss 0.36934638023376465\n",
      "epoch 1: loss 0.34156158566474915\n",
      "epoch 2: loss 0.4239214360713959\n",
      "epoch 3: loss 0.3734096884727478\n",
      "epoch 4: loss 0.3486114740371704\n",
      "epoch 5: loss 0.269620805978775\n",
      "epoch 6: loss 0.29018881916999817\n",
      "epoch 7: loss 0.23794269561767578\n",
      "epoch 8: loss 0.4511041045188904\n",
      "epoch 9: loss 0.47254064679145813\n",
      "epoch 10: loss 0.33845341205596924\n",
      "epoch 11: loss 0.28317585587501526\n",
      "epoch 12: loss 0.3916762173175812\n",
      "epoch 13: loss 0.4461153745651245\n",
      "epoch 14: loss 0.3556770384311676\n",
      "epoch 15: loss 0.3273780345916748\n",
      "epoch 16: loss 0.37293457984924316\n",
      "epoch 17: loss 0.4011693596839905\n",
      "epoch 18: loss 0.41635245084762573\n",
      "epoch 19: loss 0.281013548374176\n",
      "epoch 20: loss 0.32876381278038025\n",
      "epoch 21: loss 0.3731907606124878\n",
      "epoch 22: loss 0.3608538508415222\n",
      "epoch 23: loss 0.38287439942359924\n",
      "epoch 24: loss 0.36471909284591675\n",
      "epoch 25: loss 0.39074328541755676\n",
      "epoch 26: loss 0.3274701237678528\n",
      "epoch 27: loss 0.3673100471496582\n",
      "epoch 28: loss 0.42127126455307007\n",
      "epoch 29: loss 0.3174532353878021\n",
      "epoch 30: loss 0.28817135095596313\n",
      "epoch 31: loss 0.3438563942909241\n",
      "epoch 32: loss 0.3390154540538788\n",
      "epoch 33: loss 0.35418015718460083\n",
      "epoch 34: loss 0.3834903836250305\n",
      "epoch 35: loss 0.2620187997817993\n",
      "epoch 36: loss 0.3300015330314636\n",
      "epoch 37: loss 0.32568812370300293\n",
      "epoch 38: loss 0.22157256305217743\n",
      "epoch 39: loss 0.33559077978134155\n",
      "epoch 40: loss 0.32693007588386536\n",
      "epoch 41: loss 0.28377091884613037\n",
      "epoch 42: loss 0.2938653826713562\n",
      "epoch 43: loss 0.3284427523612976\n",
      "epoch 44: loss 0.35557055473327637\n",
      "epoch 45: loss 0.28309446573257446\n",
      "epoch 46: loss 0.3167571425437927\n",
      "epoch 47: loss 0.28527048230171204\n",
      "epoch 48: loss 0.268521249294281\n",
      "epoch 49: loss 0.3339497447013855\n",
      "epoch 50: loss 0.2882322072982788\n",
      "epoch 51: loss 0.41459447145462036\n",
      "epoch 52: loss 0.3004820644855499\n",
      "epoch 53: loss 0.3731999397277832\n",
      "epoch 54: loss 0.3403700888156891\n",
      "epoch 55: loss 0.3025231957435608\n",
      "epoch 56: loss 0.349345326423645\n",
      "epoch 57: loss 0.25532346963882446\n",
      "epoch 58: loss 0.34026312828063965\n",
      "epoch 59: loss 0.24684038758277893\n",
      "epoch 60: loss 0.3430720567703247\n",
      "epoch 61: loss 0.2165904939174652\n",
      "epoch 62: loss 0.23599855601787567\n",
      "epoch 63: loss 0.35170286893844604\n",
      "epoch 64: loss 0.2940550744533539\n",
      "epoch 65: loss 0.3971398174762726\n",
      "epoch 66: loss 0.3401569128036499\n",
      "epoch 67: loss 0.37423545122146606\n",
      "epoch 68: loss 0.36402636766433716\n",
      "epoch 69: loss 0.42694664001464844\n",
      "epoch 70: loss 0.38392025232315063\n",
      "epoch 71: loss 0.3349102735519409\n",
      "epoch 72: loss 0.2670668363571167\n",
      "epoch 73: loss 0.3477378785610199\n",
      "epoch 74: loss 0.2885993421077728\n",
      "epoch 75: loss 0.33336687088012695\n",
      "epoch 76: loss 0.3018195629119873\n",
      "epoch 77: loss 0.40531834959983826\n",
      "epoch 78: loss 0.31186816096305847\n",
      "epoch 79: loss 0.3389866352081299\n",
      "epoch 80: loss 0.375529408454895\n",
      "epoch 81: loss 0.2806370258331299\n",
      "epoch 82: loss 0.3336946666240692\n",
      "epoch 83: loss 0.38083091378211975\n",
      "epoch 84: loss 0.25526124238967896\n",
      "epoch 85: loss 0.31433188915252686\n",
      "epoch 86: loss 0.34634655714035034\n",
      "epoch 87: loss 0.34417828917503357\n",
      "epoch 88: loss 0.2651109993457794\n",
      "epoch 89: loss 0.3313564658164978\n",
      "epoch 90: loss 0.26679491996765137\n",
      "epoch 91: loss 0.3243340253829956\n",
      "epoch 92: loss 0.27610987424850464\n",
      "epoch 93: loss 0.3513677418231964\n",
      "epoch 94: loss 0.2971528470516205\n",
      "epoch 95: loss 0.1891246736049652\n",
      "epoch 96: loss 0.23822051286697388\n",
      "epoch 97: loss 0.3231217861175537\n",
      "epoch 98: loss 0.35408926010131836\n",
      "epoch 99: loss 0.375021368265152\n",
      "epoch 100: loss 0.4757126271724701\n",
      "epoch 101: loss 0.3270966410636902\n",
      "epoch 102: loss 0.2954956293106079\n",
      "epoch 103: loss 0.3138684034347534\n",
      "epoch 104: loss 0.27353209257125854\n",
      "epoch 105: loss 0.36705929040908813\n",
      "epoch 106: loss 0.3510199785232544\n",
      "epoch 107: loss 0.3009200692176819\n",
      "epoch 108: loss 0.34643232822418213\n",
      "epoch 109: loss 0.38239312171936035\n",
      "epoch 110: loss 0.35992613434791565\n",
      "epoch 111: loss 0.390042781829834\n",
      "epoch 112: loss 0.3801018297672272\n",
      "epoch 113: loss 0.3117448091506958\n",
      "epoch 114: loss 0.2420234978199005\n",
      "epoch 115: loss 0.40334922075271606\n",
      "epoch 116: loss 0.31520700454711914\n",
      "epoch 117: loss 0.28576964139938354\n",
      "epoch 118: loss 0.3276937007904053\n",
      "epoch 119: loss 0.36745837330818176\n",
      "epoch 120: loss 0.28477364778518677\n",
      "epoch 121: loss 0.4027850031852722\n",
      "epoch 122: loss 0.267891526222229\n",
      "epoch 123: loss 0.4000105857849121\n",
      "epoch 124: loss 0.30757057666778564\n",
      "epoch 125: loss 0.3313908874988556\n",
      "epoch 126: loss 0.35790950059890747\n",
      "epoch 127: loss 0.3831172585487366\n",
      "epoch 128: loss 0.28366854786872864\n",
      "epoch 129: loss 0.34442073106765747\n",
      "epoch 130: loss 0.3828716278076172\n",
      "epoch 131: loss 0.32410234212875366\n",
      "epoch 132: loss 0.3206787705421448\n",
      "epoch 133: loss 0.3599773049354553\n",
      "epoch 134: loss 0.3206608295440674\n",
      "epoch 135: loss 0.26482003927230835\n",
      "epoch 136: loss 0.24231213331222534\n",
      "epoch 0: loss 0.3693561851978302\n",
      "epoch 1: loss 0.3387955129146576\n",
      "epoch 2: loss 0.43172961473464966\n",
      "epoch 3: loss 0.3694363236427307\n",
      "epoch 4: loss 0.33033254742622375\n",
      "epoch 5: loss 0.27918481826782227\n",
      "epoch 6: loss 0.28761929273605347\n",
      "epoch 7: loss 0.23166048526763916\n",
      "epoch 8: loss 0.433641642332077\n",
      "epoch 9: loss 0.456234335899353\n",
      "epoch 10: loss 0.3419038653373718\n",
      "epoch 11: loss 0.2773079574108124\n",
      "epoch 12: loss 0.38258177042007446\n",
      "epoch 13: loss 0.43217724561691284\n",
      "epoch 14: loss 0.34295570850372314\n",
      "epoch 15: loss 0.30373549461364746\n",
      "epoch 16: loss 0.3429754972457886\n",
      "epoch 17: loss 0.3625164330005646\n",
      "epoch 18: loss 0.39558297395706177\n",
      "epoch 19: loss 0.2708311676979065\n",
      "epoch 20: loss 0.31452953815460205\n",
      "epoch 21: loss 0.3512451648712158\n",
      "epoch 22: loss 0.329013466835022\n",
      "epoch 23: loss 0.37395232915878296\n",
      "epoch 24: loss 0.36001908779144287\n",
      "epoch 25: loss 0.39336228370666504\n",
      "epoch 26: loss 0.3159385323524475\n",
      "epoch 27: loss 0.35045379400253296\n",
      "epoch 28: loss 0.39791297912597656\n",
      "epoch 29: loss 0.3183831572532654\n",
      "epoch 30: loss 0.28174781799316406\n",
      "epoch 31: loss 0.343606561422348\n",
      "epoch 32: loss 0.3339000344276428\n",
      "epoch 33: loss 0.34589171409606934\n",
      "epoch 34: loss 0.38690832257270813\n",
      "epoch 35: loss 0.25698065757751465\n",
      "epoch 36: loss 0.3318510353565216\n",
      "epoch 37: loss 0.31660374999046326\n",
      "epoch 38: loss 0.21361863613128662\n",
      "epoch 39: loss 0.32143768668174744\n",
      "epoch 40: loss 0.31359928846359253\n",
      "epoch 41: loss 0.28286129236221313\n",
      "epoch 42: loss 0.288104772567749\n",
      "epoch 43: loss 0.3193923532962799\n",
      "epoch 44: loss 0.355437695980072\n",
      "epoch 45: loss 0.2837155759334564\n",
      "epoch 46: loss 0.3222361207008362\n",
      "epoch 47: loss 0.2883726954460144\n",
      "epoch 48: loss 0.26497501134872437\n",
      "epoch 49: loss 0.3354472517967224\n",
      "epoch 50: loss 0.2877299189567566\n",
      "epoch 51: loss 0.4127810597419739\n",
      "epoch 52: loss 0.29898321628570557\n",
      "epoch 53: loss 0.36589038372039795\n",
      "epoch 54: loss 0.33870983123779297\n",
      "epoch 55: loss 0.3018221855163574\n",
      "epoch 56: loss 0.34863919019699097\n",
      "epoch 57: loss 0.25045180320739746\n",
      "epoch 58: loss 0.3400980234146118\n",
      "epoch 59: loss 0.2463744431734085\n",
      "epoch 60: loss 0.3408834934234619\n",
      "epoch 61: loss 0.21890395879745483\n",
      "epoch 62: loss 0.23848703503608704\n",
      "epoch 63: loss 0.3475475311279297\n",
      "epoch 64: loss 0.2928188145160675\n",
      "epoch 65: loss 0.39658570289611816\n",
      "epoch 66: loss 0.33737727999687195\n",
      "epoch 67: loss 0.373956561088562\n",
      "epoch 68: loss 0.3622910976409912\n",
      "epoch 69: loss 0.4237143397331238\n",
      "epoch 70: loss 0.3796687722206116\n",
      "epoch 71: loss 0.33707356452941895\n",
      "epoch 72: loss 0.2613223195075989\n",
      "epoch 73: loss 0.3484545946121216\n",
      "epoch 74: loss 0.28865566849708557\n",
      "epoch 75: loss 0.3339240550994873\n",
      "epoch 76: loss 0.3008885979652405\n",
      "epoch 77: loss 0.40311071276664734\n",
      "epoch 78: loss 0.310666561126709\n",
      "epoch 79: loss 0.33673131465911865\n",
      "epoch 80: loss 0.37530139088630676\n",
      "epoch 81: loss 0.2738909423351288\n",
      "epoch 82: loss 0.3316378891468048\n",
      "epoch 83: loss 0.37814944982528687\n",
      "epoch 84: loss 0.25425422191619873\n",
      "epoch 85: loss 0.31138619780540466\n",
      "epoch 86: loss 0.34489572048187256\n",
      "epoch 87: loss 0.34431391954421997\n",
      "epoch 88: loss 0.2659071087837219\n",
      "epoch 89: loss 0.33530956506729126\n",
      "epoch 90: loss 0.2726340889930725\n",
      "epoch 91: loss 0.32395094633102417\n",
      "epoch 92: loss 0.2773600220680237\n",
      "epoch 93: loss 0.3521747887134552\n",
      "epoch 94: loss 0.29384785890579224\n",
      "epoch 95: loss 0.19072052836418152\n",
      "epoch 96: loss 0.24603374302387238\n",
      "epoch 97: loss 0.3270878791809082\n",
      "epoch 98: loss 0.3570977747440338\n",
      "epoch 99: loss 0.37493693828582764\n",
      "epoch 100: loss 0.47568610310554504\n",
      "epoch 101: loss 0.3200085461139679\n",
      "epoch 102: loss 0.2891263961791992\n",
      "epoch 103: loss 0.32065558433532715\n",
      "epoch 104: loss 0.27878445386886597\n",
      "epoch 105: loss 0.3667108118534088\n",
      "epoch 106: loss 0.34449100494384766\n",
      "epoch 107: loss 0.29477226734161377\n",
      "epoch 108: loss 0.35040175914764404\n",
      "epoch 109: loss 0.3815504312515259\n",
      "epoch 110: loss 0.3628475069999695\n",
      "epoch 111: loss 0.39129793643951416\n",
      "epoch 112: loss 0.3787153959274292\n",
      "epoch 113: loss 0.30487969517707825\n",
      "epoch 114: loss 0.2261439561843872\n",
      "epoch 115: loss 0.3976643681526184\n",
      "epoch 116: loss 0.3150120973587036\n",
      "epoch 117: loss 0.28569453954696655\n",
      "epoch 118: loss 0.32623642683029175\n",
      "epoch 119: loss 0.3691692352294922\n",
      "epoch 120: loss 0.28495457768440247\n",
      "epoch 121: loss 0.40181583166122437\n",
      "epoch 122: loss 0.2677391767501831\n",
      "epoch 123: loss 0.40454649925231934\n",
      "epoch 124: loss 0.30978891253471375\n",
      "epoch 125: loss 0.3236950635910034\n",
      "epoch 126: loss 0.36039799451828003\n",
      "epoch 127: loss 0.38693416118621826\n",
      "epoch 128: loss 0.2929201126098633\n",
      "epoch 129: loss 0.3449215292930603\n",
      "epoch 130: loss 0.37739098072052\n",
      "epoch 131: loss 0.3229783773422241\n",
      "epoch 132: loss 0.3242301344871521\n",
      "epoch 133: loss 0.3671655058860779\n",
      "epoch 134: loss 0.32112717628479004\n",
      "epoch 135: loss 0.2571638822555542\n",
      "epoch 136: loss 0.24040330946445465\n",
      "epoch 0: loss 0.35138124227523804\n",
      "epoch 1: loss 0.34060871601104736\n",
      "epoch 2: loss 0.3936343193054199\n",
      "epoch 3: loss 0.3800102472305298\n",
      "epoch 4: loss 0.32090193033218384\n",
      "epoch 5: loss 0.26255911588668823\n",
      "epoch 6: loss 0.2929544150829315\n",
      "epoch 7: loss 0.22913020849227905\n",
      "epoch 8: loss 0.4278053939342499\n",
      "epoch 9: loss 0.4360681176185608\n",
      "epoch 10: loss 0.3210598826408386\n",
      "epoch 11: loss 0.3037886619567871\n",
      "epoch 12: loss 0.3238535523414612\n",
      "epoch 13: loss 0.4601268470287323\n",
      "epoch 14: loss 0.3525282144546509\n",
      "epoch 15: loss 0.311252623796463\n",
      "epoch 16: loss 0.350018173456192\n",
      "epoch 17: loss 0.3564373254776001\n",
      "epoch 18: loss 0.3793192505836487\n",
      "epoch 19: loss 0.29124897718429565\n",
      "epoch 20: loss 0.33113643527030945\n",
      "epoch 21: loss 0.34355393052101135\n",
      "epoch 22: loss 0.3277239203453064\n",
      "epoch 23: loss 0.3842083811759949\n",
      "epoch 24: loss 0.37047144770622253\n",
      "epoch 25: loss 0.39647066593170166\n",
      "epoch 26: loss 0.3226850628852844\n",
      "epoch 27: loss 0.35804933309555054\n",
      "epoch 28: loss 0.39285996556282043\n",
      "epoch 29: loss 0.3264952301979065\n",
      "epoch 30: loss 0.2979171872138977\n",
      "epoch 31: loss 0.3536398112773895\n",
      "epoch 32: loss 0.33637702465057373\n",
      "epoch 33: loss 0.3443041443824768\n",
      "epoch 34: loss 0.37937188148498535\n",
      "epoch 35: loss 0.2597833573818207\n",
      "epoch 36: loss 0.3337298631668091\n",
      "epoch 37: loss 0.3471630811691284\n",
      "epoch 38: loss 0.23121368885040283\n",
      "epoch 39: loss 0.33267009258270264\n",
      "epoch 40: loss 0.3191123604774475\n",
      "epoch 41: loss 0.28799039125442505\n",
      "epoch 42: loss 0.31950685381889343\n",
      "epoch 43: loss 0.32052966952323914\n",
      "epoch 44: loss 0.3582230508327484\n",
      "epoch 45: loss 0.2918737530708313\n",
      "epoch 46: loss 0.36048105359077454\n",
      "epoch 47: loss 0.3056471347808838\n",
      "epoch 48: loss 0.28307127952575684\n",
      "epoch 49: loss 0.3285825550556183\n",
      "epoch 50: loss 0.3020206689834595\n",
      "epoch 51: loss 0.4182720184326172\n",
      "epoch 52: loss 0.3046507239341736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53: loss 0.36675208806991577\n",
      "epoch 54: loss 0.3363795876502991\n",
      "epoch 55: loss 0.3043982982635498\n",
      "epoch 56: loss 0.35795700550079346\n",
      "epoch 57: loss 0.2451597899198532\n",
      "epoch 58: loss 0.363217830657959\n",
      "epoch 59: loss 0.2576349675655365\n",
      "epoch 60: loss 0.3546989858150482\n",
      "epoch 61: loss 0.22259864211082458\n",
      "epoch 62: loss 0.24126508831977844\n",
      "epoch 63: loss 0.3468225300312042\n",
      "epoch 64: loss 0.31038177013397217\n",
      "epoch 65: loss 0.3973197340965271\n",
      "epoch 66: loss 0.3345717191696167\n",
      "epoch 67: loss 0.3744947016239166\n",
      "epoch 68: loss 0.36759036779403687\n",
      "epoch 69: loss 0.43704652786254883\n",
      "epoch 70: loss 0.40029454231262207\n",
      "epoch 71: loss 0.31904128193855286\n",
      "epoch 72: loss 0.27815574407577515\n",
      "epoch 73: loss 0.33593547344207764\n",
      "epoch 74: loss 0.28935006260871887\n",
      "epoch 75: loss 0.3292589783668518\n",
      "epoch 76: loss 0.3063817620277405\n",
      "epoch 77: loss 0.39729952812194824\n",
      "epoch 78: loss 0.3125033378601074\n",
      "epoch 79: loss 0.358177125453949\n",
      "epoch 80: loss 0.3871462643146515\n",
      "epoch 81: loss 0.26909583806991577\n",
      "epoch 82: loss 0.3334676921367645\n",
      "epoch 83: loss 0.38721078634262085\n",
      "epoch 84: loss 0.25799763202667236\n",
      "epoch 85: loss 0.31167054176330566\n",
      "epoch 86: loss 0.3535096347332001\n",
      "epoch 87: loss 0.34733688831329346\n",
      "epoch 88: loss 0.2685900628566742\n",
      "epoch 89: loss 0.3487674593925476\n",
      "epoch 90: loss 0.28005629777908325\n",
      "epoch 91: loss 0.3284343183040619\n",
      "epoch 92: loss 0.26856887340545654\n",
      "epoch 93: loss 0.39962393045425415\n",
      "epoch 94: loss 0.2970690131187439\n",
      "epoch 95: loss 0.19434046745300293\n",
      "epoch 96: loss 0.26815128326416016\n",
      "epoch 97: loss 0.33609119057655334\n",
      "epoch 98: loss 0.3801950216293335\n",
      "epoch 99: loss 0.3899063766002655\n",
      "epoch 100: loss 0.48793086409568787\n",
      "epoch 101: loss 0.3172129988670349\n",
      "epoch 102: loss 0.3014312982559204\n",
      "epoch 103: loss 0.3506101667881012\n",
      "epoch 104: loss 0.2967926263809204\n",
      "epoch 105: loss 0.36531707644462585\n",
      "epoch 106: loss 0.3439895510673523\n",
      "epoch 107: loss 0.3053193688392639\n",
      "epoch 108: loss 0.33676615357398987\n",
      "epoch 109: loss 0.4055424928665161\n",
      "epoch 110: loss 0.36010149121284485\n",
      "epoch 111: loss 0.40516871213912964\n",
      "epoch 112: loss 0.3956901431083679\n",
      "epoch 113: loss 0.3137962818145752\n",
      "epoch 114: loss 0.22251233458518982\n",
      "epoch 115: loss 0.40253761410713196\n",
      "epoch 116: loss 0.34064674377441406\n",
      "epoch 117: loss 0.2991826832294464\n",
      "epoch 118: loss 0.33154624700546265\n",
      "epoch 119: loss 0.3689974248409271\n",
      "epoch 120: loss 0.2966102361679077\n",
      "epoch 121: loss 0.41410690546035767\n",
      "epoch 122: loss 0.27993258833885193\n",
      "epoch 123: loss 0.4252033829689026\n",
      "epoch 124: loss 0.3160319924354553\n",
      "epoch 125: loss 0.319919228553772\n",
      "epoch 126: loss 0.36511069536209106\n",
      "epoch 127: loss 0.3893246352672577\n",
      "epoch 128: loss 0.291550874710083\n",
      "epoch 129: loss 0.35589295625686646\n",
      "epoch 130: loss 0.36964356899261475\n",
      "epoch 131: loss 0.3272169828414917\n",
      "epoch 132: loss 0.3186568021774292\n",
      "epoch 133: loss 0.3589611053466797\n",
      "epoch 134: loss 0.31892287731170654\n",
      "epoch 135: loss 0.2633907198905945\n",
      "epoch 136: loss 0.2381948083639145\n",
      "epoch 0: loss 0.382020503282547\n",
      "epoch 1: loss 0.3396921157836914\n",
      "epoch 2: loss 0.4191570281982422\n",
      "epoch 3: loss 0.37416625022888184\n",
      "epoch 4: loss 0.3392314612865448\n",
      "epoch 5: loss 0.27615201473236084\n",
      "epoch 6: loss 0.29224520921707153\n",
      "epoch 7: loss 0.23216187953948975\n",
      "epoch 8: loss 0.4273104965686798\n",
      "epoch 9: loss 0.460823655128479\n",
      "epoch 10: loss 0.3536766767501831\n",
      "epoch 11: loss 0.2786715030670166\n",
      "epoch 12: loss 0.3366849720478058\n",
      "epoch 13: loss 0.4217623770236969\n",
      "epoch 14: loss 0.36356866359710693\n",
      "epoch 15: loss 0.2952561676502228\n",
      "epoch 16: loss 0.33559054136276245\n",
      "epoch 17: loss 0.3584330379962921\n",
      "epoch 18: loss 0.4126957356929779\n",
      "epoch 19: loss 0.2712108790874481\n",
      "epoch 20: loss 0.30845022201538086\n",
      "epoch 21: loss 0.3418661952018738\n",
      "epoch 22: loss 0.32472342252731323\n",
      "epoch 23: loss 0.37562328577041626\n",
      "epoch 24: loss 0.3646897077560425\n",
      "epoch 25: loss 0.3965519070625305\n",
      "epoch 26: loss 0.3163954019546509\n",
      "epoch 27: loss 0.35162553191185\n",
      "epoch 28: loss 0.41130542755126953\n",
      "epoch 29: loss 0.3169691264629364\n",
      "epoch 30: loss 0.2806623578071594\n",
      "epoch 31: loss 0.3385965824127197\n",
      "epoch 32: loss 0.3391192853450775\n",
      "epoch 33: loss 0.34441274404525757\n",
      "epoch 34: loss 0.386955201625824\n",
      "epoch 35: loss 0.26017919182777405\n",
      "epoch 36: loss 0.3283222019672394\n",
      "epoch 37: loss 0.3177931308746338\n",
      "epoch 38: loss 0.21940430998802185\n",
      "epoch 39: loss 0.3272731900215149\n",
      "epoch 40: loss 0.3254326581954956\n",
      "epoch 41: loss 0.28001588582992554\n",
      "epoch 42: loss 0.26948368549346924\n",
      "epoch 43: loss 0.3271636962890625\n",
      "epoch 44: loss 0.3479155898094177\n",
      "epoch 45: loss 0.27918773889541626\n",
      "epoch 46: loss 0.30199986696243286\n",
      "epoch 47: loss 0.279883474111557\n",
      "epoch 48: loss 0.26180216670036316\n",
      "epoch 49: loss 0.3292025625705719\n",
      "epoch 50: loss 0.28521865606307983\n",
      "epoch 51: loss 0.4283420741558075\n",
      "epoch 52: loss 0.3009325861930847\n",
      "epoch 53: loss 0.3748781085014343\n",
      "epoch 54: loss 0.3465832471847534\n",
      "epoch 55: loss 0.3043128252029419\n",
      "epoch 56: loss 0.3548424243927002\n",
      "epoch 57: loss 0.25707143545150757\n",
      "epoch 58: loss 0.3367193937301636\n",
      "epoch 59: loss 0.2458893060684204\n",
      "epoch 60: loss 0.35293668508529663\n",
      "epoch 61: loss 0.2212456464767456\n",
      "epoch 62: loss 0.2378174066543579\n",
      "epoch 63: loss 0.37068939208984375\n",
      "epoch 64: loss 0.2961128056049347\n",
      "epoch 65: loss 0.4005487561225891\n",
      "epoch 66: loss 0.35104429721832275\n",
      "epoch 67: loss 0.37927716970443726\n",
      "epoch 68: loss 0.36498767137527466\n",
      "epoch 69: loss 0.431989848613739\n",
      "epoch 70: loss 0.3969113826751709\n",
      "epoch 71: loss 0.31059393286705017\n",
      "epoch 72: loss 0.2656080722808838\n",
      "epoch 73: loss 0.338616281747818\n",
      "epoch 74: loss 0.2869332432746887\n",
      "epoch 75: loss 0.3266448378562927\n",
      "epoch 76: loss 0.3141273558139801\n",
      "epoch 77: loss 0.39348697662353516\n",
      "epoch 78: loss 0.31667327880859375\n",
      "epoch 79: loss 0.3353874683380127\n",
      "epoch 80: loss 0.38527387380599976\n",
      "epoch 81: loss 0.26646655797958374\n",
      "epoch 82: loss 0.32990700006484985\n",
      "epoch 83: loss 0.37802091240882874\n",
      "epoch 84: loss 0.29500889778137207\n",
      "epoch 85: loss 0.3110467195510864\n",
      "epoch 86: loss 0.34840553998947144\n",
      "epoch 87: loss 0.35125815868377686\n",
      "epoch 88: loss 0.2757379412651062\n",
      "epoch 89: loss 0.3628433346748352\n",
      "epoch 90: loss 0.2854176163673401\n",
      "epoch 91: loss 0.344296932220459\n",
      "epoch 92: loss 0.27198702096939087\n",
      "epoch 93: loss 0.394033282995224\n",
      "epoch 94: loss 0.31596487760543823\n",
      "epoch 95: loss 0.19618305563926697\n",
      "epoch 96: loss 0.23838168382644653\n",
      "epoch 97: loss 0.3220478296279907\n",
      "epoch 98: loss 0.3700628876686096\n",
      "epoch 99: loss 0.39293861389160156\n",
      "epoch 100: loss 0.5107747316360474\n",
      "epoch 101: loss 0.30905258655548096\n",
      "epoch 102: loss 0.28192955255508423\n",
      "epoch 103: loss 0.3382198214530945\n",
      "epoch 104: loss 0.3090953826904297\n",
      "epoch 105: loss 0.37166404724121094\n",
      "epoch 106: loss 0.3293589949607849\n",
      "epoch 107: loss 0.29375651478767395\n",
      "epoch 108: loss 0.3459365963935852\n",
      "epoch 109: loss 0.3813180923461914\n",
      "epoch 110: loss 0.36354830861091614\n",
      "epoch 111: loss 0.4059593677520752\n",
      "epoch 112: loss 0.4023404121398926\n",
      "epoch 113: loss 0.31211912631988525\n",
      "epoch 114: loss 0.2016245424747467\n",
      "epoch 115: loss 0.40346530079841614\n",
      "epoch 116: loss 0.32191547751426697\n",
      "epoch 117: loss 0.3105319142341614\n",
      "epoch 118: loss 0.33091962337493896\n",
      "epoch 119: loss 0.36947858333587646\n",
      "epoch 120: loss 0.2866112291812897\n",
      "epoch 121: loss 0.40392202138900757\n",
      "epoch 122: loss 0.27099913358688354\n",
      "epoch 123: loss 0.4168665409088135\n",
      "epoch 124: loss 0.31552547216415405\n",
      "epoch 125: loss 0.3185402750968933\n",
      "epoch 126: loss 0.3625653088092804\n",
      "epoch 127: loss 0.39842689037323\n",
      "epoch 128: loss 0.280050665140152\n",
      "epoch 129: loss 0.3502035439014435\n",
      "epoch 130: loss 0.37272822856903076\n",
      "epoch 131: loss 0.3382871150970459\n",
      "epoch 132: loss 0.3192785382270813\n",
      "epoch 133: loss 0.3483659625053406\n",
      "epoch 134: loss 0.321173757314682\n",
      "epoch 135: loss 0.2608491778373718\n",
      "epoch 136: loss 0.23861098289489746\n",
      "epoch 0: loss 0.3829721212387085\n",
      "epoch 1: loss 0.33931469917297363\n",
      "epoch 2: loss 0.427716463804245\n",
      "epoch 3: loss 0.3705539107322693\n",
      "epoch 4: loss 0.33216893672943115\n",
      "epoch 5: loss 0.28359007835388184\n",
      "epoch 6: loss 0.2908214330673218\n",
      "epoch 7: loss 0.2309986650943756\n",
      "epoch 8: loss 0.42788344621658325\n",
      "epoch 9: loss 0.46099016070365906\n",
      "epoch 10: loss 0.35429972410202026\n",
      "epoch 11: loss 0.2787982225418091\n",
      "epoch 12: loss 0.32791996002197266\n",
      "epoch 13: loss 0.42361313104629517\n",
      "epoch 14: loss 0.36394965648651123\n",
      "epoch 15: loss 0.2926984429359436\n",
      "epoch 16: loss 0.3361854553222656\n",
      "epoch 17: loss 0.3574065566062927\n",
      "epoch 18: loss 0.41332128643989563\n",
      "epoch 19: loss 0.26927390694618225\n",
      "epoch 20: loss 0.30687254667282104\n",
      "epoch 21: loss 0.3397197127342224\n",
      "epoch 22: loss 0.3224141597747803\n",
      "epoch 23: loss 0.37285324931144714\n",
      "epoch 24: loss 0.36118537187576294\n",
      "epoch 25: loss 0.3951804041862488\n",
      "epoch 26: loss 0.31651461124420166\n",
      "epoch 27: loss 0.3509194552898407\n",
      "epoch 28: loss 0.405974417924881\n",
      "epoch 29: loss 0.3170965313911438\n",
      "epoch 30: loss 0.27955979108810425\n",
      "epoch 31: loss 0.337744802236557\n",
      "epoch 32: loss 0.3391682207584381\n",
      "epoch 33: loss 0.34427231550216675\n",
      "epoch 34: loss 0.3882664740085602\n",
      "epoch 35: loss 0.2594381272792816\n",
      "epoch 36: loss 0.32762429118156433\n",
      "epoch 37: loss 0.31650012731552124\n",
      "epoch 38: loss 0.21902108192443848\n",
      "epoch 39: loss 0.32537317276000977\n",
      "epoch 40: loss 0.32172539830207825\n",
      "epoch 41: loss 0.27989083528518677\n",
      "epoch 42: loss 0.2686536908149719\n",
      "epoch 43: loss 0.3257754445075989\n",
      "epoch 44: loss 0.3456953167915344\n",
      "epoch 45: loss 0.2786915898323059\n",
      "epoch 46: loss 0.29960429668426514\n",
      "epoch 47: loss 0.27954334020614624\n",
      "epoch 48: loss 0.2595577836036682\n",
      "epoch 49: loss 0.33081552386283875\n",
      "epoch 50: loss 0.28499048948287964\n",
      "epoch 51: loss 0.4245641529560089\n",
      "epoch 52: loss 0.2994195818901062\n",
      "epoch 53: loss 0.37385445833206177\n",
      "epoch 54: loss 0.34579846262931824\n",
      "epoch 55: loss 0.3025107681751251\n",
      "epoch 56: loss 0.3538915514945984\n",
      "epoch 57: loss 0.2560138404369354\n",
      "epoch 58: loss 0.3332885503768921\n",
      "epoch 59: loss 0.24893194437026978\n",
      "epoch 60: loss 0.35073190927505493\n",
      "epoch 61: loss 0.2234293520450592\n",
      "epoch 62: loss 0.23970620334148407\n",
      "epoch 63: loss 0.3735010027885437\n",
      "epoch 64: loss 0.2984638214111328\n",
      "epoch 65: loss 0.4018073081970215\n",
      "epoch 66: loss 0.3442010283470154\n",
      "epoch 67: loss 0.38327428698539734\n",
      "epoch 68: loss 0.3706045150756836\n",
      "epoch 69: loss 0.4269532561302185\n",
      "epoch 70: loss 0.39365583658218384\n",
      "epoch 71: loss 0.3100532591342926\n",
      "epoch 72: loss 0.26203596591949463\n",
      "epoch 73: loss 0.3389902710914612\n",
      "epoch 74: loss 0.2854807376861572\n",
      "epoch 75: loss 0.32887905836105347\n",
      "epoch 76: loss 0.3136502802371979\n",
      "epoch 77: loss 0.39604073762893677\n",
      "epoch 78: loss 0.31393104791641235\n",
      "epoch 79: loss 0.33646056056022644\n",
      "epoch 80: loss 0.3877699673175812\n",
      "epoch 81: loss 0.26789504289627075\n",
      "epoch 82: loss 0.3321179151535034\n",
      "epoch 83: loss 0.37927955389022827\n",
      "epoch 84: loss 0.2881075143814087\n",
      "epoch 85: loss 0.3127896785736084\n",
      "epoch 86: loss 0.339469850063324\n",
      "epoch 87: loss 0.3463279604911804\n",
      "epoch 88: loss 0.2674286365509033\n",
      "epoch 89: loss 0.3563985228538513\n",
      "epoch 90: loss 0.28689277172088623\n",
      "epoch 91: loss 0.353113055229187\n",
      "epoch 92: loss 0.2822932302951813\n",
      "epoch 93: loss 0.35072389245033264\n",
      "epoch 94: loss 0.34079286456108093\n",
      "epoch 95: loss 0.20268258452415466\n",
      "epoch 96: loss 0.22535718977451324\n",
      "epoch 97: loss 0.32337284088134766\n",
      "epoch 98: loss 0.3748708963394165\n",
      "epoch 99: loss 0.4101138710975647\n",
      "epoch 100: loss 0.5462885499000549\n",
      "epoch 101: loss 0.30901509523391724\n",
      "epoch 102: loss 0.2736404538154602\n",
      "epoch 103: loss 0.32159146666526794\n",
      "epoch 104: loss 0.3251400291919708\n",
      "epoch 105: loss 0.3787184953689575\n",
      "epoch 106: loss 0.3235936164855957\n",
      "epoch 107: loss 0.2961723506450653\n",
      "epoch 108: loss 0.3515375256538391\n",
      "epoch 109: loss 0.36466890573501587\n",
      "epoch 110: loss 0.3618593215942383\n",
      "epoch 111: loss 0.3947599232196808\n",
      "epoch 112: loss 0.4018639326095581\n",
      "epoch 113: loss 0.31131428480148315\n",
      "epoch 114: loss 0.19405338168144226\n",
      "epoch 115: loss 0.4064837098121643\n",
      "epoch 116: loss 0.3094261884689331\n",
      "epoch 117: loss 0.29987478256225586\n",
      "epoch 118: loss 0.3320343494415283\n",
      "epoch 119: loss 0.36855220794677734\n",
      "epoch 120: loss 0.27662500739097595\n",
      "epoch 121: loss 0.4006548523902893\n",
      "epoch 122: loss 0.26622623205184937\n",
      "epoch 123: loss 0.4009113311767578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 124: loss 0.3059365451335907\n",
      "epoch 125: loss 0.3299711346626282\n",
      "epoch 126: loss 0.36326873302459717\n",
      "epoch 127: loss 0.37814706563949585\n",
      "epoch 128: loss 0.2986811101436615\n",
      "epoch 129: loss 0.3479200005531311\n",
      "epoch 130: loss 0.3714503049850464\n",
      "epoch 131: loss 0.32038915157318115\n",
      "epoch 132: loss 0.3194490671157837\n",
      "epoch 133: loss 0.3607443869113922\n",
      "epoch 134: loss 0.3211700916290283\n",
      "epoch 135: loss 0.2567511796951294\n",
      "epoch 136: loss 0.24010874330997467\n",
      "epoch 0: loss 0.35392042994499207\n",
      "epoch 1: loss 0.33634886145591736\n",
      "epoch 2: loss 0.3937298357486725\n",
      "epoch 3: loss 0.3795315623283386\n",
      "epoch 4: loss 0.3253854811191559\n",
      "epoch 5: loss 0.26276296377182007\n",
      "epoch 6: loss 0.289714515209198\n",
      "epoch 7: loss 0.23107436299324036\n",
      "epoch 8: loss 0.43028128147125244\n",
      "epoch 9: loss 0.44739818572998047\n",
      "epoch 10: loss 0.3358626961708069\n",
      "epoch 11: loss 0.2757834494113922\n",
      "epoch 12: loss 0.36867856979370117\n",
      "epoch 13: loss 0.4363040328025818\n",
      "epoch 14: loss 0.3411861062049866\n",
      "epoch 15: loss 0.3022555112838745\n",
      "epoch 16: loss 0.33947432041168213\n",
      "epoch 17: loss 0.35566145181655884\n",
      "epoch 18: loss 0.39147305488586426\n",
      "epoch 19: loss 0.26640430092811584\n",
      "epoch 20: loss 0.30457639694213867\n",
      "epoch 21: loss 0.3470112085342407\n",
      "epoch 22: loss 0.3265303075313568\n",
      "epoch 23: loss 0.3703770637512207\n",
      "epoch 24: loss 0.35467204451560974\n",
      "epoch 25: loss 0.39385783672332764\n",
      "epoch 26: loss 0.32471343874931335\n",
      "epoch 27: loss 0.344946950674057\n",
      "epoch 28: loss 0.3981482982635498\n",
      "epoch 29: loss 0.31464195251464844\n",
      "epoch 30: loss 0.27895790338516235\n",
      "epoch 31: loss 0.3422010540962219\n",
      "epoch 32: loss 0.33902257680892944\n",
      "epoch 33: loss 0.34404394030570984\n",
      "epoch 34: loss 0.38951417803764343\n",
      "epoch 35: loss 0.2602841854095459\n",
      "epoch 36: loss 0.33187758922576904\n",
      "epoch 37: loss 0.31370246410369873\n",
      "epoch 38: loss 0.21305498480796814\n",
      "epoch 39: loss 0.3163701891899109\n",
      "epoch 40: loss 0.3126699924468994\n",
      "epoch 41: loss 0.2830548882484436\n",
      "epoch 42: loss 0.27588599920272827\n",
      "epoch 43: loss 0.321040540933609\n",
      "epoch 44: loss 0.35038554668426514\n",
      "epoch 45: loss 0.2810543179512024\n",
      "epoch 46: loss 0.3086181581020355\n",
      "epoch 47: loss 0.2848188281059265\n",
      "epoch 48: loss 0.25690317153930664\n",
      "epoch 49: loss 0.34000322222709656\n",
      "epoch 50: loss 0.28727585077285767\n",
      "epoch 51: loss 0.41645383834838867\n",
      "epoch 52: loss 0.29459860920906067\n",
      "epoch 53: loss 0.36836040019989014\n",
      "epoch 54: loss 0.34411630034446716\n",
      "epoch 55: loss 0.30015021562576294\n",
      "epoch 56: loss 0.34909477829933167\n",
      "epoch 57: loss 0.24942466616630554\n",
      "epoch 58: loss 0.33369868993759155\n",
      "epoch 59: loss 0.2537916600704193\n",
      "epoch 60: loss 0.33548974990844727\n",
      "epoch 61: loss 0.22073695063591003\n",
      "epoch 62: loss 0.2406158298254013\n",
      "epoch 63: loss 0.35181453824043274\n",
      "epoch 64: loss 0.29384171962738037\n",
      "epoch 65: loss 0.39595717191696167\n",
      "epoch 66: loss 0.33764469623565674\n",
      "epoch 67: loss 0.3732796907424927\n",
      "epoch 68: loss 0.3633752465248108\n",
      "epoch 69: loss 0.42267826199531555\n",
      "epoch 70: loss 0.37929707765579224\n",
      "epoch 71: loss 0.3307473063468933\n",
      "epoch 72: loss 0.2603680193424225\n",
      "epoch 73: loss 0.34101155400276184\n",
      "epoch 74: loss 0.28457891941070557\n",
      "epoch 75: loss 0.337971031665802\n",
      "epoch 76: loss 0.3037256598472595\n",
      "epoch 77: loss 0.408626914024353\n",
      "epoch 78: loss 0.30902165174484253\n",
      "epoch 79: loss 0.3366177976131439\n",
      "epoch 80: loss 0.3775554597377777\n",
      "epoch 81: loss 0.2718628942966461\n",
      "epoch 82: loss 0.33210068941116333\n",
      "epoch 83: loss 0.3801931142807007\n",
      "epoch 84: loss 0.2560533881187439\n",
      "epoch 85: loss 0.3119000792503357\n",
      "epoch 86: loss 0.3428856134414673\n",
      "epoch 87: loss 0.34330564737319946\n",
      "epoch 88: loss 0.2673223316669464\n",
      "epoch 89: loss 0.34602484107017517\n",
      "epoch 90: loss 0.2815488576889038\n",
      "epoch 91: loss 0.33644771575927734\n",
      "epoch 92: loss 0.27261456847190857\n",
      "epoch 93: loss 0.3581331968307495\n",
      "epoch 94: loss 0.3077737092971802\n",
      "epoch 95: loss 0.19584669172763824\n",
      "epoch 96: loss 0.2197662889957428\n",
      "epoch 97: loss 0.31882983446121216\n",
      "epoch 98: loss 0.3586583733558655\n",
      "epoch 99: loss 0.3825617730617523\n",
      "epoch 100: loss 0.49940726161003113\n",
      "epoch 101: loss 0.3099822700023651\n",
      "epoch 102: loss 0.27184897661209106\n",
      "epoch 103: loss 0.3149387836456299\n",
      "epoch 104: loss 0.2948286235332489\n",
      "epoch 105: loss 0.3760923147201538\n",
      "epoch 106: loss 0.32082775235176086\n",
      "epoch 107: loss 0.2909913659095764\n",
      "epoch 108: loss 0.3529887795448303\n",
      "epoch 109: loss 0.36153295636177063\n",
      "epoch 110: loss 0.356550395488739\n",
      "epoch 111: loss 0.3868735134601593\n",
      "epoch 112: loss 0.37881672382354736\n",
      "epoch 113: loss 0.29566001892089844\n",
      "epoch 114: loss 0.19819030165672302\n",
      "epoch 115: loss 0.39649802446365356\n",
      "epoch 116: loss 0.31190162897109985\n",
      "epoch 117: loss 0.27474504709243774\n",
      "epoch 118: loss 0.32791560888290405\n",
      "epoch 119: loss 0.3721344470977783\n",
      "epoch 120: loss 0.28418487310409546\n",
      "epoch 121: loss 0.4085436463356018\n",
      "epoch 122: loss 0.26832476258277893\n",
      "epoch 123: loss 0.4017941355705261\n",
      "epoch 124: loss 0.3055132329463959\n",
      "epoch 125: loss 0.326987087726593\n",
      "epoch 126: loss 0.35902243852615356\n",
      "epoch 127: loss 0.3802173137664795\n",
      "epoch 128: loss 0.3017972409725189\n",
      "epoch 129: loss 0.3438692092895508\n",
      "epoch 130: loss 0.3819493055343628\n",
      "epoch 131: loss 0.32401472330093384\n",
      "epoch 132: loss 0.33004188537597656\n",
      "epoch 133: loss 0.37247198820114136\n",
      "epoch 134: loss 0.31700944900512695\n",
      "epoch 135: loss 0.2605605721473694\n",
      "epoch 136: loss 0.24022208154201508\n",
      "epoch 0: loss 0.36642372608184814\n",
      "epoch 1: loss 0.33401408791542053\n",
      "epoch 2: loss 0.395882248878479\n",
      "epoch 3: loss 0.3908085227012634\n",
      "epoch 4: loss 0.33032381534576416\n",
      "epoch 5: loss 0.26824694871902466\n",
      "epoch 6: loss 0.29246705770492554\n",
      "epoch 7: loss 0.2419264018535614\n",
      "epoch 8: loss 0.4384761154651642\n",
      "epoch 9: loss 0.46938732266426086\n",
      "epoch 10: loss 0.3587222993373871\n",
      "epoch 11: loss 0.2769649624824524\n",
      "epoch 12: loss 0.3546810448169708\n",
      "epoch 13: loss 0.41570407152175903\n",
      "epoch 14: loss 0.3535783290863037\n",
      "epoch 15: loss 0.29246455430984497\n",
      "epoch 16: loss 0.33623820543289185\n",
      "epoch 17: loss 0.3536638021469116\n",
      "epoch 18: loss 0.4040891230106354\n",
      "epoch 19: loss 0.2705252468585968\n",
      "epoch 20: loss 0.3071094751358032\n",
      "epoch 21: loss 0.33622169494628906\n",
      "epoch 22: loss 0.31896692514419556\n",
      "epoch 23: loss 0.36961203813552856\n",
      "epoch 24: loss 0.3621993660926819\n",
      "epoch 25: loss 0.39154180884361267\n",
      "epoch 26: loss 0.31369736790657043\n",
      "epoch 27: loss 0.3486504852771759\n",
      "epoch 28: loss 0.40950891375541687\n",
      "epoch 29: loss 0.3170177638530731\n",
      "epoch 30: loss 0.277576208114624\n",
      "epoch 31: loss 0.33844733238220215\n",
      "epoch 32: loss 0.3385656476020813\n",
      "epoch 33: loss 0.348357617855072\n",
      "epoch 34: loss 0.3890955150127411\n",
      "epoch 35: loss 0.2602968215942383\n",
      "epoch 36: loss 0.33392104506492615\n",
      "epoch 37: loss 0.31051522493362427\n",
      "epoch 38: loss 0.21604463458061218\n",
      "epoch 39: loss 0.3218431770801544\n",
      "epoch 40: loss 0.32113125920295715\n",
      "epoch 41: loss 0.27923622727394104\n",
      "epoch 42: loss 0.2630935311317444\n",
      "epoch 43: loss 0.32287997007369995\n",
      "epoch 44: loss 0.34452611207962036\n",
      "epoch 45: loss 0.27763956785202026\n",
      "epoch 46: loss 0.2956089973449707\n",
      "epoch 47: loss 0.2776804268360138\n",
      "epoch 48: loss 0.2565207779407501\n",
      "epoch 49: loss 0.3309454917907715\n",
      "epoch 50: loss 0.2866840958595276\n",
      "epoch 51: loss 0.42941251397132874\n",
      "epoch 52: loss 0.2994377613067627\n",
      "epoch 53: loss 0.3698921501636505\n",
      "epoch 54: loss 0.34916749596595764\n",
      "epoch 55: loss 0.3017427623271942\n",
      "epoch 56: loss 0.3553585410118103\n",
      "epoch 57: loss 0.2581138610839844\n",
      "epoch 58: loss 0.3308941721916199\n",
      "epoch 59: loss 0.249969944357872\n",
      "epoch 60: loss 0.3435171842575073\n",
      "epoch 61: loss 0.2240675389766693\n",
      "epoch 62: loss 0.23877349495887756\n",
      "epoch 63: loss 0.3719788193702698\n",
      "epoch 64: loss 0.30277153849601746\n",
      "epoch 65: loss 0.4022916853427887\n",
      "epoch 66: loss 0.339962363243103\n",
      "epoch 67: loss 0.38123199343681335\n",
      "epoch 68: loss 0.37096303701400757\n",
      "epoch 69: loss 0.42480146884918213\n",
      "epoch 70: loss 0.39313554763793945\n",
      "epoch 71: loss 0.30776506662368774\n",
      "epoch 72: loss 0.2609248757362366\n",
      "epoch 73: loss 0.3381468653678894\n",
      "epoch 74: loss 0.286699116230011\n",
      "epoch 75: loss 0.3279004693031311\n",
      "epoch 76: loss 0.3135219216346741\n",
      "epoch 77: loss 0.3950299620628357\n",
      "epoch 78: loss 0.31803804636001587\n",
      "epoch 79: loss 0.33076339960098267\n",
      "epoch 80: loss 0.3812084197998047\n",
      "epoch 81: loss 0.26589035987854004\n",
      "epoch 82: loss 0.3283882737159729\n",
      "epoch 83: loss 0.37646692991256714\n",
      "epoch 84: loss 0.2951164245605469\n",
      "epoch 85: loss 0.3077274560928345\n",
      "epoch 86: loss 0.3448411822319031\n",
      "epoch 87: loss 0.3519449234008789\n",
      "epoch 88: loss 0.2726821303367615\n",
      "epoch 89: loss 0.35884958505630493\n",
      "epoch 90: loss 0.2894497215747833\n",
      "epoch 91: loss 0.34983471035957336\n",
      "epoch 92: loss 0.27369850873947144\n",
      "epoch 93: loss 0.37073636054992676\n",
      "epoch 94: loss 0.3427456021308899\n",
      "epoch 95: loss 0.2015189826488495\n",
      "epoch 96: loss 0.22787925601005554\n",
      "epoch 97: loss 0.32070648670196533\n",
      "epoch 98: loss 0.3718850612640381\n",
      "epoch 99: loss 0.39862552285194397\n",
      "epoch 100: loss 0.5331284999847412\n",
      "epoch 101: loss 0.30787479877471924\n",
      "epoch 102: loss 0.27052855491638184\n",
      "epoch 103: loss 0.3188595771789551\n",
      "epoch 104: loss 0.31554606556892395\n",
      "epoch 105: loss 0.37377601861953735\n",
      "epoch 106: loss 0.3198074698448181\n",
      "epoch 107: loss 0.29210329055786133\n",
      "epoch 108: loss 0.35116827487945557\n",
      "epoch 109: loss 0.36469024419784546\n",
      "epoch 110: loss 0.35815316438674927\n",
      "epoch 111: loss 0.3886129856109619\n",
      "epoch 112: loss 0.39233165979385376\n",
      "epoch 113: loss 0.30646830797195435\n",
      "epoch 114: loss 0.19525772333145142\n",
      "epoch 115: loss 0.4013002812862396\n",
      "epoch 116: loss 0.3146657347679138\n",
      "epoch 117: loss 0.3012484014034271\n",
      "epoch 118: loss 0.32912230491638184\n",
      "epoch 119: loss 0.37000012397766113\n",
      "epoch 120: loss 0.28289124369621277\n",
      "epoch 121: loss 0.4087955355644226\n",
      "epoch 122: loss 0.26879656314849854\n",
      "epoch 123: loss 0.4129151701927185\n",
      "epoch 124: loss 0.31334608793258667\n",
      "epoch 125: loss 0.32115909457206726\n",
      "epoch 126: loss 0.3633449077606201\n",
      "epoch 127: loss 0.3888258934020996\n",
      "epoch 128: loss 0.28003573417663574\n",
      "epoch 129: loss 0.35365819931030273\n",
      "epoch 130: loss 0.36530134081840515\n",
      "epoch 131: loss 0.33067554235458374\n",
      "epoch 132: loss 0.3164718747138977\n",
      "epoch 133: loss 0.3502315878868103\n",
      "epoch 134: loss 0.3205621838569641\n",
      "epoch 135: loss 0.2582244873046875\n",
      "epoch 136: loss 0.23879839479923248\n",
      "epoch 0: loss 0.3808751106262207\n",
      "epoch 1: loss 0.33749181032180786\n",
      "epoch 2: loss 0.4215092658996582\n",
      "epoch 3: loss 0.3663874864578247\n",
      "epoch 4: loss 0.324606716632843\n",
      "epoch 5: loss 0.27632635831832886\n",
      "epoch 6: loss 0.2877674698829651\n",
      "epoch 7: loss 0.23005211353302002\n",
      "epoch 8: loss 0.41544240713119507\n",
      "epoch 9: loss 0.44295060634613037\n",
      "epoch 10: loss 0.35013875365257263\n",
      "epoch 11: loss 0.2810647487640381\n",
      "epoch 12: loss 0.3138369917869568\n",
      "epoch 13: loss 0.4194832444190979\n",
      "epoch 14: loss 0.3807602524757385\n",
      "epoch 15: loss 0.2895354628562927\n",
      "epoch 16: loss 0.33540570735931396\n",
      "epoch 17: loss 0.3684384226799011\n",
      "epoch 18: loss 0.42085790634155273\n",
      "epoch 19: loss 0.27694571018218994\n",
      "epoch 20: loss 0.31079351902008057\n",
      "epoch 21: loss 0.3399962782859802\n",
      "epoch 22: loss 0.32017266750335693\n",
      "epoch 23: loss 0.3759310245513916\n",
      "epoch 24: loss 0.36233770847320557\n",
      "epoch 25: loss 0.39066773653030396\n",
      "epoch 26: loss 0.31435781717300415\n",
      "epoch 27: loss 0.352247953414917\n",
      "epoch 28: loss 0.40638378262519836\n",
      "epoch 29: loss 0.31678757071495056\n",
      "epoch 30: loss 0.28361600637435913\n",
      "epoch 31: loss 0.3364585041999817\n",
      "epoch 32: loss 0.33689016103744507\n",
      "epoch 33: loss 0.34558600187301636\n",
      "epoch 34: loss 0.3868046998977661\n",
      "epoch 35: loss 0.259195476770401\n",
      "epoch 36: loss 0.32768741250038147\n",
      "epoch 37: loss 0.3149637281894684\n",
      "epoch 38: loss 0.21846598386764526\n",
      "epoch 39: loss 0.3237627148628235\n",
      "epoch 40: loss 0.3196382522583008\n",
      "epoch 41: loss 0.27964654564857483\n",
      "epoch 42: loss 0.2629151940345764\n",
      "epoch 43: loss 0.32390129566192627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44: loss 0.34453344345092773\n",
      "epoch 45: loss 0.2779581844806671\n",
      "epoch 46: loss 0.29379844665527344\n",
      "epoch 47: loss 0.278336763381958\n",
      "epoch 48: loss 0.2560812830924988\n",
      "epoch 49: loss 0.33073896169662476\n",
      "epoch 50: loss 0.28643810749053955\n",
      "epoch 51: loss 0.4294721484184265\n",
      "epoch 52: loss 0.3003113269805908\n",
      "epoch 53: loss 0.3705654740333557\n",
      "epoch 54: loss 0.34852439165115356\n",
      "epoch 55: loss 0.3016382157802582\n",
      "epoch 56: loss 0.3531770408153534\n",
      "epoch 57: loss 0.2574208378791809\n",
      "epoch 58: loss 0.33124327659606934\n",
      "epoch 59: loss 0.24996882677078247\n",
      "epoch 60: loss 0.3420765995979309\n",
      "epoch 61: loss 0.22222954034805298\n",
      "epoch 62: loss 0.23748819530010223\n",
      "epoch 63: loss 0.3703973591327667\n",
      "epoch 64: loss 0.3000366985797882\n",
      "epoch 65: loss 0.400296688079834\n",
      "epoch 66: loss 0.33951160311698914\n",
      "epoch 67: loss 0.3799762725830078\n",
      "epoch 68: loss 0.36982470750808716\n",
      "epoch 69: loss 0.4218781292438507\n",
      "epoch 70: loss 0.3895031809806824\n",
      "epoch 71: loss 0.3090566098690033\n",
      "epoch 72: loss 0.2615852653980255\n",
      "epoch 73: loss 0.3375388979911804\n",
      "epoch 74: loss 0.2846892774105072\n",
      "epoch 75: loss 0.3302326798439026\n",
      "epoch 76: loss 0.31019383668899536\n",
      "epoch 77: loss 0.3947508931159973\n",
      "epoch 78: loss 0.3155839145183563\n",
      "epoch 79: loss 0.330590158700943\n",
      "epoch 80: loss 0.3785915672779083\n",
      "epoch 81: loss 0.2662831246852875\n",
      "epoch 82: loss 0.3263700604438782\n",
      "epoch 83: loss 0.37659212946891785\n",
      "epoch 84: loss 0.2869895100593567\n",
      "epoch 85: loss 0.30876606702804565\n",
      "epoch 86: loss 0.341600239276886\n",
      "epoch 87: loss 0.3528386354446411\n",
      "epoch 88: loss 0.2704213261604309\n",
      "epoch 89: loss 0.3605138063430786\n",
      "epoch 90: loss 0.2900550961494446\n",
      "epoch 91: loss 0.35810554027557373\n",
      "epoch 92: loss 0.28122901916503906\n",
      "epoch 93: loss 0.3470017910003662\n",
      "epoch 94: loss 0.36826133728027344\n",
      "epoch 95: loss 0.20557096600532532\n",
      "epoch 96: loss 0.23077115416526794\n",
      "epoch 97: loss 0.3217804729938507\n",
      "epoch 98: loss 0.3800855576992035\n",
      "epoch 99: loss 0.41744881868362427\n",
      "epoch 100: loss 0.563888430595398\n",
      "epoch 101: loss 0.31832364201545715\n",
      "epoch 102: loss 0.27854061126708984\n",
      "epoch 103: loss 0.31779447197914124\n",
      "epoch 104: loss 0.3138827383518219\n",
      "epoch 105: loss 0.3954402208328247\n",
      "epoch 106: loss 0.34631919860839844\n",
      "epoch 107: loss 0.3004872798919678\n",
      "epoch 108: loss 0.35582447052001953\n",
      "epoch 109: loss 0.354292094707489\n",
      "epoch 110: loss 0.35910937190055847\n",
      "epoch 111: loss 0.38226592540740967\n",
      "epoch 112: loss 0.3986053466796875\n",
      "epoch 113: loss 0.31177109479904175\n",
      "epoch 114: loss 0.1897951066493988\n",
      "epoch 115: loss 0.4112606942653656\n",
      "epoch 116: loss 0.3039938807487488\n",
      "epoch 117: loss 0.2882903218269348\n",
      "epoch 118: loss 0.33256274461746216\n",
      "epoch 119: loss 0.3674309253692627\n",
      "epoch 120: loss 0.27022606134414673\n",
      "epoch 121: loss 0.3992905020713806\n",
      "epoch 122: loss 0.2609488368034363\n",
      "epoch 123: loss 0.3931376039981842\n",
      "epoch 124: loss 0.3066423535346985\n",
      "epoch 125: loss 0.3392154574394226\n",
      "epoch 126: loss 0.3605029582977295\n",
      "epoch 127: loss 0.3748657703399658\n",
      "epoch 128: loss 0.27910542488098145\n",
      "epoch 129: loss 0.34835416078567505\n",
      "epoch 130: loss 0.36958950757980347\n",
      "epoch 131: loss 0.3165954649448395\n",
      "epoch 132: loss 0.3136345148086548\n",
      "epoch 133: loss 0.3545983135700226\n",
      "epoch 134: loss 0.3202018737792969\n",
      "epoch 135: loss 0.26163870096206665\n",
      "epoch 136: loss 0.24481596052646637\n",
      "epoch 0: loss 0.3635138273239136\n",
      "epoch 1: loss 0.3355032503604889\n",
      "epoch 2: loss 0.41869470477104187\n",
      "epoch 3: loss 0.37010979652404785\n",
      "epoch 4: loss 0.3402799963951111\n",
      "epoch 5: loss 0.2693406939506531\n",
      "epoch 6: loss 0.28745222091674805\n",
      "epoch 7: loss 0.23325452208518982\n",
      "epoch 8: loss 0.43815574049949646\n",
      "epoch 9: loss 0.4550190567970276\n",
      "epoch 10: loss 0.3449561297893524\n",
      "epoch 11: loss 0.27421438694000244\n",
      "epoch 12: loss 0.3618846535682678\n",
      "epoch 13: loss 0.4272416830062866\n",
      "epoch 14: loss 0.3410731256008148\n",
      "epoch 15: loss 0.29456886649131775\n",
      "epoch 16: loss 0.3341540992259979\n",
      "epoch 17: loss 0.35175859928131104\n",
      "epoch 18: loss 0.3988187611103058\n",
      "epoch 19: loss 0.2630765438079834\n",
      "epoch 20: loss 0.30006468296051025\n",
      "epoch 21: loss 0.33457690477371216\n",
      "epoch 22: loss 0.3220826983451843\n",
      "epoch 23: loss 0.3703041076660156\n",
      "epoch 24: loss 0.3551938533782959\n",
      "epoch 25: loss 0.39696410298347473\n",
      "epoch 26: loss 0.3193676173686981\n",
      "epoch 27: loss 0.3447134494781494\n",
      "epoch 28: loss 0.40305930376052856\n",
      "epoch 29: loss 0.31451255083084106\n",
      "epoch 30: loss 0.27565354108810425\n",
      "epoch 31: loss 0.34160810708999634\n",
      "epoch 32: loss 0.33921563625335693\n",
      "epoch 33: loss 0.34808042645454407\n",
      "epoch 34: loss 0.39028531312942505\n",
      "epoch 35: loss 0.2599514424800873\n",
      "epoch 36: loss 0.3364134132862091\n",
      "epoch 37: loss 0.30619242787361145\n",
      "epoch 38: loss 0.2134481966495514\n",
      "epoch 39: loss 0.31284481287002563\n",
      "epoch 40: loss 0.31105971336364746\n",
      "epoch 41: loss 0.28002554178237915\n",
      "epoch 42: loss 0.26678401231765747\n",
      "epoch 43: loss 0.3172004818916321\n",
      "epoch 44: loss 0.3500661253929138\n",
      "epoch 45: loss 0.28054991364479065\n",
      "epoch 46: loss 0.29292482137680054\n",
      "epoch 47: loss 0.27799856662750244\n",
      "epoch 48: loss 0.2614862322807312\n",
      "epoch 49: loss 0.33574622869491577\n",
      "epoch 50: loss 0.2839248776435852\n",
      "epoch 51: loss 0.42271688580513\n",
      "epoch 52: loss 0.29743969440460205\n",
      "epoch 53: loss 0.36829328536987305\n",
      "epoch 54: loss 0.3483678698539734\n",
      "epoch 55: loss 0.3012004494667053\n",
      "epoch 56: loss 0.3538208603858948\n",
      "epoch 57: loss 0.25887298583984375\n",
      "epoch 58: loss 0.33477598428726196\n",
      "epoch 59: loss 0.252266526222229\n",
      "epoch 60: loss 0.3437778353691101\n",
      "epoch 61: loss 0.2196963131427765\n",
      "epoch 62: loss 0.23718945682048798\n",
      "epoch 63: loss 0.3642944097518921\n",
      "epoch 64: loss 0.2952342629432678\n",
      "epoch 65: loss 0.39877957105636597\n",
      "epoch 66: loss 0.34038305282592773\n",
      "epoch 67: loss 0.379177987575531\n",
      "epoch 68: loss 0.36592793464660645\n",
      "epoch 69: loss 0.42441874742507935\n",
      "epoch 70: loss 0.38613003492355347\n",
      "epoch 71: loss 0.31167054176330566\n",
      "epoch 72: loss 0.26359647512435913\n",
      "epoch 73: loss 0.33329978585243225\n",
      "epoch 74: loss 0.2790452241897583\n",
      "epoch 75: loss 0.3265506625175476\n",
      "epoch 76: loss 0.30857789516448975\n",
      "epoch 77: loss 0.39140668511390686\n",
      "epoch 78: loss 0.3194710612297058\n",
      "epoch 79: loss 0.3306382894515991\n",
      "epoch 80: loss 0.37450653314590454\n",
      "epoch 81: loss 0.2640184462070465\n",
      "epoch 82: loss 0.32492607831954956\n",
      "epoch 83: loss 0.3754734992980957\n",
      "epoch 84: loss 0.27486804127693176\n",
      "epoch 85: loss 0.310043066740036\n",
      "epoch 86: loss 0.3418152332305908\n",
      "epoch 87: loss 0.35320061445236206\n",
      "epoch 88: loss 0.2692142724990845\n",
      "epoch 89: loss 0.34805774688720703\n",
      "epoch 90: loss 0.28421324491500854\n",
      "epoch 91: loss 0.3382807672023773\n",
      "epoch 92: loss 0.2667964696884155\n",
      "epoch 93: loss 0.3782842457294464\n",
      "epoch 94: loss 0.3082256019115448\n",
      "epoch 95: loss 0.19271406531333923\n",
      "epoch 96: loss 0.22264325618743896\n",
      "epoch 97: loss 0.32075566053390503\n",
      "epoch 98: loss 0.3602491021156311\n",
      "epoch 99: loss 0.3748731315135956\n",
      "epoch 100: loss 0.5020199418067932\n",
      "epoch 101: loss 0.30604004859924316\n",
      "epoch 102: loss 0.2682816982269287\n",
      "epoch 103: loss 0.3110862970352173\n",
      "epoch 104: loss 0.29265618324279785\n",
      "epoch 105: loss 0.374165415763855\n",
      "epoch 106: loss 0.3168077766895294\n",
      "epoch 107: loss 0.2844752073287964\n",
      "epoch 108: loss 0.35056400299072266\n",
      "epoch 109: loss 0.35429847240448\n",
      "epoch 110: loss 0.35477927327156067\n",
      "epoch 111: loss 0.37812286615371704\n",
      "epoch 112: loss 0.3723670244216919\n",
      "epoch 113: loss 0.2853030264377594\n",
      "epoch 114: loss 0.19408170878887177\n",
      "epoch 115: loss 0.39583492279052734\n",
      "epoch 116: loss 0.2985249161720276\n",
      "epoch 117: loss 0.26439163088798523\n",
      "epoch 118: loss 0.32537901401519775\n",
      "epoch 119: loss 0.37728267908096313\n",
      "epoch 120: loss 0.2674454152584076\n",
      "epoch 121: loss 0.37749388813972473\n",
      "epoch 122: loss 0.25361424684524536\n",
      "epoch 123: loss 0.38368216156959534\n",
      "epoch 124: loss 0.3191271424293518\n",
      "epoch 125: loss 0.33297115564346313\n",
      "epoch 126: loss 0.37071678042411804\n",
      "epoch 127: loss 0.42802950739860535\n",
      "epoch 128: loss 0.2875976860523224\n",
      "epoch 129: loss 0.34478506445884705\n",
      "epoch 130: loss 0.4111858010292053\n",
      "epoch 131: loss 0.32968729734420776\n",
      "epoch 132: loss 0.3290044069290161\n",
      "epoch 133: loss 0.3537617325782776\n",
      "epoch 134: loss 0.32789331674575806\n",
      "epoch 135: loss 0.2835521697998047\n",
      "epoch 136: loss 0.27376821637153625\n",
      "epoch 0: loss 0.3547314703464508\n",
      "epoch 1: loss 0.3379962742328644\n",
      "epoch 2: loss 0.4037584960460663\n",
      "epoch 3: loss 0.36449357867240906\n",
      "epoch 4: loss 0.31277868151664734\n",
      "epoch 5: loss 0.2677059471607208\n",
      "epoch 6: loss 0.2876690626144409\n",
      "epoch 7: loss 0.23262453079223633\n",
      "epoch 8: loss 0.40018323063850403\n",
      "epoch 9: loss 0.4078746438026428\n",
      "epoch 10: loss 0.31439706683158875\n",
      "epoch 11: loss 0.27276816964149475\n",
      "epoch 12: loss 0.308224081993103\n",
      "epoch 13: loss 0.44597822427749634\n",
      "epoch 14: loss 0.3413993716239929\n",
      "epoch 15: loss 0.27904391288757324\n",
      "epoch 16: loss 0.3432008624076843\n",
      "epoch 17: loss 0.3301381468772888\n",
      "epoch 18: loss 0.36084267497062683\n",
      "epoch 19: loss 0.3092641532421112\n",
      "epoch 20: loss 0.28480851650238037\n",
      "epoch 21: loss 0.360342800617218\n",
      "epoch 22: loss 0.3349292278289795\n",
      "epoch 23: loss 0.395351767539978\n",
      "epoch 24: loss 0.3679797649383545\n",
      "epoch 25: loss 0.3882976770401001\n",
      "epoch 26: loss 0.368102490901947\n",
      "epoch 27: loss 0.34185630083084106\n",
      "epoch 28: loss 0.40488606691360474\n",
      "epoch 29: loss 0.3136028051376343\n",
      "epoch 30: loss 0.29488223791122437\n",
      "epoch 31: loss 0.3397079408168793\n",
      "epoch 32: loss 0.3413717746734619\n",
      "epoch 33: loss 0.33692944049835205\n",
      "epoch 34: loss 0.37795335054397583\n",
      "epoch 35: loss 0.2638155519962311\n",
      "epoch 36: loss 0.3282565474510193\n",
      "epoch 37: loss 0.3188641667366028\n",
      "epoch 38: loss 0.22912997007369995\n",
      "epoch 39: loss 0.31656867265701294\n",
      "epoch 40: loss 0.31261831521987915\n",
      "epoch 41: loss 0.27970626950263977\n",
      "epoch 42: loss 0.2679445743560791\n",
      "epoch 43: loss 0.3233099579811096\n",
      "epoch 44: loss 0.34525570273399353\n",
      "epoch 45: loss 0.2780529260635376\n",
      "epoch 46: loss 0.30119431018829346\n",
      "epoch 47: loss 0.2845677435398102\n",
      "epoch 48: loss 0.2558419108390808\n",
      "epoch 49: loss 0.3315061330795288\n",
      "epoch 50: loss 0.2815329134464264\n",
      "epoch 51: loss 0.41382652521133423\n",
      "epoch 52: loss 0.29316002130508423\n",
      "epoch 53: loss 0.36625438928604126\n",
      "epoch 54: loss 0.3423101305961609\n",
      "epoch 55: loss 0.29644280672073364\n",
      "epoch 56: loss 0.349022775888443\n",
      "epoch 57: loss 0.24618732929229736\n",
      "epoch 58: loss 0.3305870294570923\n",
      "epoch 59: loss 0.253653883934021\n",
      "epoch 60: loss 0.33629751205444336\n",
      "epoch 61: loss 0.2168217897415161\n",
      "epoch 62: loss 0.23817767202854156\n",
      "epoch 63: loss 0.3528909683227539\n",
      "epoch 64: loss 0.2928909957408905\n",
      "epoch 65: loss 0.392825722694397\n",
      "epoch 66: loss 0.33683741092681885\n",
      "epoch 67: loss 0.3720848262310028\n",
      "epoch 68: loss 0.3616511821746826\n",
      "epoch 69: loss 0.42357707023620605\n",
      "epoch 70: loss 0.3786796033382416\n",
      "epoch 71: loss 0.3287348747253418\n",
      "epoch 72: loss 0.2605309784412384\n",
      "epoch 73: loss 0.3357451260089874\n",
      "epoch 74: loss 0.27722448110580444\n",
      "epoch 75: loss 0.3389272093772888\n",
      "epoch 76: loss 0.3045119345188141\n",
      "epoch 77: loss 0.4088743329048157\n",
      "epoch 78: loss 0.3106100261211395\n",
      "epoch 79: loss 0.3312971591949463\n",
      "epoch 80: loss 0.37378308176994324\n",
      "epoch 81: loss 0.26780515909194946\n",
      "epoch 82: loss 0.3262436091899872\n",
      "epoch 83: loss 0.3758506774902344\n",
      "epoch 84: loss 0.260312020778656\n",
      "epoch 85: loss 0.3086851239204407\n",
      "epoch 86: loss 0.3387834429740906\n",
      "epoch 87: loss 0.3457701802253723\n",
      "epoch 88: loss 0.2666807174682617\n",
      "epoch 89: loss 0.34914225339889526\n",
      "epoch 90: loss 0.28946900367736816\n",
      "epoch 91: loss 0.34501680731773376\n",
      "epoch 92: loss 0.27720820903778076\n",
      "epoch 93: loss 0.3407924175262451\n",
      "epoch 94: loss 0.33946436643600464\n",
      "epoch 95: loss 0.20032107830047607\n",
      "epoch 96: loss 0.2216394543647766\n",
      "epoch 97: loss 0.32575562596321106\n",
      "epoch 98: loss 0.36874961853027344\n",
      "epoch 99: loss 0.38866758346557617\n",
      "epoch 100: loss 0.5242997407913208\n",
      "epoch 101: loss 0.30554771423339844\n",
      "epoch 102: loss 0.2689944803714752\n",
      "epoch 103: loss 0.32033461332321167\n",
      "epoch 104: loss 0.30312108993530273\n",
      "epoch 105: loss 0.3727370500564575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 106: loss 0.3211989998817444\n",
      "epoch 107: loss 0.2912852168083191\n",
      "epoch 108: loss 0.3501991033554077\n",
      "epoch 109: loss 0.36761200428009033\n",
      "epoch 110: loss 0.3589288592338562\n",
      "epoch 111: loss 0.3900170624256134\n",
      "epoch 112: loss 0.3894140422344208\n",
      "epoch 113: loss 0.3039039075374603\n",
      "epoch 114: loss 0.19651943445205688\n",
      "epoch 115: loss 0.39377012848854065\n",
      "epoch 116: loss 0.3351152241230011\n",
      "epoch 117: loss 0.2804480791091919\n",
      "epoch 118: loss 0.32715606689453125\n",
      "epoch 119: loss 0.37233003973960876\n",
      "epoch 120: loss 0.29095810651779175\n",
      "epoch 121: loss 0.41371119022369385\n",
      "epoch 122: loss 0.2717219293117523\n",
      "epoch 123: loss 0.41609668731689453\n",
      "epoch 124: loss 0.314349502325058\n",
      "epoch 125: loss 0.3183843493461609\n",
      "epoch 126: loss 0.36403143405914307\n",
      "epoch 127: loss 0.3841734528541565\n",
      "epoch 128: loss 0.28223907947540283\n",
      "epoch 129: loss 0.3506276607513428\n",
      "epoch 130: loss 0.3660022020339966\n",
      "epoch 131: loss 0.32768094539642334\n",
      "epoch 132: loss 0.3159340023994446\n",
      "epoch 133: loss 0.35715150833129883\n",
      "epoch 134: loss 0.31746912002563477\n",
      "epoch 135: loss 0.2610935568809509\n",
      "epoch 136: loss 0.23908193409442902\n",
      "epoch 0: loss 0.3836379647254944\n",
      "epoch 1: loss 0.34033632278442383\n",
      "epoch 2: loss 0.42513811588287354\n",
      "epoch 3: loss 0.3656241297721863\n",
      "epoch 4: loss 0.31551915407180786\n",
      "epoch 5: loss 0.2754063904285431\n",
      "epoch 6: loss 0.287202388048172\n",
      "epoch 7: loss 0.23058190941810608\n",
      "epoch 8: loss 0.4056010842323303\n",
      "epoch 9: loss 0.4145418107509613\n",
      "epoch 10: loss 0.3345835506916046\n",
      "epoch 11: loss 0.27022916078567505\n",
      "epoch 12: loss 0.32403022050857544\n",
      "epoch 13: loss 0.41405510902404785\n",
      "epoch 14: loss 0.3593478798866272\n",
      "epoch 15: loss 0.280582994222641\n",
      "epoch 16: loss 0.341296911239624\n",
      "epoch 17: loss 0.3501322269439697\n",
      "epoch 18: loss 0.4090418815612793\n",
      "epoch 19: loss 0.2631342113018036\n",
      "epoch 20: loss 0.2996900677680969\n",
      "epoch 21: loss 0.33201876282691956\n",
      "epoch 22: loss 0.31163686513900757\n",
      "epoch 23: loss 0.3694310784339905\n",
      "epoch 24: loss 0.3489232659339905\n",
      "epoch 25: loss 0.39173460006713867\n",
      "epoch 26: loss 0.3114566504955292\n",
      "epoch 27: loss 0.34734559059143066\n",
      "epoch 28: loss 0.41021233797073364\n",
      "epoch 29: loss 0.3154141902923584\n",
      "epoch 30: loss 0.2776942849159241\n",
      "epoch 31: loss 0.33802634477615356\n",
      "epoch 32: loss 0.33644258975982666\n",
      "epoch 33: loss 0.34565988183021545\n",
      "epoch 34: loss 0.38446006178855896\n",
      "epoch 35: loss 0.26058241724967957\n",
      "epoch 36: loss 0.3275904059410095\n",
      "epoch 37: loss 0.3187040686607361\n",
      "epoch 38: loss 0.22124367952346802\n",
      "epoch 39: loss 0.32391923666000366\n",
      "epoch 40: loss 0.3211371600627899\n",
      "epoch 41: loss 0.27849283814430237\n",
      "epoch 42: loss 0.2617281973361969\n",
      "epoch 43: loss 0.32610762119293213\n",
      "epoch 44: loss 0.341152548789978\n",
      "epoch 45: loss 0.27718836069107056\n",
      "epoch 46: loss 0.2926449775695801\n",
      "epoch 47: loss 0.2776612937450409\n",
      "epoch 48: loss 0.25458934903144836\n",
      "epoch 49: loss 0.3326084017753601\n",
      "epoch 50: loss 0.2855042517185211\n",
      "epoch 51: loss 0.4269700348377228\n",
      "epoch 52: loss 0.29534298181533813\n",
      "epoch 53: loss 0.36942416429519653\n",
      "epoch 54: loss 0.34579455852508545\n",
      "epoch 55: loss 0.30676761269569397\n",
      "epoch 56: loss 0.35793983936309814\n",
      "epoch 57: loss 0.24712315201759338\n",
      "epoch 58: loss 0.3317410349845886\n",
      "epoch 59: loss 0.25232994556427\n",
      "epoch 60: loss 0.3458794355392456\n",
      "epoch 61: loss 0.22323980927467346\n",
      "epoch 62: loss 0.24144819378852844\n",
      "epoch 63: loss 0.3626331090927124\n",
      "epoch 64: loss 0.2892489433288574\n",
      "epoch 65: loss 0.3959972858428955\n",
      "epoch 66: loss 0.34122222661972046\n",
      "epoch 67: loss 0.38070905208587646\n",
      "epoch 68: loss 0.3611506521701813\n",
      "epoch 69: loss 0.426092267036438\n",
      "epoch 70: loss 0.39140763878822327\n",
      "epoch 71: loss 0.3066624402999878\n",
      "epoch 72: loss 0.2594738006591797\n",
      "epoch 73: loss 0.331828236579895\n",
      "epoch 74: loss 0.27952757477760315\n",
      "epoch 75: loss 0.32872486114501953\n",
      "epoch 76: loss 0.31483757495880127\n",
      "epoch 77: loss 0.39594414830207825\n",
      "epoch 78: loss 0.3182723820209503\n",
      "epoch 79: loss 0.3264898657798767\n",
      "epoch 80: loss 0.3786439597606659\n",
      "epoch 81: loss 0.26555007696151733\n",
      "epoch 82: loss 0.33055350184440613\n",
      "epoch 83: loss 0.3741776645183563\n",
      "epoch 84: loss 0.2885746359825134\n",
      "epoch 85: loss 0.3050544857978821\n",
      "epoch 86: loss 0.34072014689445496\n",
      "epoch 87: loss 0.35135918855667114\n",
      "epoch 88: loss 0.270291805267334\n",
      "epoch 89: loss 0.3561820387840271\n",
      "epoch 90: loss 0.2923041880130768\n",
      "epoch 91: loss 0.35407111048698425\n",
      "epoch 92: loss 0.2785990834236145\n",
      "epoch 93: loss 0.35553619265556335\n",
      "epoch 94: loss 0.3579331040382385\n",
      "epoch 95: loss 0.20528072118759155\n",
      "epoch 96: loss 0.22332462668418884\n",
      "epoch 97: loss 0.3215315341949463\n",
      "epoch 98: loss 0.37295299768447876\n",
      "epoch 99: loss 0.396903932094574\n",
      "epoch 100: loss 0.542978823184967\n",
      "epoch 101: loss 0.31071674823760986\n",
      "epoch 102: loss 0.2690986394882202\n",
      "epoch 103: loss 0.3097594380378723\n",
      "epoch 104: loss 0.3167726397514343\n",
      "epoch 105: loss 0.3795052766799927\n",
      "epoch 106: loss 0.3204244077205658\n",
      "epoch 107: loss 0.2868569791316986\n",
      "epoch 108: loss 0.34828850626945496\n",
      "epoch 109: loss 0.35828644037246704\n",
      "epoch 110: loss 0.354816198348999\n",
      "epoch 111: loss 0.38440537452697754\n",
      "epoch 112: loss 0.38317883014678955\n",
      "epoch 113: loss 0.29366517066955566\n",
      "epoch 114: loss 0.18665441870689392\n",
      "epoch 115: loss 0.3954973816871643\n",
      "epoch 116: loss 0.3146169185638428\n",
      "epoch 117: loss 0.2690965533256531\n",
      "epoch 118: loss 0.32696735858917236\n",
      "epoch 119: loss 0.38066503405570984\n",
      "epoch 120: loss 0.28372764587402344\n",
      "epoch 121: loss 0.4136558771133423\n",
      "epoch 122: loss 0.26584184169769287\n",
      "epoch 123: loss 0.3995826840400696\n",
      "epoch 124: loss 0.30232203006744385\n",
      "epoch 125: loss 0.3341442048549652\n",
      "epoch 126: loss 0.3570598363876343\n",
      "epoch 127: loss 0.3791689872741699\n",
      "epoch 128: loss 0.27703186869621277\n",
      "epoch 129: loss 0.34457123279571533\n",
      "epoch 130: loss 0.37374088168144226\n",
      "epoch 131: loss 0.3211159110069275\n",
      "epoch 132: loss 0.3217771649360657\n",
      "epoch 133: loss 0.36354243755340576\n",
      "epoch 134: loss 0.31928086280822754\n",
      "epoch 135: loss 0.2611791789531708\n",
      "epoch 136: loss 0.2417481690645218\n",
      "epoch 0: loss 0.3616501986980438\n",
      "epoch 1: loss 0.3327469527721405\n",
      "epoch 2: loss 0.39239200949668884\n",
      "epoch 3: loss 0.3843538165092468\n",
      "epoch 4: loss 0.3311889171600342\n",
      "epoch 5: loss 0.26380670070648193\n",
      "epoch 6: loss 0.29190897941589355\n",
      "epoch 7: loss 0.24307793378829956\n",
      "epoch 8: loss 0.4444596469402313\n",
      "epoch 9: loss 0.4729501008987427\n",
      "epoch 10: loss 0.3665093183517456\n",
      "epoch 11: loss 0.2821749150753021\n",
      "epoch 12: loss 0.328392893075943\n",
      "epoch 13: loss 0.4174203872680664\n",
      "epoch 14: loss 0.36903566122055054\n",
      "epoch 15: loss 0.2891733646392822\n",
      "epoch 16: loss 0.33655989170074463\n",
      "epoch 17: loss 0.36712750792503357\n",
      "epoch 18: loss 0.4139665961265564\n",
      "epoch 19: loss 0.28375351428985596\n",
      "epoch 20: loss 0.3147472143173218\n",
      "epoch 21: loss 0.3396243155002594\n",
      "epoch 22: loss 0.3192353844642639\n",
      "epoch 23: loss 0.3772503137588501\n",
      "epoch 24: loss 0.3668150305747986\n",
      "epoch 25: loss 0.39162176847457886\n",
      "epoch 26: loss 0.3111252188682556\n",
      "epoch 27: loss 0.3612957298755646\n",
      "epoch 28: loss 0.41682279109954834\n",
      "epoch 29: loss 0.31752437353134155\n",
      "epoch 30: loss 0.28405994176864624\n",
      "epoch 31: loss 0.33553963899612427\n",
      "epoch 32: loss 0.3372446894645691\n",
      "epoch 33: loss 0.3511015772819519\n",
      "epoch 34: loss 0.39097851514816284\n",
      "epoch 35: loss 0.26220741868019104\n",
      "epoch 36: loss 0.33667871356010437\n",
      "epoch 37: loss 0.30826687812805176\n",
      "epoch 38: loss 0.2179756611585617\n",
      "epoch 39: loss 0.31746548414230347\n",
      "epoch 40: loss 0.3133827745914459\n",
      "epoch 41: loss 0.2761944532394409\n",
      "epoch 42: loss 0.2570963203907013\n",
      "epoch 43: loss 0.31931811571121216\n",
      "epoch 44: loss 0.343519002199173\n",
      "epoch 45: loss 0.2767829895019531\n",
      "epoch 46: loss 0.2922228276729584\n",
      "epoch 47: loss 0.27674978971481323\n",
      "epoch 48: loss 0.2540245056152344\n",
      "epoch 49: loss 0.33261626958847046\n",
      "epoch 50: loss 0.2884232997894287\n",
      "epoch 51: loss 0.4351837933063507\n",
      "epoch 52: loss 0.3031409680843353\n",
      "epoch 53: loss 0.36920592188835144\n",
      "epoch 54: loss 0.3484690189361572\n",
      "epoch 55: loss 0.30083394050598145\n",
      "epoch 56: loss 0.3581874668598175\n",
      "epoch 57: loss 0.2565124034881592\n",
      "epoch 58: loss 0.328665554523468\n",
      "epoch 59: loss 0.2525143027305603\n",
      "epoch 60: loss 0.34070488810539246\n",
      "epoch 61: loss 0.2246396839618683\n",
      "epoch 62: loss 0.2401409149169922\n",
      "epoch 63: loss 0.3709670901298523\n",
      "epoch 64: loss 0.30100005865097046\n",
      "epoch 65: loss 0.40093880891799927\n",
      "epoch 66: loss 0.3343346416950226\n",
      "epoch 67: loss 0.37784555554389954\n",
      "epoch 68: loss 0.37462958693504333\n",
      "epoch 69: loss 0.416068971157074\n",
      "epoch 70: loss 0.38345956802368164\n",
      "epoch 71: loss 0.30752333998680115\n",
      "epoch 72: loss 0.26117682456970215\n",
      "epoch 73: loss 0.3362996280193329\n",
      "epoch 74: loss 0.2827557921409607\n",
      "epoch 75: loss 0.33240818977355957\n",
      "epoch 76: loss 0.30798324942588806\n",
      "epoch 77: loss 0.3968156576156616\n",
      "epoch 78: loss 0.3095589876174927\n",
      "epoch 79: loss 0.3325905501842499\n",
      "epoch 80: loss 0.3833412230014801\n",
      "epoch 81: loss 0.26587727665901184\n",
      "epoch 82: loss 0.32871299982070923\n",
      "epoch 83: loss 0.37482452392578125\n",
      "epoch 84: loss 0.30089426040649414\n",
      "epoch 85: loss 0.3062818646430969\n",
      "epoch 86: loss 0.3425775170326233\n",
      "epoch 87: loss 0.34584343433380127\n",
      "epoch 88: loss 0.2812603712081909\n",
      "epoch 89: loss 0.37714892625808716\n",
      "epoch 90: loss 0.30012714862823486\n",
      "epoch 91: loss 0.40209460258483887\n",
      "epoch 92: loss 0.31904155015945435\n",
      "epoch 93: loss 0.3335418403148651\n",
      "epoch 94: loss 0.3683449923992157\n",
      "epoch 95: loss 0.24749547243118286\n",
      "epoch 96: loss 0.23189863562583923\n",
      "epoch 97: loss 0.3112976551055908\n",
      "epoch 98: loss 0.35391831398010254\n",
      "epoch 99: loss 0.3789392113685608\n",
      "epoch 100: loss 0.5366790294647217\n",
      "epoch 101: loss 0.31630879640579224\n",
      "epoch 102: loss 0.27068978548049927\n",
      "epoch 103: loss 0.3082275390625\n",
      "epoch 104: loss 0.2801473140716553\n",
      "epoch 105: loss 0.37017202377319336\n",
      "epoch 106: loss 0.3497893214225769\n",
      "epoch 107: loss 0.2847101092338562\n",
      "epoch 108: loss 0.3494041860103607\n",
      "epoch 109: loss 0.36132436990737915\n",
      "epoch 110: loss 0.36554640531539917\n",
      "epoch 111: loss 0.39476531744003296\n",
      "epoch 112: loss 0.4098711609840393\n",
      "epoch 113: loss 0.3139065206050873\n",
      "epoch 114: loss 0.18302473425865173\n",
      "epoch 115: loss 0.40894824266433716\n",
      "epoch 116: loss 0.30222633481025696\n",
      "epoch 117: loss 0.31267139315605164\n",
      "epoch 118: loss 0.32684531807899475\n",
      "epoch 119: loss 0.37159132957458496\n",
      "epoch 120: loss 0.28015291690826416\n",
      "epoch 121: loss 0.39522603154182434\n",
      "epoch 122: loss 0.2670268714427948\n",
      "epoch 123: loss 0.4084954857826233\n",
      "epoch 124: loss 0.31228184700012207\n",
      "epoch 125: loss 0.31602296233177185\n",
      "epoch 126: loss 0.3601606488227844\n",
      "epoch 127: loss 0.3971424102783203\n",
      "epoch 128: loss 0.27590417861938477\n",
      "epoch 129: loss 0.3455279469490051\n",
      "epoch 130: loss 0.3662455081939697\n",
      "epoch 131: loss 0.3354797065258026\n",
      "epoch 132: loss 0.32031846046447754\n",
      "epoch 133: loss 0.3412722051143646\n",
      "epoch 134: loss 0.3242320418357849\n",
      "epoch 135: loss 0.2625635862350464\n",
      "epoch 136: loss 0.23900951445102692\n",
      "epoch 0: loss 0.3607909381389618\n",
      "epoch 1: loss 0.3334592580795288\n",
      "epoch 2: loss 0.4084165692329407\n",
      "epoch 3: loss 0.36739563941955566\n",
      "epoch 4: loss 0.3255842924118042\n",
      "epoch 5: loss 0.2613071799278259\n",
      "epoch 6: loss 0.2833353877067566\n",
      "epoch 7: loss 0.22399528324604034\n",
      "epoch 8: loss 0.42244988679885864\n",
      "epoch 9: loss 0.4321155846118927\n",
      "epoch 10: loss 0.33497220277786255\n",
      "epoch 11: loss 0.26657795906066895\n",
      "epoch 12: loss 0.33865371346473694\n",
      "epoch 13: loss 0.41986310482025146\n",
      "epoch 14: loss 0.3469986021518707\n",
      "epoch 15: loss 0.27919089794158936\n",
      "epoch 16: loss 0.34553369879722595\n",
      "epoch 17: loss 0.3357471227645874\n",
      "epoch 18: loss 0.3883323669433594\n",
      "epoch 19: loss 0.2584744989871979\n",
      "epoch 20: loss 0.2918281555175781\n",
      "epoch 21: loss 0.33341002464294434\n",
      "epoch 22: loss 0.31176644563674927\n",
      "epoch 23: loss 0.3627564311027527\n",
      "epoch 24: loss 0.344340980052948\n",
      "epoch 25: loss 0.39582550525665283\n",
      "epoch 26: loss 0.324107825756073\n",
      "epoch 27: loss 0.34208908677101135\n",
      "epoch 28: loss 0.4058865010738373\n",
      "epoch 29: loss 0.3156067132949829\n",
      "epoch 30: loss 0.27973490953445435\n",
      "epoch 31: loss 0.3337741196155548\n",
      "epoch 32: loss 0.34005972743034363\n",
      "epoch 33: loss 0.3429177403450012\n",
      "epoch 34: loss 0.38837653398513794\n",
      "epoch 35: loss 0.25955116748809814\n",
      "epoch 36: loss 0.3259814381599426\n",
      "epoch 37: loss 0.3177924156188965\n",
      "epoch 38: loss 0.22108842432498932\n",
      "epoch 39: loss 0.31887286901474\n",
      "epoch 40: loss 0.31238850951194763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 41: loss 0.28222233057022095\n",
      "epoch 42: loss 0.2630782127380371\n",
      "epoch 43: loss 0.32374608516693115\n",
      "epoch 44: loss 0.34066933393478394\n",
      "epoch 45: loss 0.2777637541294098\n",
      "epoch 46: loss 0.2916315793991089\n",
      "epoch 47: loss 0.27922165393829346\n",
      "epoch 48: loss 0.2527204751968384\n",
      "epoch 49: loss 0.3355211615562439\n",
      "epoch 50: loss 0.2881992757320404\n",
      "epoch 51: loss 0.42687079310417175\n",
      "epoch 52: loss 0.297009140253067\n",
      "epoch 53: loss 0.368549644947052\n",
      "epoch 54: loss 0.3486844301223755\n",
      "epoch 55: loss 0.29996711015701294\n",
      "epoch 56: loss 0.354648619890213\n",
      "epoch 57: loss 0.25429970026016235\n",
      "epoch 58: loss 0.32721957564353943\n",
      "epoch 59: loss 0.25539278984069824\n",
      "epoch 60: loss 0.3375166654586792\n",
      "epoch 61: loss 0.22566655278205872\n",
      "epoch 62: loss 0.24189971387386322\n",
      "epoch 63: loss 0.3693583905696869\n",
      "epoch 64: loss 0.29864415526390076\n",
      "epoch 65: loss 0.4014045298099518\n",
      "epoch 66: loss 0.33382850885391235\n",
      "epoch 67: loss 0.37618860602378845\n",
      "epoch 68: loss 0.37392497062683105\n",
      "epoch 69: loss 0.4172070026397705\n",
      "epoch 70: loss 0.38225656747817993\n",
      "epoch 71: loss 0.30808448791503906\n",
      "epoch 72: loss 0.26003336906433105\n",
      "epoch 73: loss 0.33454859256744385\n",
      "epoch 74: loss 0.2783118486404419\n",
      "epoch 75: loss 0.3345051109790802\n",
      "epoch 76: loss 0.30769291520118713\n",
      "epoch 77: loss 0.39724239706993103\n",
      "epoch 78: loss 0.3104472756385803\n",
      "epoch 79: loss 0.3274894952774048\n",
      "epoch 80: loss 0.3821682929992676\n",
      "epoch 81: loss 0.2657720446586609\n",
      "epoch 82: loss 0.3282793164253235\n",
      "epoch 83: loss 0.3747190237045288\n",
      "epoch 84: loss 0.29773077368736267\n",
      "epoch 85: loss 0.30595844984054565\n",
      "epoch 86: loss 0.3428398370742798\n",
      "epoch 87: loss 0.3435973823070526\n",
      "epoch 88: loss 0.28211820125579834\n",
      "epoch 89: loss 0.3766316771507263\n",
      "epoch 90: loss 0.302047461271286\n",
      "epoch 91: loss 0.4022185802459717\n",
      "epoch 92: loss 0.32077378034591675\n",
      "epoch 93: loss 0.3270662724971771\n",
      "epoch 94: loss 0.3632083535194397\n",
      "epoch 95: loss 0.2528623938560486\n",
      "epoch 96: loss 0.23398235440254211\n",
      "epoch 97: loss 0.3029170036315918\n",
      "epoch 98: loss 0.34559983015060425\n",
      "epoch 99: loss 0.3657691776752472\n",
      "epoch 100: loss 0.5185344219207764\n",
      "epoch 101: loss 0.31039589643478394\n",
      "epoch 102: loss 0.26719993352890015\n",
      "epoch 103: loss 0.30426037311553955\n",
      "epoch 104: loss 0.2758845090866089\n",
      "epoch 105: loss 0.3709924519062042\n",
      "epoch 106: loss 0.34389135241508484\n",
      "epoch 107: loss 0.28412503004074097\n",
      "epoch 108: loss 0.34990766644477844\n",
      "epoch 109: loss 0.3590504229068756\n",
      "epoch 110: loss 0.36366161704063416\n",
      "epoch 111: loss 0.39453575015068054\n",
      "epoch 112: loss 0.4071740508079529\n",
      "epoch 113: loss 0.31296586990356445\n",
      "epoch 114: loss 0.18393033742904663\n",
      "epoch 115: loss 0.41031357645988464\n",
      "epoch 116: loss 0.3055787682533264\n",
      "epoch 117: loss 0.3086662292480469\n",
      "epoch 118: loss 0.3280537724494934\n",
      "epoch 119: loss 0.3716745972633362\n",
      "epoch 120: loss 0.2794783413410187\n",
      "epoch 121: loss 0.3982604146003723\n",
      "epoch 122: loss 0.26741135120391846\n",
      "epoch 123: loss 0.40702545642852783\n",
      "epoch 124: loss 0.3106738030910492\n",
      "epoch 125: loss 0.3175530731678009\n",
      "epoch 126: loss 0.3610718846321106\n",
      "epoch 127: loss 0.39596641063690186\n",
      "epoch 128: loss 0.27564537525177\n",
      "epoch 129: loss 0.34837275743484497\n",
      "epoch 130: loss 0.36555808782577515\n",
      "epoch 131: loss 0.33409011363983154\n",
      "epoch 132: loss 0.31833213567733765\n",
      "epoch 133: loss 0.34233587980270386\n",
      "epoch 134: loss 0.321841835975647\n",
      "epoch 135: loss 0.25951308012008667\n",
      "epoch 136: loss 0.23912055790424347\n",
      "epoch 0: loss 0.36796095967292786\n",
      "epoch 1: loss 0.3332665264606476\n",
      "epoch 2: loss 0.4134798049926758\n",
      "epoch 3: loss 0.36579519510269165\n",
      "epoch 4: loss 0.3260837495326996\n",
      "epoch 5: loss 0.2685781419277191\n",
      "epoch 6: loss 0.2827538847923279\n",
      "epoch 7: loss 0.22657927870750427\n",
      "epoch 8: loss 0.4231158494949341\n",
      "epoch 9: loss 0.44032159447669983\n",
      "epoch 10: loss 0.3444635272026062\n",
      "epoch 11: loss 0.27397269010543823\n",
      "epoch 12: loss 0.32102423906326294\n",
      "epoch 13: loss 0.41552478075027466\n",
      "epoch 14: loss 0.3591107130050659\n",
      "epoch 15: loss 0.27977532148361206\n",
      "epoch 16: loss 0.3425026535987854\n",
      "epoch 17: loss 0.3481411635875702\n",
      "epoch 18: loss 0.4081569314002991\n",
      "epoch 19: loss 0.26237064599990845\n",
      "epoch 20: loss 0.29842913150787354\n",
      "epoch 21: loss 0.3313778042793274\n",
      "epoch 22: loss 0.31127119064331055\n",
      "epoch 23: loss 0.3648681044578552\n",
      "epoch 24: loss 0.3507179021835327\n",
      "epoch 25: loss 0.39104539155960083\n",
      "epoch 26: loss 0.3110913336277008\n",
      "epoch 27: loss 0.3474659323692322\n",
      "epoch 28: loss 0.4053070545196533\n",
      "epoch 29: loss 0.3169407546520233\n",
      "epoch 30: loss 0.2792993485927582\n",
      "epoch 31: loss 0.3342243432998657\n",
      "epoch 32: loss 0.33663028478622437\n",
      "epoch 33: loss 0.34395188093185425\n",
      "epoch 34: loss 0.38618743419647217\n",
      "epoch 35: loss 0.2603301703929901\n",
      "epoch 36: loss 0.324762761592865\n",
      "epoch 37: loss 0.31984400749206543\n",
      "epoch 38: loss 0.22368361055850983\n",
      "epoch 39: loss 0.3210959732532501\n",
      "epoch 40: loss 0.3151625692844391\n",
      "epoch 41: loss 0.28041982650756836\n",
      "epoch 42: loss 0.25773924589157104\n",
      "epoch 43: loss 0.32398611307144165\n",
      "epoch 44: loss 0.34115713834762573\n",
      "epoch 45: loss 0.2750735580921173\n",
      "epoch 46: loss 0.2930217385292053\n",
      "epoch 47: loss 0.27972501516342163\n",
      "epoch 48: loss 0.2513640224933624\n",
      "epoch 49: loss 0.33418142795562744\n",
      "epoch 50: loss 0.28939157724380493\n",
      "epoch 51: loss 0.43381932377815247\n",
      "epoch 52: loss 0.3042391240596771\n",
      "epoch 53: loss 0.36953461170196533\n",
      "epoch 54: loss 0.34811264276504517\n",
      "epoch 55: loss 0.2984406352043152\n",
      "epoch 56: loss 0.35448893904685974\n",
      "epoch 57: loss 0.2573956847190857\n",
      "epoch 58: loss 0.3249441683292389\n",
      "epoch 59: loss 0.25689220428466797\n",
      "epoch 60: loss 0.3345155119895935\n",
      "epoch 61: loss 0.22615808248519897\n",
      "epoch 62: loss 0.24140411615371704\n",
      "epoch 63: loss 0.3743961453437805\n",
      "epoch 64: loss 0.3106424808502197\n",
      "epoch 65: loss 0.410438597202301\n",
      "epoch 66: loss 0.32885003089904785\n",
      "epoch 67: loss 0.37008947134017944\n",
      "epoch 68: loss 0.3719608187675476\n",
      "epoch 69: loss 0.40708938241004944\n",
      "epoch 70: loss 0.38188791275024414\n",
      "epoch 71: loss 0.3199498653411865\n",
      "epoch 72: loss 0.25943103432655334\n",
      "epoch 73: loss 0.34153443574905396\n",
      "epoch 74: loss 0.29136425256729126\n",
      "epoch 75: loss 0.33562812209129333\n",
      "epoch 76: loss 0.3003673553466797\n",
      "epoch 77: loss 0.4081662893295288\n",
      "epoch 78: loss 0.3054995536804199\n",
      "epoch 79: loss 0.340152382850647\n",
      "epoch 80: loss 0.3743678629398346\n",
      "epoch 81: loss 0.2738299071788788\n",
      "epoch 82: loss 0.33758795261383057\n",
      "epoch 83: loss 0.3794633746147156\n",
      "epoch 84: loss 0.25229012966156006\n",
      "epoch 85: loss 0.305292546749115\n",
      "epoch 86: loss 0.3366367220878601\n",
      "epoch 87: loss 0.3453820049762726\n",
      "epoch 88: loss 0.2639976739883423\n",
      "epoch 89: loss 0.33927738666534424\n",
      "epoch 90: loss 0.288577675819397\n",
      "epoch 91: loss 0.3369232416152954\n",
      "epoch 92: loss 0.27820873260498047\n",
      "epoch 93: loss 0.3165256381034851\n",
      "epoch 94: loss 0.3580869734287262\n",
      "epoch 95: loss 0.1990203857421875\n",
      "epoch 96: loss 0.21311235427856445\n",
      "epoch 97: loss 0.32556334137916565\n",
      "epoch 98: loss 0.37786152958869934\n",
      "epoch 99: loss 0.410666286945343\n",
      "epoch 100: loss 0.5610017776489258\n",
      "epoch 101: loss 0.31166499853134155\n",
      "epoch 102: loss 0.2687165141105652\n",
      "epoch 103: loss 0.30963870882987976\n",
      "epoch 104: loss 0.30241715908050537\n",
      "epoch 105: loss 0.38561975955963135\n",
      "epoch 106: loss 0.3358537554740906\n",
      "epoch 107: loss 0.2898024916648865\n",
      "epoch 108: loss 0.34592825174331665\n",
      "epoch 109: loss 0.3523944020271301\n",
      "epoch 110: loss 0.3548133671283722\n",
      "epoch 111: loss 0.38591644167900085\n",
      "epoch 112: loss 0.3882814943790436\n",
      "epoch 113: loss 0.2997140884399414\n",
      "epoch 114: loss 0.18381203711032867\n",
      "epoch 115: loss 0.40470266342163086\n",
      "epoch 116: loss 0.3071443438529968\n",
      "epoch 117: loss 0.29062697291374207\n",
      "epoch 118: loss 0.32951781153678894\n",
      "epoch 119: loss 0.3716464936733246\n",
      "epoch 120: loss 0.27226993441581726\n",
      "epoch 121: loss 0.41143685579299927\n",
      "epoch 122: loss 0.26426970958709717\n",
      "epoch 123: loss 0.400968074798584\n",
      "epoch 124: loss 0.3048861026763916\n",
      "epoch 125: loss 0.33416685461997986\n",
      "epoch 126: loss 0.3655368685722351\n",
      "epoch 127: loss 0.3757520318031311\n",
      "epoch 128: loss 0.2935260832309723\n",
      "epoch 129: loss 0.3485659062862396\n",
      "epoch 130: loss 0.37024810910224915\n",
      "epoch 131: loss 0.3175834119319916\n",
      "epoch 132: loss 0.32010021805763245\n",
      "epoch 133: loss 0.36610156297683716\n",
      "epoch 134: loss 0.31877636909484863\n",
      "epoch 135: loss 0.2594574987888336\n",
      "epoch 136: loss 0.24455386400222778\n",
      "epoch 0: loss 0.36608144640922546\n",
      "epoch 1: loss 0.3299371600151062\n",
      "epoch 2: loss 0.40172049403190613\n",
      "epoch 3: loss 0.37093600630760193\n",
      "epoch 4: loss 0.3401069641113281\n",
      "epoch 5: loss 0.26761630177497864\n",
      "epoch 6: loss 0.2879016399383545\n",
      "epoch 7: loss 0.2324773520231247\n",
      "epoch 8: loss 0.4294901490211487\n",
      "epoch 9: loss 0.4535672068595886\n",
      "epoch 10: loss 0.3510803282260895\n",
      "epoch 11: loss 0.2776455879211426\n",
      "epoch 12: loss 0.3218982219696045\n",
      "epoch 13: loss 0.4219587743282318\n",
      "epoch 14: loss 0.3627915382385254\n",
      "epoch 15: loss 0.28750428557395935\n",
      "epoch 16: loss 0.3372735381126404\n",
      "epoch 17: loss 0.36341771483421326\n",
      "epoch 18: loss 0.4164743721485138\n",
      "epoch 19: loss 0.2731778025627136\n",
      "epoch 20: loss 0.3101806640625\n",
      "epoch 21: loss 0.3368164896965027\n",
      "epoch 22: loss 0.3150726556777954\n",
      "epoch 23: loss 0.3832656741142273\n",
      "epoch 24: loss 0.3498498499393463\n",
      "epoch 25: loss 0.39402318000793457\n",
      "epoch 26: loss 0.3113287687301636\n",
      "epoch 27: loss 0.3580009937286377\n",
      "epoch 28: loss 0.4087100028991699\n",
      "epoch 29: loss 0.3174479007720947\n",
      "epoch 30: loss 0.2848742604255676\n",
      "epoch 31: loss 0.33401554822921753\n",
      "epoch 32: loss 0.3375028073787689\n",
      "epoch 33: loss 0.3443993926048279\n",
      "epoch 34: loss 0.3880445957183838\n",
      "epoch 35: loss 0.2625219225883484\n",
      "epoch 36: loss 0.3273773789405823\n",
      "epoch 37: loss 0.31674057245254517\n",
      "epoch 38: loss 0.22049398720264435\n",
      "epoch 39: loss 0.3195682168006897\n",
      "epoch 40: loss 0.3125224709510803\n",
      "epoch 41: loss 0.27833232283592224\n",
      "epoch 42: loss 0.2586641013622284\n",
      "epoch 43: loss 0.3235645294189453\n",
      "epoch 44: loss 0.3430318236351013\n",
      "epoch 45: loss 0.2759209871292114\n",
      "epoch 46: loss 0.2895911931991577\n",
      "epoch 47: loss 0.27827733755111694\n",
      "epoch 48: loss 0.25181567668914795\n",
      "epoch 49: loss 0.33579298853874207\n",
      "epoch 50: loss 0.29040002822875977\n",
      "epoch 51: loss 0.4333338439464569\n",
      "epoch 52: loss 0.30192118883132935\n",
      "epoch 53: loss 0.3713224530220032\n",
      "epoch 54: loss 0.350726842880249\n",
      "epoch 55: loss 0.29836004972457886\n",
      "epoch 56: loss 0.3535146713256836\n",
      "epoch 57: loss 0.2576577067375183\n",
      "epoch 58: loss 0.3275452256202698\n",
      "epoch 59: loss 0.25695934891700745\n",
      "epoch 60: loss 0.33449772000312805\n",
      "epoch 61: loss 0.22555437684059143\n",
      "epoch 62: loss 0.24153831601142883\n",
      "epoch 63: loss 0.37002092599868774\n",
      "epoch 64: loss 0.3028322160243988\n",
      "epoch 65: loss 0.40456175804138184\n",
      "epoch 66: loss 0.330864280462265\n",
      "epoch 67: loss 0.37298694252967834\n",
      "epoch 68: loss 0.37705981731414795\n",
      "epoch 69: loss 0.41102731227874756\n",
      "epoch 70: loss 0.38098636269569397\n",
      "epoch 71: loss 0.31393179297447205\n",
      "epoch 72: loss 0.25877273082733154\n",
      "epoch 73: loss 0.33891046047210693\n",
      "epoch 74: loss 0.28398776054382324\n",
      "epoch 75: loss 0.33745628595352173\n",
      "epoch 76: loss 0.30162879824638367\n",
      "epoch 77: loss 0.4060051143169403\n",
      "epoch 78: loss 0.30584120750427246\n",
      "epoch 79: loss 0.3443593978881836\n",
      "epoch 80: loss 0.37812554836273193\n",
      "epoch 81: loss 0.26969751715660095\n",
      "epoch 82: loss 0.3348861038684845\n",
      "epoch 83: loss 0.3785632252693176\n",
      "epoch 84: loss 0.24973802268505096\n",
      "epoch 85: loss 0.30613619089126587\n",
      "epoch 86: loss 0.33698251843452454\n",
      "epoch 87: loss 0.34636542201042175\n",
      "epoch 88: loss 0.26482874155044556\n",
      "epoch 89: loss 0.34288346767425537\n",
      "epoch 90: loss 0.2895190119743347\n",
      "epoch 91: loss 0.3397563099861145\n",
      "epoch 92: loss 0.2807116210460663\n",
      "epoch 93: loss 0.31578531861305237\n",
      "epoch 94: loss 0.3563959300518036\n",
      "epoch 95: loss 0.20213395357131958\n",
      "epoch 96: loss 0.21285149455070496\n",
      "epoch 97: loss 0.32488206028938293\n",
      "epoch 98: loss 0.37295663356781006\n",
      "epoch 99: loss 0.401202917098999\n",
      "epoch 100: loss 0.5566656589508057\n",
      "epoch 101: loss 0.31659823656082153\n",
      "epoch 102: loss 0.27253758907318115\n",
      "epoch 103: loss 0.3053378164768219\n",
      "epoch 104: loss 0.29420316219329834\n",
      "epoch 105: loss 0.38524097204208374\n",
      "epoch 106: loss 0.34654825925827026\n",
      "epoch 107: loss 0.29218876361846924\n",
      "epoch 108: loss 0.34720608592033386\n",
      "epoch 109: loss 0.35440778732299805\n",
      "epoch 110: loss 0.35437652468681335\n",
      "epoch 111: loss 0.386355996131897\n",
      "epoch 112: loss 0.40589189529418945\n",
      "epoch 113: loss 0.3183654546737671\n",
      "epoch 114: loss 0.19276738166809082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 115: loss 0.4302626848220825\n",
      "epoch 116: loss 0.3054974377155304\n",
      "epoch 117: loss 0.27256351709365845\n",
      "epoch 118: loss 0.3328639268875122\n",
      "epoch 119: loss 0.3680138885974884\n",
      "epoch 120: loss 0.28211045265197754\n",
      "epoch 121: loss 0.38130560517311096\n",
      "epoch 122: loss 0.2610894739627838\n",
      "epoch 123: loss 0.3931502103805542\n",
      "epoch 124: loss 0.3079255521297455\n",
      "epoch 125: loss 0.33700183033943176\n",
      "epoch 126: loss 0.36277762055397034\n",
      "epoch 127: loss 0.37711024284362793\n",
      "epoch 128: loss 0.28943759202957153\n",
      "epoch 129: loss 0.34801730513572693\n",
      "epoch 130: loss 0.3695262670516968\n",
      "epoch 131: loss 0.3151152729988098\n",
      "epoch 132: loss 0.31315648555755615\n",
      "epoch 133: loss 0.3549252152442932\n",
      "epoch 134: loss 0.3224891126155853\n",
      "epoch 135: loss 0.26696667075157166\n",
      "epoch 136: loss 0.24544405937194824\n",
      "epoch 0: loss 0.36566999554634094\n",
      "epoch 1: loss 0.339180588722229\n",
      "epoch 2: loss 0.4361017346382141\n",
      "epoch 3: loss 0.3684803545475006\n",
      "epoch 4: loss 0.3388885259628296\n",
      "epoch 5: loss 0.2795562148094177\n",
      "epoch 6: loss 0.2860097885131836\n",
      "epoch 7: loss 0.23258402943611145\n",
      "epoch 8: loss 0.4476584196090698\n",
      "epoch 9: loss 0.46330398321151733\n",
      "epoch 10: loss 0.3508465886116028\n",
      "epoch 11: loss 0.2766401767730713\n",
      "epoch 12: loss 0.3275357484817505\n",
      "epoch 13: loss 0.4305832087993622\n",
      "epoch 14: loss 0.36243051290512085\n",
      "epoch 15: loss 0.29468297958374023\n",
      "epoch 16: loss 0.3336896300315857\n",
      "epoch 17: loss 0.3590404689311981\n",
      "epoch 18: loss 0.41751575469970703\n",
      "epoch 19: loss 0.2728712260723114\n",
      "epoch 20: loss 0.3121691346168518\n",
      "epoch 21: loss 0.3408123552799225\n",
      "epoch 22: loss 0.32039251923561096\n",
      "epoch 23: loss 0.38048043847084045\n",
      "epoch 24: loss 0.3790411353111267\n",
      "epoch 25: loss 0.399611234664917\n",
      "epoch 26: loss 0.3121178150177002\n",
      "epoch 27: loss 0.3682986795902252\n",
      "epoch 28: loss 0.4241102933883667\n",
      "epoch 29: loss 0.31919118762016296\n",
      "epoch 30: loss 0.29511120915412903\n",
      "epoch 31: loss 0.3325440287590027\n",
      "epoch 32: loss 0.34042829275131226\n",
      "epoch 33: loss 0.3452572226524353\n",
      "epoch 34: loss 0.38701558113098145\n",
      "epoch 35: loss 0.27182602882385254\n",
      "epoch 36: loss 0.3414877951145172\n",
      "epoch 37: loss 0.3086290955543518\n",
      "epoch 38: loss 0.2195008248090744\n",
      "epoch 39: loss 0.3109256625175476\n",
      "epoch 40: loss 0.30869534611701965\n",
      "epoch 41: loss 0.28020763397216797\n",
      "epoch 42: loss 0.2606164216995239\n",
      "epoch 43: loss 0.3151445984840393\n",
      "epoch 44: loss 0.3479458689689636\n",
      "epoch 45: loss 0.27980825304985046\n",
      "epoch 46: loss 0.28766053915023804\n",
      "epoch 47: loss 0.27758437395095825\n",
      "epoch 48: loss 0.2582733631134033\n",
      "epoch 49: loss 0.3400549292564392\n",
      "epoch 50: loss 0.29345864057540894\n",
      "epoch 51: loss 0.44436705112457275\n",
      "epoch 52: loss 0.32039254903793335\n",
      "epoch 53: loss 0.37627944350242615\n",
      "epoch 54: loss 0.3491729497909546\n",
      "epoch 55: loss 0.3016934394836426\n",
      "epoch 56: loss 0.34942254424095154\n",
      "epoch 57: loss 0.26061299443244934\n",
      "epoch 58: loss 0.33197498321533203\n",
      "epoch 59: loss 0.266384094953537\n",
      "epoch 60: loss 0.33937573432922363\n",
      "epoch 61: loss 0.22531607747077942\n",
      "epoch 62: loss 0.24248157441616058\n",
      "epoch 63: loss 0.35253211855888367\n",
      "epoch 64: loss 0.29376310110092163\n",
      "epoch 65: loss 0.3997095823287964\n",
      "epoch 66: loss 0.3312290906906128\n",
      "epoch 67: loss 0.3700220584869385\n",
      "epoch 68: loss 0.36307981610298157\n",
      "epoch 69: loss 0.41078853607177734\n",
      "epoch 70: loss 0.3780173361301422\n",
      "epoch 71: loss 0.32435643672943115\n",
      "epoch 72: loss 0.25889840722084045\n",
      "epoch 73: loss 0.338442325592041\n",
      "epoch 74: loss 0.28411638736724854\n",
      "epoch 75: loss 0.3390922546386719\n",
      "epoch 76: loss 0.3033539652824402\n",
      "epoch 77: loss 0.40402811765670776\n",
      "epoch 78: loss 0.30850622057914734\n",
      "epoch 79: loss 0.3360852897167206\n",
      "epoch 80: loss 0.37882891297340393\n",
      "epoch 81: loss 0.26593464612960815\n",
      "epoch 82: loss 0.32489022612571716\n",
      "epoch 83: loss 0.3860554099082947\n",
      "epoch 84: loss 0.25455278158187866\n",
      "epoch 85: loss 0.3081218898296356\n",
      "epoch 86: loss 0.3394404649734497\n",
      "epoch 87: loss 0.34419485926628113\n",
      "epoch 88: loss 0.2702639698982239\n",
      "epoch 89: loss 0.36106082797050476\n",
      "epoch 90: loss 0.29377245903015137\n",
      "epoch 91: loss 0.37570884823799133\n",
      "epoch 92: loss 0.3098467290401459\n",
      "epoch 93: loss 0.316506952047348\n",
      "epoch 94: loss 0.33798831701278687\n",
      "epoch 95: loss 0.23830652236938477\n",
      "epoch 96: loss 0.22045272588729858\n",
      "epoch 97: loss 0.3040524125099182\n",
      "epoch 98: loss 0.34601640701293945\n",
      "epoch 99: loss 0.3670332431793213\n",
      "epoch 100: loss 0.5125997066497803\n",
      "epoch 101: loss 0.30783358216285706\n",
      "epoch 102: loss 0.2655389606952667\n",
      "epoch 103: loss 0.30374467372894287\n",
      "epoch 104: loss 0.2734891474246979\n",
      "epoch 105: loss 0.37131980061531067\n",
      "epoch 106: loss 0.32935965061187744\n",
      "epoch 107: loss 0.2914004623889923\n",
      "epoch 108: loss 0.34724071621894836\n",
      "epoch 109: loss 0.354594886302948\n",
      "epoch 110: loss 0.35798969864845276\n",
      "epoch 111: loss 0.3896714448928833\n",
      "epoch 112: loss 0.39730149507522583\n",
      "epoch 113: loss 0.30874955654144287\n",
      "epoch 114: loss 0.18056851625442505\n",
      "epoch 115: loss 0.41487425565719604\n",
      "epoch 116: loss 0.30516451597213745\n",
      "epoch 117: loss 0.3005905747413635\n",
      "epoch 118: loss 0.32777613401412964\n",
      "epoch 119: loss 0.37421199679374695\n",
      "epoch 120: loss 0.2705281376838684\n",
      "epoch 121: loss 0.3876532316207886\n",
      "epoch 122: loss 0.25908419489860535\n",
      "epoch 123: loss 0.3987983763217926\n",
      "epoch 124: loss 0.29904210567474365\n",
      "epoch 125: loss 0.3303515315055847\n",
      "epoch 126: loss 0.35001903772354126\n",
      "epoch 127: loss 0.38050806522369385\n",
      "epoch 128: loss 0.2933015823364258\n",
      "epoch 129: loss 0.34647834300994873\n",
      "epoch 130: loss 0.3856573700904846\n",
      "epoch 131: loss 0.32559627294540405\n",
      "epoch 132: loss 0.3346216380596161\n",
      "epoch 133: loss 0.37668657302856445\n",
      "epoch 134: loss 0.31596773862838745\n",
      "epoch 135: loss 0.2618124186992645\n",
      "epoch 136: loss 0.2415456622838974\n",
      "epoch 0: loss 0.3699079751968384\n",
      "epoch 1: loss 0.3347226083278656\n",
      "epoch 2: loss 0.4010698199272156\n",
      "epoch 3: loss 0.36915120482444763\n",
      "epoch 4: loss 0.34972885251045227\n",
      "epoch 5: loss 0.267741858959198\n",
      "epoch 6: loss 0.2901267409324646\n",
      "epoch 7: loss 0.22873109579086304\n",
      "epoch 8: loss 0.4163307547569275\n",
      "epoch 9: loss 0.4451558589935303\n",
      "epoch 10: loss 0.3589783310890198\n",
      "epoch 11: loss 0.288433313369751\n",
      "epoch 12: loss 0.30123311281204224\n",
      "epoch 13: loss 0.421958327293396\n",
      "epoch 14: loss 0.39412474632263184\n",
      "epoch 15: loss 0.28552690148353577\n",
      "epoch 16: loss 0.34975868463516235\n",
      "epoch 17: loss 0.3411538600921631\n",
      "epoch 18: loss 0.400421142578125\n",
      "epoch 19: loss 0.2718300521373749\n",
      "epoch 20: loss 0.3156243562698364\n",
      "epoch 21: loss 0.34450849890708923\n",
      "epoch 22: loss 0.3215610384941101\n",
      "epoch 23: loss 0.3697316646575928\n",
      "epoch 24: loss 0.34466177225112915\n",
      "epoch 25: loss 0.41833579540252686\n",
      "epoch 26: loss 0.3236560523509979\n",
      "epoch 27: loss 0.3429039418697357\n",
      "epoch 28: loss 0.4106234908103943\n",
      "epoch 29: loss 0.3167455196380615\n",
      "epoch 30: loss 0.29965275526046753\n",
      "epoch 31: loss 0.3374173641204834\n",
      "epoch 32: loss 0.3428610563278198\n",
      "epoch 33: loss 0.34084081649780273\n",
      "epoch 34: loss 0.38452160358428955\n",
      "epoch 35: loss 0.2616245448589325\n",
      "epoch 36: loss 0.3394015431404114\n",
      "epoch 37: loss 0.3078010082244873\n",
      "epoch 38: loss 0.2248375564813614\n",
      "epoch 39: loss 0.30151647329330444\n",
      "epoch 40: loss 0.30008718371391296\n",
      "epoch 41: loss 0.27869912981987\n",
      "epoch 42: loss 0.25820744037628174\n",
      "epoch 43: loss 0.31966978311538696\n",
      "epoch 44: loss 0.3428736925125122\n",
      "epoch 45: loss 0.2799771726131439\n",
      "epoch 46: loss 0.28934481739997864\n",
      "epoch 47: loss 0.278672993183136\n",
      "epoch 48: loss 0.25494951009750366\n",
      "epoch 49: loss 0.3410039246082306\n",
      "epoch 50: loss 0.2836449146270752\n",
      "epoch 51: loss 0.4197854995727539\n",
      "epoch 52: loss 0.2884519100189209\n",
      "epoch 53: loss 0.37066081166267395\n",
      "epoch 54: loss 0.3483002781867981\n",
      "epoch 55: loss 0.29669278860092163\n",
      "epoch 56: loss 0.3538113236427307\n",
      "epoch 57: loss 0.2512384057044983\n",
      "epoch 58: loss 0.32617902755737305\n",
      "epoch 59: loss 0.25808823108673096\n",
      "epoch 60: loss 0.3343454599380493\n",
      "epoch 61: loss 0.22584009170532227\n",
      "epoch 62: loss 0.2427549660205841\n",
      "epoch 63: loss 0.36565518379211426\n",
      "epoch 64: loss 0.2984410524368286\n",
      "epoch 65: loss 0.40463319420814514\n",
      "epoch 66: loss 0.3296148180961609\n",
      "epoch 67: loss 0.3709229826927185\n",
      "epoch 68: loss 0.3687143623828888\n",
      "epoch 69: loss 0.40590211749076843\n",
      "epoch 70: loss 0.38035279512405396\n",
      "epoch 71: loss 0.31975287199020386\n",
      "epoch 72: loss 0.25662750005722046\n",
      "epoch 73: loss 0.3380718529224396\n",
      "epoch 74: loss 0.28743621706962585\n",
      "epoch 75: loss 0.3384852409362793\n",
      "epoch 76: loss 0.3010103106498718\n",
      "epoch 77: loss 0.41034889221191406\n",
      "epoch 78: loss 0.3050655126571655\n",
      "epoch 79: loss 0.34593021869659424\n",
      "epoch 80: loss 0.37489262223243713\n",
      "epoch 81: loss 0.27094388008117676\n",
      "epoch 82: loss 0.3530542850494385\n",
      "epoch 83: loss 0.37510597705841064\n",
      "epoch 84: loss 0.24427439272403717\n",
      "epoch 85: loss 0.30909931659698486\n",
      "epoch 86: loss 0.3396424353122711\n",
      "epoch 87: loss 0.3465093672275543\n",
      "epoch 88: loss 0.26439058780670166\n",
      "epoch 89: loss 0.3313298225402832\n",
      "epoch 90: loss 0.2875039577484131\n",
      "epoch 91: loss 0.32976293563842773\n",
      "epoch 92: loss 0.27133846282958984\n",
      "epoch 93: loss 0.3177269697189331\n",
      "epoch 94: loss 0.33162564039230347\n",
      "epoch 95: loss 0.20552775263786316\n",
      "epoch 96: loss 0.20970481634140015\n",
      "epoch 97: loss 0.3120567798614502\n",
      "epoch 98: loss 0.3555401563644409\n",
      "epoch 99: loss 0.37774503231048584\n",
      "epoch 100: loss 0.5126705169677734\n",
      "epoch 101: loss 0.30753716826438904\n",
      "epoch 102: loss 0.2632846236228943\n",
      "epoch 103: loss 0.2985081672668457\n",
      "epoch 104: loss 0.2762404680252075\n",
      "epoch 105: loss 0.3756372928619385\n",
      "epoch 106: loss 0.32361072301864624\n",
      "epoch 107: loss 0.28127381205558777\n",
      "epoch 108: loss 0.3458113670349121\n",
      "epoch 109: loss 0.35273033380508423\n",
      "epoch 110: loss 0.3535343408584595\n",
      "epoch 111: loss 0.379873663187027\n",
      "epoch 112: loss 0.3820146322250366\n",
      "epoch 113: loss 0.2941878139972687\n",
      "epoch 114: loss 0.18022659420967102\n",
      "epoch 115: loss 0.40326574444770813\n",
      "epoch 116: loss 0.3093402087688446\n",
      "epoch 117: loss 0.28652316331863403\n",
      "epoch 118: loss 0.32822227478027344\n",
      "epoch 119: loss 0.375864714384079\n",
      "epoch 120: loss 0.27217918634414673\n",
      "epoch 121: loss 0.40555477142333984\n",
      "epoch 122: loss 0.26526081562042236\n",
      "epoch 123: loss 0.4016515016555786\n",
      "epoch 124: loss 0.3007435202598572\n",
      "epoch 125: loss 0.33408790826797485\n",
      "epoch 126: loss 0.3572371304035187\n",
      "epoch 127: loss 0.37530362606048584\n",
      "epoch 128: loss 0.2979716658592224\n",
      "epoch 129: loss 0.3467234671115875\n",
      "epoch 130: loss 0.3769841194152832\n",
      "epoch 131: loss 0.32167333364486694\n",
      "epoch 132: loss 0.32927069067955017\n",
      "epoch 133: loss 0.37260469794273376\n",
      "epoch 134: loss 0.3144405484199524\n",
      "epoch 135: loss 0.2649693489074707\n",
      "epoch 136: loss 0.24760617315769196\n",
      "epoch 0: loss 0.3770558834075928\n",
      "epoch 1: loss 0.33317863941192627\n",
      "epoch 2: loss 0.40940386056900024\n",
      "epoch 3: loss 0.36297428607940674\n",
      "epoch 4: loss 0.31854596734046936\n",
      "epoch 5: loss 0.2710897922515869\n",
      "epoch 6: loss 0.28801608085632324\n",
      "epoch 7: loss 0.22948655486106873\n",
      "epoch 8: loss 0.4042496681213379\n",
      "epoch 9: loss 0.40495991706848145\n",
      "epoch 10: loss 0.3300212025642395\n",
      "epoch 11: loss 0.2716676592826843\n",
      "epoch 12: loss 0.30229270458221436\n",
      "epoch 13: loss 0.41571980714797974\n",
      "epoch 14: loss 0.3949419856071472\n",
      "epoch 15: loss 0.2773832082748413\n",
      "epoch 16: loss 0.34160035848617554\n",
      "epoch 17: loss 0.3687838912010193\n",
      "epoch 18: loss 0.4358985424041748\n",
      "epoch 19: loss 0.28305405378341675\n",
      "epoch 20: loss 0.3174881339073181\n",
      "epoch 21: loss 0.34238123893737793\n",
      "epoch 22: loss 0.3152376413345337\n",
      "epoch 23: loss 0.37065961956977844\n",
      "epoch 24: loss 0.3924449384212494\n",
      "epoch 25: loss 0.3887929916381836\n",
      "epoch 26: loss 0.31610777974128723\n",
      "epoch 27: loss 0.3807128071784973\n",
      "epoch 28: loss 0.4564957022666931\n",
      "epoch 29: loss 0.32603052258491516\n",
      "epoch 30: loss 0.33232471346855164\n",
      "epoch 31: loss 0.3499581217765808\n",
      "epoch 32: loss 0.3402246832847595\n",
      "epoch 33: loss 0.350485622882843\n",
      "epoch 34: loss 0.3897683620452881\n",
      "epoch 35: loss 0.27989858388900757\n",
      "epoch 36: loss 0.3564395606517792\n",
      "epoch 37: loss 0.3104133903980255\n",
      "epoch 38: loss 0.23165488243103027\n",
      "epoch 39: loss 0.2998889982700348\n",
      "epoch 40: loss 0.30294519662857056\n",
      "epoch 41: loss 0.280117005109787\n",
      "epoch 42: loss 0.2523539662361145\n",
      "epoch 43: loss 0.3149401545524597\n",
      "epoch 44: loss 0.35092538595199585\n",
      "epoch 45: loss 0.27813637256622314\n",
      "epoch 46: loss 0.2879365086555481\n",
      "epoch 47: loss 0.27666130661964417\n",
      "epoch 48: loss 0.2661132216453552\n",
      "epoch 49: loss 0.3468809723854065\n",
      "epoch 50: loss 0.2938801348209381\n",
      "epoch 51: loss 0.44560739398002625\n",
      "epoch 52: loss 0.30934271216392517\n",
      "epoch 53: loss 0.37757402658462524\n",
      "epoch 54: loss 0.35007214546203613\n",
      "epoch 55: loss 0.29899343848228455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 56: loss 0.3538740277290344\n",
      "epoch 57: loss 0.25976675748825073\n",
      "epoch 58: loss 0.32798922061920166\n",
      "epoch 59: loss 0.26173561811447144\n",
      "epoch 60: loss 0.33326804637908936\n",
      "epoch 61: loss 0.225686714053154\n",
      "epoch 62: loss 0.24215158820152283\n",
      "epoch 63: loss 0.3642202913761139\n",
      "epoch 64: loss 0.30308306217193604\n",
      "epoch 65: loss 0.4056016802787781\n",
      "epoch 66: loss 0.32846784591674805\n",
      "epoch 67: loss 0.37003329396247864\n",
      "epoch 68: loss 0.36372631788253784\n",
      "epoch 69: loss 0.4062636196613312\n",
      "epoch 70: loss 0.38596081733703613\n",
      "epoch 71: loss 0.32461267709732056\n",
      "epoch 72: loss 0.25926870107650757\n",
      "epoch 73: loss 0.3398933410644531\n",
      "epoch 74: loss 0.2897459864616394\n",
      "epoch 75: loss 0.3362184166908264\n",
      "epoch 76: loss 0.302142858505249\n",
      "epoch 77: loss 0.4101652503013611\n",
      "epoch 78: loss 0.3096039891242981\n",
      "epoch 79: loss 0.35979828238487244\n",
      "epoch 80: loss 0.37895309925079346\n",
      "epoch 81: loss 0.27226173877716064\n",
      "epoch 82: loss 0.35316699743270874\n",
      "epoch 83: loss 0.37891513109207153\n",
      "epoch 84: loss 0.24968954920768738\n",
      "epoch 85: loss 0.30654430389404297\n",
      "epoch 86: loss 0.3391704261302948\n",
      "epoch 87: loss 0.34614861011505127\n",
      "epoch 88: loss 0.2640116810798645\n",
      "epoch 89: loss 0.33711206912994385\n",
      "epoch 90: loss 0.2879605293273926\n",
      "epoch 91: loss 0.336711049079895\n",
      "epoch 92: loss 0.2801089286804199\n",
      "epoch 93: loss 0.31210947036743164\n",
      "epoch 94: loss 0.3431287407875061\n",
      "epoch 95: loss 0.20769700407981873\n",
      "epoch 96: loss 0.21167051792144775\n",
      "epoch 97: loss 0.3083336651325226\n",
      "epoch 98: loss 0.35181504487991333\n",
      "epoch 99: loss 0.3789512515068054\n",
      "epoch 100: loss 0.5202078819274902\n",
      "epoch 101: loss 0.30546802282333374\n",
      "epoch 102: loss 0.26056912541389465\n",
      "epoch 103: loss 0.3033662736415863\n",
      "epoch 104: loss 0.27689677476882935\n",
      "epoch 105: loss 0.3741893172264099\n",
      "epoch 106: loss 0.32818976044654846\n",
      "epoch 107: loss 0.2849425673484802\n",
      "epoch 108: loss 0.34592142701148987\n",
      "epoch 109: loss 0.3536068797111511\n",
      "epoch 110: loss 0.35341140627861023\n",
      "epoch 111: loss 0.3817768692970276\n",
      "epoch 112: loss 0.38411903381347656\n",
      "epoch 113: loss 0.2980571389198303\n",
      "epoch 114: loss 0.18094024062156677\n",
      "epoch 115: loss 0.4087587594985962\n",
      "epoch 116: loss 0.30879685282707214\n",
      "epoch 117: loss 0.2887236773967743\n",
      "epoch 118: loss 0.32970649003982544\n",
      "epoch 119: loss 0.37429553270339966\n",
      "epoch 120: loss 0.2680901885032654\n",
      "epoch 121: loss 0.38749054074287415\n",
      "epoch 122: loss 0.25547122955322266\n",
      "epoch 123: loss 0.39248406887054443\n",
      "epoch 124: loss 0.3034304976463318\n",
      "epoch 125: loss 0.3398096561431885\n",
      "epoch 126: loss 0.3494713306427002\n",
      "epoch 127: loss 0.37162157893180847\n",
      "epoch 128: loss 0.27625060081481934\n",
      "epoch 129: loss 0.3460400700569153\n",
      "epoch 130: loss 0.36733195185661316\n",
      "epoch 131: loss 0.3218737840652466\n",
      "epoch 132: loss 0.315878301858902\n",
      "epoch 133: loss 0.3573208153247833\n",
      "epoch 134: loss 0.3254760205745697\n",
      "epoch 135: loss 0.2515937089920044\n",
      "epoch 136: loss 0.23220588266849518\n",
      "epoch 0: loss 0.34013694524765015\n",
      "epoch 1: loss 0.3383179008960724\n",
      "epoch 2: loss 0.3777206242084503\n",
      "epoch 3: loss 0.37228432297706604\n",
      "epoch 4: loss 0.3158019781112671\n",
      "epoch 5: loss 0.25585371255874634\n",
      "epoch 6: loss 0.2837556004524231\n",
      "epoch 7: loss 0.222348153591156\n",
      "epoch 8: loss 0.41390931606292725\n",
      "epoch 9: loss 0.41668689250946045\n",
      "epoch 10: loss 0.33634376525878906\n",
      "epoch 11: loss 0.2684067487716675\n",
      "epoch 12: loss 0.3170104920864105\n",
      "epoch 13: loss 0.40666648745536804\n",
      "epoch 14: loss 0.37247583270072937\n",
      "epoch 15: loss 0.27616626024246216\n",
      "epoch 16: loss 0.3457376956939697\n",
      "epoch 17: loss 0.35032713413238525\n",
      "epoch 18: loss 0.4106428325176239\n",
      "epoch 19: loss 0.27352896332740784\n",
      "epoch 20: loss 0.31184008717536926\n",
      "epoch 21: loss 0.3365701735019684\n",
      "epoch 22: loss 0.3161173462867737\n",
      "epoch 23: loss 0.36402738094329834\n",
      "epoch 24: loss 0.38581642508506775\n",
      "epoch 25: loss 0.3883352279663086\n",
      "epoch 26: loss 0.3108738660812378\n",
      "epoch 27: loss 0.3749816417694092\n",
      "epoch 28: loss 0.43596726655960083\n",
      "epoch 29: loss 0.3220789134502411\n",
      "epoch 30: loss 0.31160876154899597\n",
      "epoch 31: loss 0.33236175775527954\n",
      "epoch 32: loss 0.33710426092147827\n",
      "epoch 33: loss 0.3483424782752991\n",
      "epoch 34: loss 0.38817259669303894\n",
      "epoch 35: loss 0.2709074020385742\n",
      "epoch 36: loss 0.35374096035957336\n",
      "epoch 37: loss 0.3053889870643616\n",
      "epoch 38: loss 0.2254083901643753\n",
      "epoch 39: loss 0.2980775237083435\n",
      "epoch 40: loss 0.3030173182487488\n",
      "epoch 41: loss 0.28034737706184387\n",
      "epoch 42: loss 0.24900367856025696\n",
      "epoch 43: loss 0.3153616189956665\n",
      "epoch 44: loss 0.34311115741729736\n",
      "epoch 45: loss 0.27799317240715027\n",
      "epoch 46: loss 0.28573286533355713\n",
      "epoch 47: loss 0.2759523391723633\n",
      "epoch 48: loss 0.26532623171806335\n",
      "epoch 49: loss 0.3438330292701721\n",
      "epoch 50: loss 0.28728199005126953\n",
      "epoch 51: loss 0.43259018659591675\n",
      "epoch 52: loss 0.3047792613506317\n",
      "epoch 53: loss 0.37161383032798767\n",
      "epoch 54: loss 0.34772443771362305\n",
      "epoch 55: loss 0.301659494638443\n",
      "epoch 56: loss 0.34712791442871094\n",
      "epoch 57: loss 0.2412257194519043\n",
      "epoch 58: loss 0.32656043767929077\n",
      "epoch 59: loss 0.2637854218482971\n",
      "epoch 60: loss 0.3328091502189636\n",
      "epoch 61: loss 0.2234085202217102\n",
      "epoch 62: loss 0.24138231575489044\n",
      "epoch 63: loss 0.34066811203956604\n",
      "epoch 64: loss 0.28816983103752136\n",
      "epoch 65: loss 0.39437413215637207\n",
      "epoch 66: loss 0.33080586791038513\n",
      "epoch 67: loss 0.3716218173503876\n",
      "epoch 68: loss 0.35808682441711426\n",
      "epoch 69: loss 0.41515249013900757\n",
      "epoch 70: loss 0.36991238594055176\n",
      "epoch 71: loss 0.3330564498901367\n",
      "epoch 72: loss 0.2631670832633972\n",
      "epoch 73: loss 0.32573986053466797\n",
      "epoch 74: loss 0.27614983916282654\n",
      "epoch 75: loss 0.34041476249694824\n",
      "epoch 76: loss 0.30608832836151123\n",
      "epoch 77: loss 0.4021470844745636\n",
      "epoch 78: loss 0.30518639087677\n",
      "epoch 79: loss 0.35343366861343384\n",
      "epoch 80: loss 0.38543975353240967\n",
      "epoch 81: loss 0.2648845911026001\n",
      "epoch 82: loss 0.3203926980495453\n",
      "epoch 83: loss 0.4075796604156494\n",
      "epoch 84: loss 0.2542290687561035\n",
      "epoch 85: loss 0.30257174372673035\n",
      "epoch 86: loss 0.3431352972984314\n",
      "epoch 87: loss 0.34050416946411133\n",
      "epoch 88: loss 0.2721584439277649\n",
      "epoch 89: loss 0.37141841650009155\n",
      "epoch 90: loss 0.3051387667655945\n",
      "epoch 91: loss 0.40287327766418457\n",
      "epoch 92: loss 0.33512306213378906\n",
      "epoch 93: loss 0.314461886882782\n",
      "epoch 94: loss 0.30357638001441956\n",
      "epoch 95: loss 0.3111095726490021\n",
      "epoch 96: loss 0.21779067814350128\n",
      "epoch 97: loss 0.29476985335350037\n",
      "epoch 98: loss 0.3405900299549103\n",
      "epoch 99: loss 0.36908286809921265\n",
      "epoch 100: loss 0.5231450796127319\n",
      "epoch 101: loss 0.3123558759689331\n",
      "epoch 102: loss 0.2707933485507965\n",
      "epoch 103: loss 0.30750295519828796\n",
      "epoch 104: loss 0.2637132406234741\n",
      "epoch 105: loss 0.3679506778717041\n",
      "epoch 106: loss 0.3466712534427643\n",
      "epoch 107: loss 0.2976173162460327\n",
      "epoch 108: loss 0.34284257888793945\n",
      "epoch 109: loss 0.36031466722488403\n",
      "epoch 110: loss 0.3573913276195526\n",
      "epoch 111: loss 0.3987874388694763\n",
      "epoch 112: loss 0.4094666838645935\n",
      "epoch 113: loss 0.31979474425315857\n",
      "epoch 114: loss 0.18543776869773865\n",
      "epoch 115: loss 0.44176700711250305\n",
      "epoch 116: loss 0.3126868009567261\n",
      "epoch 117: loss 0.2710367739200592\n",
      "epoch 118: loss 0.3321007490158081\n",
      "epoch 119: loss 0.37373924255371094\n",
      "epoch 120: loss 0.2976422905921936\n",
      "epoch 121: loss 0.368956983089447\n",
      "epoch 122: loss 0.2621001601219177\n",
      "epoch 123: loss 0.3951021730899811\n",
      "epoch 124: loss 0.30381014943122864\n",
      "epoch 125: loss 0.32927003502845764\n",
      "epoch 126: loss 0.3769081234931946\n",
      "epoch 127: loss 0.40363943576812744\n",
      "epoch 128: loss 0.2627819776535034\n",
      "epoch 129: loss 0.3478052020072937\n",
      "epoch 130: loss 0.361706405878067\n",
      "epoch 131: loss 0.3210328221321106\n",
      "epoch 132: loss 0.308449387550354\n",
      "epoch 133: loss 0.3395612835884094\n",
      "epoch 134: loss 0.3306376338005066\n",
      "epoch 135: loss 0.26561981439590454\n",
      "epoch 136: loss 0.25170519948005676\n",
      "epoch 0: loss 0.35605019330978394\n",
      "epoch 1: loss 0.3324522078037262\n",
      "epoch 2: loss 0.41379666328430176\n",
      "epoch 3: loss 0.36533990502357483\n",
      "epoch 4: loss 0.3138934373855591\n",
      "epoch 5: loss 0.2603380084037781\n",
      "epoch 6: loss 0.28318309783935547\n",
      "epoch 7: loss 0.2252974510192871\n",
      "epoch 8: loss 0.4110836386680603\n",
      "epoch 9: loss 0.4083051085472107\n",
      "epoch 10: loss 0.32519030570983887\n",
      "epoch 11: loss 0.26411673426628113\n",
      "epoch 12: loss 0.30255943536758423\n",
      "epoch 13: loss 0.4151763916015625\n",
      "epoch 14: loss 0.37715673446655273\n",
      "epoch 15: loss 0.2749268412590027\n",
      "epoch 16: loss 0.3493611216545105\n",
      "epoch 17: loss 0.34470656514167786\n",
      "epoch 18: loss 0.41098204255104065\n",
      "epoch 19: loss 0.2627527415752411\n",
      "epoch 20: loss 0.30750054121017456\n",
      "epoch 21: loss 0.3428078889846802\n",
      "epoch 22: loss 0.3190287947654724\n",
      "epoch 23: loss 0.3608846664428711\n",
      "epoch 24: loss 0.3744216859340668\n",
      "epoch 25: loss 0.38904517889022827\n",
      "epoch 26: loss 0.3095605969429016\n",
      "epoch 27: loss 0.3556421399116516\n",
      "epoch 28: loss 0.4171285033226013\n",
      "epoch 29: loss 0.3174987733364105\n",
      "epoch 30: loss 0.2975650131702423\n",
      "epoch 31: loss 0.33398476243019104\n",
      "epoch 32: loss 0.3394923508167267\n",
      "epoch 33: loss 0.33700650930404663\n",
      "epoch 34: loss 0.3819119930267334\n",
      "epoch 35: loss 0.2652212977409363\n",
      "epoch 36: loss 0.33160755038261414\n",
      "epoch 37: loss 0.3187468647956848\n",
      "epoch 38: loss 0.22250688076019287\n",
      "epoch 39: loss 0.3136710226535797\n",
      "epoch 40: loss 0.3073236346244812\n",
      "epoch 41: loss 0.2787875235080719\n",
      "epoch 42: loss 0.2534578740596771\n",
      "epoch 43: loss 0.3227865397930145\n",
      "epoch 44: loss 0.34226202964782715\n",
      "epoch 45: loss 0.27270177006721497\n",
      "epoch 46: loss 0.29601162672042847\n",
      "epoch 47: loss 0.2821500897407532\n",
      "epoch 48: loss 0.25321149826049805\n",
      "epoch 49: loss 0.33905911445617676\n",
      "epoch 50: loss 0.2899760603904724\n",
      "epoch 51: loss 0.43155691027641296\n",
      "epoch 52: loss 0.30183008313179016\n",
      "epoch 53: loss 0.3727027177810669\n",
      "epoch 54: loss 0.3503318727016449\n",
      "epoch 55: loss 0.30175936222076416\n",
      "epoch 56: loss 0.3472597002983093\n",
      "epoch 57: loss 0.24882060289382935\n",
      "epoch 58: loss 0.32675182819366455\n",
      "epoch 59: loss 0.2655123770236969\n",
      "epoch 60: loss 0.3350551724433899\n",
      "epoch 61: loss 0.22329162061214447\n",
      "epoch 62: loss 0.24169251322746277\n",
      "epoch 63: loss 0.34905076026916504\n",
      "epoch 64: loss 0.28925663232803345\n",
      "epoch 65: loss 0.39736419916152954\n",
      "epoch 66: loss 0.3289775848388672\n",
      "epoch 67: loss 0.3719031810760498\n",
      "epoch 68: loss 0.3585134744644165\n",
      "epoch 69: loss 0.41479864716529846\n",
      "epoch 70: loss 0.3760966956615448\n",
      "epoch 71: loss 0.3380695581436157\n",
      "epoch 72: loss 0.2655351758003235\n",
      "epoch 73: loss 0.3290786147117615\n",
      "epoch 74: loss 0.2782171368598938\n",
      "epoch 75: loss 0.34000563621520996\n",
      "epoch 76: loss 0.30254805088043213\n",
      "epoch 77: loss 0.4056546688079834\n",
      "epoch 78: loss 0.30363929271698\n",
      "epoch 79: loss 0.35656753182411194\n",
      "epoch 80: loss 0.3846829831600189\n",
      "epoch 81: loss 0.26642468571662903\n",
      "epoch 82: loss 0.32170048356056213\n",
      "epoch 83: loss 0.40915071964263916\n",
      "epoch 84: loss 0.2621597647666931\n",
      "epoch 85: loss 0.30189764499664307\n",
      "epoch 86: loss 0.34184500575065613\n",
      "epoch 87: loss 0.34123218059539795\n",
      "epoch 88: loss 0.26874202489852905\n",
      "epoch 89: loss 0.3679150938987732\n",
      "epoch 90: loss 0.30491968989372253\n",
      "epoch 91: loss 0.39982348680496216\n",
      "epoch 92: loss 0.3280155062675476\n",
      "epoch 93: loss 0.313620001077652\n",
      "epoch 94: loss 0.304317444562912\n",
      "epoch 95: loss 0.2758118212223053\n",
      "epoch 96: loss 0.2274477481842041\n",
      "epoch 97: loss 0.2920874357223511\n",
      "epoch 98: loss 0.31465965509414673\n",
      "epoch 99: loss 0.36595386266708374\n",
      "epoch 100: loss 0.507637619972229\n",
      "epoch 101: loss 0.3087664544582367\n",
      "epoch 102: loss 0.2689375579357147\n",
      "epoch 103: loss 0.31311309337615967\n",
      "epoch 104: loss 0.2613163888454437\n",
      "epoch 105: loss 0.3745432198047638\n",
      "epoch 106: loss 0.32712122797966003\n",
      "epoch 107: loss 0.3119855523109436\n",
      "epoch 108: loss 0.3509233891963959\n",
      "epoch 109: loss 0.3532424867153168\n",
      "epoch 110: loss 0.35692375898361206\n",
      "epoch 111: loss 0.38822877407073975\n",
      "epoch 112: loss 0.395293653011322\n",
      "epoch 113: loss 0.3059884309768677\n",
      "epoch 114: loss 0.18125338852405548\n",
      "epoch 115: loss 0.42600029706954956\n",
      "epoch 116: loss 0.3114624619483948\n",
      "epoch 117: loss 0.278921902179718\n",
      "epoch 118: loss 0.33511194586753845\n",
      "epoch 119: loss 0.3747909665107727\n",
      "epoch 120: loss 0.2869795858860016\n",
      "epoch 121: loss 0.3713254928588867\n",
      "epoch 122: loss 0.2579813003540039\n",
      "epoch 123: loss 0.39348047971725464\n",
      "epoch 124: loss 0.30208536982536316\n",
      "epoch 125: loss 0.3350658416748047\n",
      "epoch 126: loss 0.36432331800460815\n",
      "epoch 127: loss 0.37292319536209106\n",
      "epoch 128: loss 0.29281601309776306\n",
      "epoch 129: loss 0.3464075028896332\n",
      "epoch 130: loss 0.3639940619468689\n",
      "epoch 131: loss 0.31918975710868835\n",
      "epoch 132: loss 0.31219398975372314\n",
      "epoch 133: loss 0.35606759786605835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 134: loss 0.32658061385154724\n",
      "epoch 135: loss 0.25319433212280273\n",
      "epoch 136: loss 0.23851247131824493\n",
      "epoch 0: loss 0.35439932346343994\n",
      "epoch 1: loss 0.3318634033203125\n",
      "epoch 2: loss 0.3901510238647461\n",
      "epoch 3: loss 0.37384557723999023\n",
      "epoch 4: loss 0.3316376507282257\n",
      "epoch 5: loss 0.25914227962493896\n",
      "epoch 6: loss 0.28443998098373413\n",
      "epoch 7: loss 0.22489118576049805\n",
      "epoch 8: loss 0.4251505136489868\n",
      "epoch 9: loss 0.4451013505458832\n",
      "epoch 10: loss 0.3522043824195862\n",
      "epoch 11: loss 0.28206199407577515\n",
      "epoch 12: loss 0.30853062868118286\n",
      "epoch 13: loss 0.4076142907142639\n",
      "epoch 14: loss 0.4184989929199219\n",
      "epoch 15: loss 0.2823582589626312\n",
      "epoch 16: loss 0.3413568139076233\n",
      "epoch 17: loss 0.36741435527801514\n",
      "epoch 18: loss 0.4269894063472748\n",
      "epoch 19: loss 0.29837942123413086\n",
      "epoch 20: loss 0.3284135162830353\n",
      "epoch 21: loss 0.3612387478351593\n",
      "epoch 22: loss 0.3257681429386139\n",
      "epoch 23: loss 0.3728792667388916\n",
      "epoch 24: loss 0.3423202633857727\n",
      "epoch 25: loss 0.4163846969604492\n",
      "epoch 26: loss 0.35505008697509766\n",
      "epoch 27: loss 0.3434029817581177\n",
      "epoch 28: loss 0.40068042278289795\n",
      "epoch 29: loss 0.31575828790664673\n",
      "epoch 30: loss 0.306729257106781\n",
      "epoch 31: loss 0.34986767172813416\n",
      "epoch 32: loss 0.3543342351913452\n",
      "epoch 33: loss 0.33484572172164917\n",
      "epoch 34: loss 0.382199227809906\n",
      "epoch 35: loss 0.2585899233818054\n",
      "epoch 36: loss 0.32404494285583496\n",
      "epoch 37: loss 0.3218473196029663\n",
      "epoch 38: loss 0.22096315026283264\n",
      "epoch 39: loss 0.3110106885433197\n",
      "epoch 40: loss 0.2975403070449829\n",
      "epoch 41: loss 0.27725499868392944\n",
      "epoch 42: loss 0.2697579264640808\n",
      "epoch 43: loss 0.33410918712615967\n",
      "epoch 44: loss 0.3437933325767517\n",
      "epoch 45: loss 0.2767598032951355\n",
      "epoch 46: loss 0.29446887969970703\n",
      "epoch 47: loss 0.28463923931121826\n",
      "epoch 48: loss 0.2501119375228882\n",
      "epoch 49: loss 0.332988440990448\n",
      "epoch 50: loss 0.28317970037460327\n",
      "epoch 51: loss 0.4134255051612854\n",
      "epoch 52: loss 0.28574249148368835\n",
      "epoch 53: loss 0.3708222806453705\n",
      "epoch 54: loss 0.34624144434928894\n",
      "epoch 55: loss 0.30055323243141174\n",
      "epoch 56: loss 0.35160282254219055\n",
      "epoch 57: loss 0.25263169407844543\n",
      "epoch 58: loss 0.3269577622413635\n",
      "epoch 59: loss 0.25828856229782104\n",
      "epoch 60: loss 0.3345656991004944\n",
      "epoch 61: loss 0.223179429769516\n",
      "epoch 62: loss 0.23958784341812134\n",
      "epoch 63: loss 0.35996848344802856\n",
      "epoch 64: loss 0.29210054874420166\n",
      "epoch 65: loss 0.40222054719924927\n",
      "epoch 66: loss 0.3303513824939728\n",
      "epoch 67: loss 0.3717298209667206\n",
      "epoch 68: loss 0.3620874583721161\n",
      "epoch 69: loss 0.40710318088531494\n",
      "epoch 70: loss 0.3864920437335968\n",
      "epoch 71: loss 0.32467079162597656\n",
      "epoch 72: loss 0.25701314210891724\n",
      "epoch 73: loss 0.3398069739341736\n",
      "epoch 74: loss 0.28598999977111816\n",
      "epoch 75: loss 0.3377840518951416\n",
      "epoch 76: loss 0.3027951717376709\n",
      "epoch 77: loss 0.41686028242111206\n",
      "epoch 78: loss 0.3181537985801697\n",
      "epoch 79: loss 0.3771263360977173\n",
      "epoch 80: loss 0.3825489282608032\n",
      "epoch 81: loss 0.2714921236038208\n",
      "epoch 82: loss 0.3527747392654419\n",
      "epoch 83: loss 0.3863414525985718\n",
      "epoch 84: loss 0.2612937092781067\n",
      "epoch 85: loss 0.3018532395362854\n",
      "epoch 86: loss 0.3389573395252228\n",
      "epoch 87: loss 0.339599609375\n",
      "epoch 88: loss 0.2662000060081482\n",
      "epoch 89: loss 0.34796595573425293\n",
      "epoch 90: loss 0.29687368869781494\n",
      "epoch 91: loss 0.3553484380245209\n",
      "epoch 92: loss 0.30394792556762695\n",
      "epoch 93: loss 0.3097732961177826\n",
      "epoch 94: loss 0.2910032868385315\n",
      "epoch 95: loss 0.23297308385372162\n",
      "epoch 96: loss 0.2188204824924469\n",
      "epoch 97: loss 0.29519277811050415\n",
      "epoch 98: loss 0.3137091100215912\n",
      "epoch 99: loss 0.36459189653396606\n",
      "epoch 100: loss 0.48759040236473083\n",
      "epoch 101: loss 0.3056730628013611\n",
      "epoch 102: loss 0.26518815755844116\n",
      "epoch 103: loss 0.32160502672195435\n",
      "epoch 104: loss 0.26461124420166016\n",
      "epoch 105: loss 0.379044771194458\n",
      "epoch 106: loss 0.3190668821334839\n",
      "epoch 107: loss 0.2782140374183655\n",
      "epoch 108: loss 0.35662776231765747\n",
      "epoch 109: loss 0.3534955680370331\n",
      "epoch 110: loss 0.35146594047546387\n",
      "epoch 111: loss 0.363722562789917\n",
      "epoch 112: loss 0.36633285880088806\n",
      "epoch 113: loss 0.2772302031517029\n",
      "epoch 114: loss 0.1685256063938141\n",
      "epoch 115: loss 0.398114413022995\n",
      "epoch 116: loss 0.29560256004333496\n",
      "epoch 117: loss 0.26529935002326965\n",
      "epoch 118: loss 0.3260066509246826\n",
      "epoch 119: loss 0.3844144940376282\n",
      "epoch 120: loss 0.26300618052482605\n",
      "epoch 121: loss 0.3651226758956909\n",
      "epoch 122: loss 0.2441810965538025\n",
      "epoch 123: loss 0.3865763545036316\n",
      "epoch 124: loss 0.314963161945343\n",
      "epoch 125: loss 0.3402102589607239\n",
      "epoch 126: loss 0.3699110150337219\n",
      "epoch 127: loss 0.3974881172180176\n",
      "epoch 128: loss 0.2564244270324707\n",
      "epoch 129: loss 0.34238481521606445\n",
      "epoch 130: loss 0.36467596888542175\n",
      "epoch 131: loss 0.3352597951889038\n",
      "epoch 132: loss 0.3151288628578186\n",
      "epoch 133: loss 0.3427221477031708\n",
      "epoch 134: loss 0.32021403312683105\n",
      "epoch 135: loss 0.254574179649353\n",
      "epoch 136: loss 0.23364457488059998\n",
      "epoch 0: loss 0.36553025245666504\n",
      "epoch 1: loss 0.3358326554298401\n",
      "epoch 2: loss 0.40741124749183655\n",
      "epoch 3: loss 0.3645216226577759\n",
      "epoch 4: loss 0.3342853784561157\n",
      "epoch 5: loss 0.27073439955711365\n",
      "epoch 6: loss 0.2814551591873169\n",
      "epoch 7: loss 0.22166186571121216\n",
      "epoch 8: loss 0.41084539890289307\n",
      "epoch 9: loss 0.4247765839099884\n",
      "epoch 10: loss 0.3456459641456604\n",
      "epoch 11: loss 0.280972421169281\n",
      "epoch 12: loss 0.30697259306907654\n",
      "epoch 13: loss 0.4184602200984955\n",
      "epoch 14: loss 0.38599246740341187\n",
      "epoch 15: loss 0.28672802448272705\n",
      "epoch 16: loss 0.3539521098136902\n",
      "epoch 17: loss 0.3372581899166107\n",
      "epoch 18: loss 0.3952420949935913\n",
      "epoch 19: loss 0.26647046208381653\n",
      "epoch 20: loss 0.31162288784980774\n",
      "epoch 21: loss 0.34063130617141724\n",
      "epoch 22: loss 0.3215336799621582\n",
      "epoch 23: loss 0.36864688992500305\n",
      "epoch 24: loss 0.3390888571739197\n",
      "epoch 25: loss 0.4075486660003662\n",
      "epoch 26: loss 0.32326018810272217\n",
      "epoch 27: loss 0.34024548530578613\n",
      "epoch 28: loss 0.4031609296798706\n",
      "epoch 29: loss 0.31654226779937744\n",
      "epoch 30: loss 0.2874515950679779\n",
      "epoch 31: loss 0.3328506052494049\n",
      "epoch 32: loss 0.34473341703414917\n",
      "epoch 33: loss 0.33878928422927856\n",
      "epoch 34: loss 0.3817451596260071\n",
      "epoch 35: loss 0.25863391160964966\n",
      "epoch 36: loss 0.32393455505371094\n",
      "epoch 37: loss 0.3247329294681549\n",
      "epoch 38: loss 0.22541552782058716\n",
      "epoch 39: loss 0.315285861492157\n",
      "epoch 40: loss 0.30255556106567383\n",
      "epoch 41: loss 0.2765437960624695\n",
      "epoch 42: loss 0.260634183883667\n",
      "epoch 43: loss 0.3341519832611084\n",
      "epoch 44: loss 0.33803343772888184\n",
      "epoch 45: loss 0.27592241764068604\n",
      "epoch 46: loss 0.28861677646636963\n",
      "epoch 47: loss 0.2793568968772888\n",
      "epoch 48: loss 0.2530679702758789\n",
      "epoch 49: loss 0.3353659212589264\n",
      "epoch 50: loss 0.27977585792541504\n",
      "epoch 51: loss 0.41740351915359497\n",
      "epoch 52: loss 0.28646957874298096\n",
      "epoch 53: loss 0.37272101640701294\n",
      "epoch 54: loss 0.34664425253868103\n",
      "epoch 55: loss 0.29750940203666687\n",
      "epoch 56: loss 0.3514178395271301\n",
      "epoch 57: loss 0.2483576089143753\n",
      "epoch 58: loss 0.3245217800140381\n",
      "epoch 59: loss 0.26173335313796997\n",
      "epoch 60: loss 0.3305412828922272\n",
      "epoch 61: loss 0.22165100276470184\n",
      "epoch 62: loss 0.24063268303871155\n",
      "epoch 63: loss 0.3565369248390198\n",
      "epoch 64: loss 0.2868388891220093\n",
      "epoch 65: loss 0.40117090940475464\n",
      "epoch 66: loss 0.3283032774925232\n",
      "epoch 67: loss 0.37411439418792725\n",
      "epoch 68: loss 0.35987383127212524\n",
      "epoch 69: loss 0.40706756711006165\n",
      "epoch 70: loss 0.38036060333251953\n",
      "epoch 71: loss 0.32984811067581177\n",
      "epoch 72: loss 0.25763964653015137\n",
      "epoch 73: loss 0.33312416076660156\n",
      "epoch 74: loss 0.2826569676399231\n",
      "epoch 75: loss 0.33860471844673157\n",
      "epoch 76: loss 0.302511602640152\n",
      "epoch 77: loss 0.41049033403396606\n",
      "epoch 78: loss 0.30926018953323364\n",
      "epoch 79: loss 0.36536455154418945\n",
      "epoch 80: loss 0.38053831458091736\n",
      "epoch 81: loss 0.26819276809692383\n",
      "epoch 82: loss 0.33975091576576233\n",
      "epoch 83: loss 0.3870469629764557\n",
      "epoch 84: loss 0.25500163435935974\n",
      "epoch 85: loss 0.3004658818244934\n",
      "epoch 86: loss 0.3386160731315613\n",
      "epoch 87: loss 0.34305626153945923\n",
      "epoch 88: loss 0.26531583070755005\n",
      "epoch 89: loss 0.35225072503089905\n",
      "epoch 90: loss 0.29786866903305054\n",
      "epoch 91: loss 0.3647094666957855\n",
      "epoch 92: loss 0.30399465560913086\n",
      "epoch 93: loss 0.30981481075286865\n",
      "epoch 94: loss 0.30010873079299927\n",
      "epoch 95: loss 0.2555440664291382\n",
      "epoch 96: loss 0.21749228239059448\n",
      "epoch 97: loss 0.2900479733943939\n",
      "epoch 98: loss 0.3153214454650879\n",
      "epoch 99: loss 0.3666973412036896\n",
      "epoch 100: loss 0.5170918107032776\n",
      "epoch 101: loss 0.3134413957595825\n",
      "epoch 102: loss 0.2770463824272156\n",
      "epoch 103: loss 0.3305831551551819\n",
      "epoch 104: loss 0.2636052668094635\n",
      "epoch 105: loss 0.37446409463882446\n",
      "epoch 106: loss 0.3221552073955536\n",
      "epoch 107: loss 0.292855441570282\n",
      "epoch 108: loss 0.35684120655059814\n",
      "epoch 109: loss 0.3532106876373291\n",
      "epoch 110: loss 0.34840327501296997\n",
      "epoch 111: loss 0.3672221899032593\n",
      "epoch 112: loss 0.37045615911483765\n",
      "epoch 113: loss 0.285302996635437\n",
      "epoch 114: loss 0.17351733148097992\n",
      "epoch 115: loss 0.40328744053840637\n",
      "epoch 116: loss 0.30255982279777527\n",
      "epoch 117: loss 0.27527952194213867\n",
      "epoch 118: loss 0.32772916555404663\n",
      "epoch 119: loss 0.3832005262374878\n",
      "epoch 120: loss 0.26196473836898804\n",
      "epoch 121: loss 0.3869619369506836\n",
      "epoch 122: loss 0.24623115360736847\n",
      "epoch 123: loss 0.3932355046272278\n",
      "epoch 124: loss 0.30914947390556335\n",
      "epoch 125: loss 0.34320640563964844\n",
      "epoch 126: loss 0.34248799085617065\n",
      "epoch 127: loss 0.3732174038887024\n",
      "epoch 128: loss 0.2794446647167206\n",
      "epoch 129: loss 0.3460661470890045\n",
      "epoch 130: loss 0.3684634566307068\n",
      "epoch 131: loss 0.32357871532440186\n",
      "epoch 132: loss 0.31951507925987244\n",
      "epoch 133: loss 0.36285626888275146\n",
      "epoch 134: loss 0.3251490294933319\n",
      "epoch 135: loss 0.25560203194618225\n",
      "epoch 136: loss 0.23279723525047302\n",
      "epoch 0: loss 0.36153119802474976\n",
      "epoch 1: loss 0.33100396394729614\n",
      "epoch 2: loss 0.39659619331359863\n",
      "epoch 3: loss 0.3677052855491638\n",
      "epoch 4: loss 0.3542940020561218\n",
      "epoch 5: loss 0.2617233991622925\n",
      "epoch 6: loss 0.2859070301055908\n",
      "epoch 7: loss 0.2290523648262024\n",
      "epoch 8: loss 0.41326725482940674\n",
      "epoch 9: loss 0.4399159252643585\n",
      "epoch 10: loss 0.3621352016925812\n",
      "epoch 11: loss 0.29560980200767517\n",
      "epoch 12: loss 0.2969631552696228\n",
      "epoch 13: loss 0.43417155742645264\n",
      "epoch 14: loss 0.3539833426475525\n",
      "epoch 15: loss 0.2913508713245392\n",
      "epoch 16: loss 0.3606642484664917\n",
      "epoch 17: loss 0.33935368061065674\n",
      "epoch 18: loss 0.37229740619659424\n",
      "epoch 19: loss 0.26325932145118713\n",
      "epoch 20: loss 0.29567790031433105\n",
      "epoch 21: loss 0.33212292194366455\n",
      "epoch 22: loss 0.3187234401702881\n",
      "epoch 23: loss 0.37040501832962036\n",
      "epoch 24: loss 0.35114139318466187\n",
      "epoch 25: loss 0.3876749277114868\n",
      "epoch 26: loss 0.3358009159564972\n",
      "epoch 27: loss 0.34402886033058167\n",
      "epoch 28: loss 0.3963478207588196\n",
      "epoch 29: loss 0.3105331063270569\n",
      "epoch 30: loss 0.2768574059009552\n",
      "epoch 31: loss 0.3407239317893982\n",
      "epoch 32: loss 0.33953553438186646\n",
      "epoch 33: loss 0.3409363031387329\n",
      "epoch 34: loss 0.38695627450942993\n",
      "epoch 35: loss 0.25925788283348083\n",
      "epoch 36: loss 0.32708531618118286\n",
      "epoch 37: loss 0.33789151906967163\n",
      "epoch 38: loss 0.23468908667564392\n",
      "epoch 39: loss 0.32683685421943665\n",
      "epoch 40: loss 0.31945914030075073\n",
      "epoch 41: loss 0.28237977623939514\n",
      "epoch 42: loss 0.26438188552856445\n",
      "epoch 43: loss 0.3376656174659729\n",
      "epoch 44: loss 0.33946898579597473\n",
      "epoch 45: loss 0.2777084410190582\n",
      "epoch 46: loss 0.2919674515724182\n",
      "epoch 47: loss 0.2804221510887146\n",
      "epoch 48: loss 0.24632342159748077\n",
      "epoch 49: loss 0.32775795459747314\n",
      "epoch 50: loss 0.28822487592697144\n",
      "epoch 51: loss 0.4211088716983795\n",
      "epoch 52: loss 0.2939530611038208\n",
      "epoch 53: loss 0.36480802297592163\n",
      "epoch 54: loss 0.3528026342391968\n",
      "epoch 55: loss 0.29776591062545776\n",
      "epoch 56: loss 0.34996068477630615\n",
      "epoch 57: loss 0.24369029700756073\n",
      "epoch 58: loss 0.3256514072418213\n",
      "epoch 59: loss 0.26236671209335327\n",
      "epoch 60: loss 0.331488698720932\n",
      "epoch 61: loss 0.22195778787136078\n",
      "epoch 62: loss 0.24223384261131287\n",
      "epoch 63: loss 0.3435834050178528\n",
      "epoch 64: loss 0.28699350357055664\n",
      "epoch 65: loss 0.3969120383262634\n",
      "epoch 66: loss 0.33147674798965454\n",
      "epoch 67: loss 0.3692789077758789\n",
      "epoch 68: loss 0.3601558804512024\n",
      "epoch 69: loss 0.41136711835861206\n",
      "epoch 70: loss 0.375469446182251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 71: loss 0.32766878604888916\n",
      "epoch 72: loss 0.2564900517463684\n",
      "epoch 73: loss 0.3308102488517761\n",
      "epoch 74: loss 0.2791861295700073\n",
      "epoch 75: loss 0.33964449167251587\n",
      "epoch 76: loss 0.3032068610191345\n",
      "epoch 77: loss 0.4071635603904724\n",
      "epoch 78: loss 0.30679914355278015\n",
      "epoch 79: loss 0.35717231035232544\n",
      "epoch 80: loss 0.37998247146606445\n",
      "epoch 81: loss 0.2654266655445099\n",
      "epoch 82: loss 0.3356899321079254\n",
      "epoch 83: loss 0.38972949981689453\n",
      "epoch 84: loss 0.2508071959018707\n",
      "epoch 85: loss 0.2997514605522156\n",
      "epoch 86: loss 0.3391880691051483\n",
      "epoch 87: loss 0.3440726399421692\n",
      "epoch 88: loss 0.2657005786895752\n",
      "epoch 89: loss 0.35347384214401245\n",
      "epoch 90: loss 0.2993990182876587\n",
      "epoch 91: loss 0.35853251814842224\n",
      "epoch 92: loss 0.30215930938720703\n",
      "epoch 93: loss 0.3106911778450012\n",
      "epoch 94: loss 0.2976562976837158\n",
      "epoch 95: loss 0.25704696774482727\n",
      "epoch 96: loss 0.21374602615833282\n",
      "epoch 97: loss 0.2902225852012634\n",
      "epoch 98: loss 0.3158816695213318\n",
      "epoch 99: loss 0.3701479434967041\n",
      "epoch 100: loss 0.5130161046981812\n",
      "epoch 101: loss 0.3094843327999115\n",
      "epoch 102: loss 0.27157264947891235\n",
      "epoch 103: loss 0.3213973045349121\n",
      "epoch 104: loss 0.25986194610595703\n",
      "epoch 105: loss 0.37517234683036804\n",
      "epoch 106: loss 0.32626402378082275\n",
      "epoch 107: loss 0.30126863718032837\n",
      "epoch 108: loss 0.3509984016418457\n",
      "epoch 109: loss 0.3501460552215576\n",
      "epoch 110: loss 0.3488818109035492\n",
      "epoch 111: loss 0.37358933687210083\n",
      "epoch 112: loss 0.37625205516815186\n",
      "epoch 113: loss 0.289547324180603\n",
      "epoch 114: loss 0.17486420273780823\n",
      "epoch 115: loss 0.4059738218784332\n",
      "epoch 116: loss 0.3087117075920105\n",
      "epoch 117: loss 0.27831488847732544\n",
      "epoch 118: loss 0.3286728858947754\n",
      "epoch 119: loss 0.3776100277900696\n",
      "epoch 120: loss 0.26258131861686707\n",
      "epoch 121: loss 0.39050278067588806\n",
      "epoch 122: loss 0.25052714347839355\n",
      "epoch 123: loss 0.3909167945384979\n",
      "epoch 124: loss 0.3057682514190674\n",
      "epoch 125: loss 0.345386803150177\n",
      "epoch 126: loss 0.34762248396873474\n",
      "epoch 127: loss 0.3729349970817566\n",
      "epoch 128: loss 0.2736768424510956\n",
      "epoch 129: loss 0.34584668278694153\n",
      "epoch 130: loss 0.3639000654220581\n",
      "epoch 131: loss 0.3206165134906769\n",
      "epoch 132: loss 0.3142358362674713\n",
      "epoch 133: loss 0.35403603315353394\n",
      "epoch 134: loss 0.3225339651107788\n",
      "epoch 135: loss 0.2528213560581207\n",
      "epoch 136: loss 0.23116452991962433\n",
      "epoch 0: loss 0.3444356322288513\n",
      "epoch 1: loss 0.3331722021102905\n",
      "epoch 2: loss 0.3808899521827698\n",
      "epoch 3: loss 0.37623313069343567\n",
      "epoch 4: loss 0.3311000168323517\n",
      "epoch 5: loss 0.254896879196167\n",
      "epoch 6: loss 0.2846309542655945\n",
      "epoch 7: loss 0.22593675553798676\n",
      "epoch 8: loss 0.4224822521209717\n",
      "epoch 9: loss 0.4414268136024475\n",
      "epoch 10: loss 0.35480034351348877\n",
      "epoch 11: loss 0.2856144905090332\n",
      "epoch 12: loss 0.301170289516449\n",
      "epoch 13: loss 0.4117947220802307\n",
      "epoch 14: loss 0.4037436842918396\n",
      "epoch 15: loss 0.28103572130203247\n",
      "epoch 16: loss 0.3486044406890869\n",
      "epoch 17: loss 0.34494641423225403\n",
      "epoch 18: loss 0.4051320552825928\n",
      "epoch 19: loss 0.2784190773963928\n",
      "epoch 20: loss 0.3213854432106018\n",
      "epoch 21: loss 0.34955719113349915\n",
      "epoch 22: loss 0.3225112557411194\n",
      "epoch 23: loss 0.3711288571357727\n",
      "epoch 24: loss 0.3397993743419647\n",
      "epoch 25: loss 0.4156762659549713\n",
      "epoch 26: loss 0.33784037828445435\n",
      "epoch 27: loss 0.3411930501461029\n",
      "epoch 28: loss 0.4009553790092468\n",
      "epoch 29: loss 0.3160894811153412\n",
      "epoch 30: loss 0.2957610487937927\n",
      "epoch 31: loss 0.3366686701774597\n",
      "epoch 32: loss 0.3475431203842163\n",
      "epoch 33: loss 0.33779048919677734\n",
      "epoch 34: loss 0.3814866244792938\n",
      "epoch 35: loss 0.2567746639251709\n",
      "epoch 36: loss 0.32526862621307373\n",
      "epoch 37: loss 0.3189953565597534\n",
      "epoch 38: loss 0.2208479344844818\n",
      "epoch 39: loss 0.3088163733482361\n",
      "epoch 40: loss 0.2997989356517792\n",
      "epoch 41: loss 0.2763662338256836\n",
      "epoch 42: loss 0.26111721992492676\n",
      "epoch 43: loss 0.3319029211997986\n",
      "epoch 44: loss 0.33736759424209595\n",
      "epoch 45: loss 0.27582108974456787\n",
      "epoch 46: loss 0.2892970144748688\n",
      "epoch 47: loss 0.2808030843734741\n",
      "epoch 48: loss 0.25040125846862793\n",
      "epoch 49: loss 0.3346300721168518\n",
      "epoch 50: loss 0.2801609933376312\n",
      "epoch 51: loss 0.4124215245246887\n",
      "epoch 52: loss 0.2839779257774353\n",
      "epoch 53: loss 0.3710523545742035\n",
      "epoch 54: loss 0.346979022026062\n",
      "epoch 55: loss 0.29887285828590393\n",
      "epoch 56: loss 0.34920087456703186\n",
      "epoch 57: loss 0.2431543469429016\n",
      "epoch 58: loss 0.3248944878578186\n",
      "epoch 59: loss 0.26297205686569214\n",
      "epoch 60: loss 0.33105531334877014\n",
      "epoch 61: loss 0.22222883999347687\n",
      "epoch 62: loss 0.24034926295280457\n",
      "epoch 63: loss 0.34889447689056396\n",
      "epoch 64: loss 0.28386613726615906\n",
      "epoch 65: loss 0.39535391330718994\n",
      "epoch 66: loss 0.32966312766075134\n",
      "epoch 67: loss 0.37397000193595886\n",
      "epoch 68: loss 0.35877612233161926\n",
      "epoch 69: loss 0.4073340594768524\n",
      "epoch 70: loss 0.37826234102249146\n",
      "epoch 71: loss 0.3297441899776459\n",
      "epoch 72: loss 0.25556138157844543\n",
      "epoch 73: loss 0.33162564039230347\n",
      "epoch 74: loss 0.28118857741355896\n",
      "epoch 75: loss 0.3411191701889038\n",
      "epoch 76: loss 0.30379706621170044\n",
      "epoch 77: loss 0.40926194190979004\n",
      "epoch 78: loss 0.30899983644485474\n",
      "epoch 79: loss 0.3620305359363556\n",
      "epoch 80: loss 0.3824560046195984\n",
      "epoch 81: loss 0.26528677344322205\n",
      "epoch 82: loss 0.3330889940261841\n",
      "epoch 83: loss 0.4015466272830963\n",
      "epoch 84: loss 0.25299596786499023\n",
      "epoch 85: loss 0.30048033595085144\n",
      "epoch 86: loss 0.34291183948516846\n",
      "epoch 87: loss 0.3402848243713379\n",
      "epoch 88: loss 0.26855140924453735\n",
      "epoch 89: loss 0.36587971448898315\n",
      "epoch 90: loss 0.30488163232803345\n",
      "epoch 91: loss 0.39250490069389343\n",
      "epoch 92: loss 0.3277231454849243\n",
      "epoch 93: loss 0.3105180263519287\n",
      "epoch 94: loss 0.290447473526001\n",
      "epoch 95: loss 0.2626081705093384\n",
      "epoch 96: loss 0.22802485525608063\n",
      "epoch 97: loss 0.291475772857666\n",
      "epoch 98: loss 0.313393235206604\n",
      "epoch 99: loss 0.3642079830169678\n",
      "epoch 100: loss 0.49984389543533325\n",
      "epoch 101: loss 0.3098679482936859\n",
      "epoch 102: loss 0.27636831998825073\n",
      "epoch 103: loss 0.33161354064941406\n",
      "epoch 104: loss 0.26509615778923035\n",
      "epoch 105: loss 0.3774082064628601\n",
      "epoch 106: loss 0.319225013256073\n",
      "epoch 107: loss 0.2827814221382141\n",
      "epoch 108: loss 0.357407808303833\n",
      "epoch 109: loss 0.35423848032951355\n",
      "epoch 110: loss 0.34656664729118347\n",
      "epoch 111: loss 0.36784279346466064\n",
      "epoch 112: loss 0.3684732913970947\n",
      "epoch 113: loss 0.276547908782959\n",
      "epoch 114: loss 0.16903728246688843\n",
      "epoch 115: loss 0.40170857310295105\n",
      "epoch 116: loss 0.2973858714103699\n",
      "epoch 117: loss 0.26124900579452515\n",
      "epoch 118: loss 0.32807376980781555\n",
      "epoch 119: loss 0.3823986351490021\n",
      "epoch 120: loss 0.25935691595077515\n",
      "epoch 121: loss 0.374556303024292\n",
      "epoch 122: loss 0.2453887015581131\n",
      "epoch 123: loss 0.38527780771255493\n",
      "epoch 124: loss 0.3187374770641327\n",
      "epoch 125: loss 0.3434606194496155\n",
      "epoch 126: loss 0.3601331114768982\n",
      "epoch 127: loss 0.3850298821926117\n",
      "epoch 128: loss 0.2598758935928345\n",
      "epoch 129: loss 0.3457798957824707\n",
      "epoch 130: loss 0.363489031791687\n",
      "epoch 131: loss 0.32461559772491455\n",
      "epoch 132: loss 0.3114292025566101\n",
      "epoch 133: loss 0.34370434284210205\n",
      "epoch 134: loss 0.31589871644973755\n",
      "epoch 135: loss 0.24744901061058044\n",
      "epoch 136: loss 0.22688189148902893\n",
      "epoch 0: loss 0.34568339586257935\n",
      "epoch 1: loss 0.33370497822761536\n",
      "epoch 2: loss 0.3855111598968506\n",
      "epoch 3: loss 0.3708755373954773\n",
      "epoch 4: loss 0.3295440375804901\n",
      "epoch 5: loss 0.2509145736694336\n",
      "epoch 6: loss 0.28176772594451904\n",
      "epoch 7: loss 0.22259171307086945\n",
      "epoch 8: loss 0.42590025067329407\n",
      "epoch 9: loss 0.43849217891693115\n",
      "epoch 10: loss 0.34678810834884644\n",
      "epoch 11: loss 0.27629196643829346\n",
      "epoch 12: loss 0.31197023391723633\n",
      "epoch 13: loss 0.4069822132587433\n",
      "epoch 14: loss 0.39974555373191833\n",
      "epoch 15: loss 0.27815866470336914\n",
      "epoch 16: loss 0.3429071009159088\n",
      "epoch 17: loss 0.35709452629089355\n",
      "epoch 18: loss 0.4172714352607727\n",
      "epoch 19: loss 0.2833455801010132\n",
      "epoch 20: loss 0.3232194781303406\n",
      "epoch 21: loss 0.34766194224357605\n",
      "epoch 22: loss 0.32254886627197266\n",
      "epoch 23: loss 0.37333375215530396\n",
      "epoch 24: loss 0.34844720363616943\n",
      "epoch 25: loss 0.4104737639427185\n",
      "epoch 26: loss 0.32505685091018677\n",
      "epoch 27: loss 0.3420045077800751\n",
      "epoch 28: loss 0.3957795202732086\n",
      "epoch 29: loss 0.3158755302429199\n",
      "epoch 30: loss 0.2795449197292328\n",
      "epoch 31: loss 0.3293493986129761\n",
      "epoch 32: loss 0.3427608609199524\n",
      "epoch 33: loss 0.34158265590667725\n",
      "epoch 34: loss 0.3871614933013916\n",
      "epoch 35: loss 0.2559736967086792\n",
      "epoch 36: loss 0.32677918672561646\n",
      "epoch 37: loss 0.32459792494773865\n",
      "epoch 38: loss 0.22392518818378448\n",
      "epoch 39: loss 0.3143024146556854\n",
      "epoch 40: loss 0.302562415599823\n",
      "epoch 41: loss 0.27438920736312866\n",
      "epoch 42: loss 0.2584927976131439\n",
      "epoch 43: loss 0.3312738537788391\n",
      "epoch 44: loss 0.33757075667381287\n",
      "epoch 45: loss 0.27489662170410156\n",
      "epoch 46: loss 0.28801798820495605\n",
      "epoch 47: loss 0.2803444266319275\n",
      "epoch 48: loss 0.25143131613731384\n",
      "epoch 49: loss 0.33233582973480225\n",
      "epoch 50: loss 0.28113001585006714\n",
      "epoch 51: loss 0.4128990173339844\n",
      "epoch 52: loss 0.2868156433105469\n",
      "epoch 53: loss 0.3686739206314087\n",
      "epoch 54: loss 0.3463584780693054\n",
      "epoch 55: loss 0.2985723316669464\n",
      "epoch 56: loss 0.3489636778831482\n",
      "epoch 57: loss 0.24434511363506317\n",
      "epoch 58: loss 0.32524657249450684\n",
      "epoch 59: loss 0.26271867752075195\n",
      "epoch 60: loss 0.3304485082626343\n",
      "epoch 61: loss 0.2223397195339203\n",
      "epoch 62: loss 0.2405461072921753\n",
      "epoch 63: loss 0.34519457817077637\n",
      "epoch 64: loss 0.2824169397354126\n",
      "epoch 65: loss 0.3960610628128052\n",
      "epoch 66: loss 0.3305533230304718\n",
      "epoch 67: loss 0.374824583530426\n",
      "epoch 68: loss 0.3601151704788208\n",
      "epoch 69: loss 0.40752512216567993\n",
      "epoch 70: loss 0.37668853998184204\n",
      "epoch 71: loss 0.32341107726097107\n",
      "epoch 72: loss 0.253257155418396\n",
      "epoch 73: loss 0.33028674125671387\n",
      "epoch 74: loss 0.27569639682769775\n",
      "epoch 75: loss 0.34251120686531067\n",
      "epoch 76: loss 0.3043014705181122\n",
      "epoch 77: loss 0.40519440174102783\n",
      "epoch 78: loss 0.30337291955947876\n",
      "epoch 79: loss 0.3511333465576172\n",
      "epoch 80: loss 0.3824244737625122\n",
      "epoch 81: loss 0.2656330466270447\n",
      "epoch 82: loss 0.3238435387611389\n",
      "epoch 83: loss 0.4020819365978241\n",
      "epoch 84: loss 0.26752814650535583\n",
      "epoch 85: loss 0.2984656095504761\n",
      "epoch 86: loss 0.34010931849479675\n",
      "epoch 87: loss 0.3395289480686188\n",
      "epoch 88: loss 0.2704380452632904\n",
      "epoch 89: loss 0.3707529902458191\n",
      "epoch 90: loss 0.3092692792415619\n",
      "epoch 91: loss 0.4167022109031677\n",
      "epoch 92: loss 0.3442087471485138\n",
      "epoch 93: loss 0.313360333442688\n",
      "epoch 94: loss 0.2934846878051758\n",
      "epoch 95: loss 0.2676190137863159\n",
      "epoch 96: loss 0.23100745677947998\n",
      "epoch 97: loss 0.2916226089000702\n",
      "epoch 98: loss 0.3128150999546051\n",
      "epoch 99: loss 0.36285480856895447\n",
      "epoch 100: loss 0.5063848495483398\n",
      "epoch 101: loss 0.3108239471912384\n",
      "epoch 102: loss 0.27655377984046936\n",
      "epoch 103: loss 0.3376496434211731\n",
      "epoch 104: loss 0.2677699327468872\n",
      "epoch 105: loss 0.3727065920829773\n",
      "epoch 106: loss 0.3173324465751648\n",
      "epoch 107: loss 0.28026774525642395\n",
      "epoch 108: loss 0.3589287996292114\n",
      "epoch 109: loss 0.35856497287750244\n",
      "epoch 110: loss 0.34893038868904114\n",
      "epoch 111: loss 0.36731964349746704\n",
      "epoch 112: loss 0.36641091108322144\n",
      "epoch 113: loss 0.27689212560653687\n",
      "epoch 114: loss 0.16426756978034973\n",
      "epoch 115: loss 0.4031945466995239\n",
      "epoch 116: loss 0.2942171096801758\n",
      "epoch 117: loss 0.2598285675048828\n",
      "epoch 118: loss 0.3244217336177826\n",
      "epoch 119: loss 0.3848983645439148\n",
      "epoch 120: loss 0.2692623734474182\n",
      "epoch 121: loss 0.35788995027542114\n",
      "epoch 122: loss 0.2425394058227539\n",
      "epoch 123: loss 0.38962414860725403\n",
      "epoch 124: loss 0.3049594461917877\n",
      "epoch 125: loss 0.34271061420440674\n",
      "epoch 126: loss 0.3500986695289612\n",
      "epoch 127: loss 0.37544456124305725\n",
      "epoch 128: loss 0.2968990206718445\n",
      "epoch 129: loss 0.3448689877986908\n",
      "epoch 130: loss 0.3721839487552643\n",
      "epoch 131: loss 0.326265811920166\n",
      "epoch 132: loss 0.3234022557735443\n",
      "epoch 133: loss 0.369087278842926\n",
      "epoch 134: loss 0.3252987861633301\n",
      "epoch 135: loss 0.25481271743774414\n",
      "epoch 136: loss 0.23272663354873657\n",
      "epoch 0: loss 0.3605552911758423\n",
      "epoch 1: loss 0.3321055471897125\n",
      "epoch 2: loss 0.39348793029785156\n",
      "epoch 3: loss 0.3672216534614563\n",
      "epoch 4: loss 0.35105931758880615\n",
      "epoch 5: loss 0.2651839256286621\n",
      "epoch 6: loss 0.2860892713069916\n",
      "epoch 7: loss 0.2277785837650299\n",
      "epoch 8: loss 0.41060253977775574\n",
      "epoch 9: loss 0.43961775302886963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: loss 0.36007624864578247\n",
      "epoch 11: loss 0.28994834423065186\n",
      "epoch 12: loss 0.30584585666656494\n",
      "epoch 13: loss 0.42786115407943726\n",
      "epoch 14: loss 0.368476927280426\n",
      "epoch 15: loss 0.2947118878364563\n",
      "epoch 16: loss 0.36506134271621704\n",
      "epoch 17: loss 0.3343546986579895\n",
      "epoch 18: loss 0.37728607654571533\n",
      "epoch 19: loss 0.2626453638076782\n",
      "epoch 20: loss 0.3025375008583069\n",
      "epoch 21: loss 0.335965633392334\n",
      "epoch 22: loss 0.31954365968704224\n",
      "epoch 23: loss 0.3722144067287445\n",
      "epoch 24: loss 0.34290602803230286\n",
      "epoch 25: loss 0.4032787084579468\n",
      "epoch 26: loss 0.35334527492523193\n",
      "epoch 27: loss 0.3396647274494171\n",
      "epoch 28: loss 0.4069865345954895\n",
      "epoch 29: loss 0.31741583347320557\n",
      "epoch 30: loss 0.2892848253250122\n",
      "epoch 31: loss 0.3366710841655731\n",
      "epoch 32: loss 0.3520079255104065\n",
      "epoch 33: loss 0.3363686800003052\n",
      "epoch 34: loss 0.3836732506752014\n",
      "epoch 35: loss 0.2585662007331848\n",
      "epoch 36: loss 0.32320213317871094\n",
      "epoch 37: loss 0.32498878240585327\n",
      "epoch 38: loss 0.22556713223457336\n",
      "epoch 39: loss 0.3146520256996155\n",
      "epoch 40: loss 0.3042747974395752\n",
      "epoch 41: loss 0.28128862380981445\n",
      "epoch 42: loss 0.2648071050643921\n",
      "epoch 43: loss 0.33414310216903687\n",
      "epoch 44: loss 0.33652380108833313\n",
      "epoch 45: loss 0.2780113220214844\n",
      "epoch 46: loss 0.28862297534942627\n",
      "epoch 47: loss 0.2823494076728821\n",
      "epoch 48: loss 0.24694305658340454\n",
      "epoch 49: loss 0.33338993787765503\n",
      "epoch 50: loss 0.28338873386383057\n",
      "epoch 51: loss 0.4127141833305359\n",
      "epoch 52: loss 0.287017285823822\n",
      "epoch 53: loss 0.3671702444553375\n",
      "epoch 54: loss 0.34698405861854553\n",
      "epoch 55: loss 0.297773540019989\n",
      "epoch 56: loss 0.35069721937179565\n",
      "epoch 57: loss 0.24217011034488678\n",
      "epoch 58: loss 0.32604366540908813\n",
      "epoch 59: loss 0.26164841651916504\n",
      "epoch 60: loss 0.3281446397304535\n",
      "epoch 61: loss 0.22079908847808838\n",
      "epoch 62: loss 0.24010755121707916\n",
      "epoch 63: loss 0.3420064151287079\n",
      "epoch 64: loss 0.2848054766654968\n",
      "epoch 65: loss 0.39719825983047485\n",
      "epoch 66: loss 0.3305911123752594\n",
      "epoch 67: loss 0.3748515248298645\n",
      "epoch 68: loss 0.36074596643447876\n",
      "epoch 69: loss 0.4068816304206848\n",
      "epoch 70: loss 0.3842070400714874\n",
      "epoch 71: loss 0.3223208487033844\n",
      "epoch 72: loss 0.25176095962524414\n",
      "epoch 73: loss 0.3325478434562683\n",
      "epoch 74: loss 0.2791156768798828\n",
      "epoch 75: loss 0.34139567613601685\n",
      "epoch 76: loss 0.30235302448272705\n",
      "epoch 77: loss 0.40991395711898804\n",
      "epoch 78: loss 0.3041927218437195\n",
      "epoch 79: loss 0.35413116216659546\n",
      "epoch 80: loss 0.3794461190700531\n",
      "epoch 81: loss 0.2659037113189697\n",
      "epoch 82: loss 0.33043885231018066\n",
      "epoch 83: loss 0.3979577124118805\n",
      "epoch 84: loss 0.25583091378211975\n",
      "epoch 85: loss 0.299184113740921\n",
      "epoch 86: loss 0.34148916602134705\n",
      "epoch 87: loss 0.34000372886657715\n",
      "epoch 88: loss 0.2695316672325134\n",
      "epoch 89: loss 0.35959184169769287\n",
      "epoch 90: loss 0.3049890100955963\n",
      "epoch 91: loss 0.3894914388656616\n",
      "epoch 92: loss 0.3261992335319519\n",
      "epoch 93: loss 0.3125731647014618\n",
      "epoch 94: loss 0.2879793047904968\n",
      "epoch 95: loss 0.2547115087509155\n",
      "epoch 96: loss 0.23555128276348114\n",
      "epoch 97: loss 0.29534807801246643\n",
      "epoch 98: loss 0.3109892010688782\n",
      "epoch 99: loss 0.3621770143508911\n",
      "epoch 100: loss 0.49035632610321045\n",
      "epoch 101: loss 0.3058464229106903\n",
      "epoch 102: loss 0.266461580991745\n",
      "epoch 103: loss 0.3212842345237732\n",
      "epoch 104: loss 0.2623640298843384\n",
      "epoch 105: loss 0.3821057379245758\n",
      "epoch 106: loss 0.31966710090637207\n",
      "epoch 107: loss 0.2806164026260376\n",
      "epoch 108: loss 0.35840731859207153\n",
      "epoch 109: loss 0.3536897599697113\n",
      "epoch 110: loss 0.3470594882965088\n",
      "epoch 111: loss 0.367174357175827\n",
      "epoch 112: loss 0.3700316548347473\n",
      "epoch 113: loss 0.27763253450393677\n",
      "epoch 114: loss 0.16849243640899658\n",
      "epoch 115: loss 0.40290939807891846\n",
      "epoch 116: loss 0.30098050832748413\n",
      "epoch 117: loss 0.26371845602989197\n",
      "epoch 118: loss 0.32764148712158203\n",
      "epoch 119: loss 0.3855152130126953\n",
      "epoch 120: loss 0.2623986005783081\n",
      "epoch 121: loss 0.3737730383872986\n",
      "epoch 122: loss 0.24473923444747925\n",
      "epoch 123: loss 0.38299083709716797\n",
      "epoch 124: loss 0.32331758737564087\n",
      "epoch 125: loss 0.3386697769165039\n",
      "epoch 126: loss 0.3645852208137512\n",
      "epoch 127: loss 0.39086127281188965\n",
      "epoch 128: loss 0.2563836872577667\n",
      "epoch 129: loss 0.34322354197502136\n",
      "epoch 130: loss 0.3630237579345703\n",
      "epoch 131: loss 0.3311299681663513\n",
      "epoch 132: loss 0.3117748200893402\n",
      "epoch 133: loss 0.3402663767337799\n",
      "epoch 134: loss 0.3204592764377594\n",
      "epoch 135: loss 0.2505818009376526\n",
      "epoch 136: loss 0.2296375334262848\n",
      "epoch 0: loss 0.35368889570236206\n",
      "epoch 1: loss 0.33111217617988586\n",
      "epoch 2: loss 0.395111620426178\n",
      "epoch 3: loss 0.3672516942024231\n",
      "epoch 4: loss 0.3466681241989136\n",
      "epoch 5: loss 0.25117331743240356\n",
      "epoch 6: loss 0.28424230217933655\n",
      "epoch 7: loss 0.22675243020057678\n",
      "epoch 8: loss 0.4259571135044098\n",
      "epoch 9: loss 0.4465819001197815\n",
      "epoch 10: loss 0.361931174993515\n",
      "epoch 11: loss 0.29018744826316833\n",
      "epoch 12: loss 0.2965679168701172\n",
      "epoch 13: loss 0.4072857201099396\n",
      "epoch 14: loss 0.407382071018219\n",
      "epoch 15: loss 0.27944865822792053\n",
      "epoch 16: loss 0.34596312046051025\n",
      "epoch 17: loss 0.3500373661518097\n",
      "epoch 18: loss 0.40758389234542847\n",
      "epoch 19: loss 0.2837737202644348\n",
      "epoch 20: loss 0.3241531550884247\n",
      "epoch 21: loss 0.3511885702610016\n",
      "epoch 22: loss 0.3252832889556885\n",
      "epoch 23: loss 0.37370365858078003\n",
      "epoch 24: loss 0.3417844772338867\n",
      "epoch 25: loss 0.4077049791812897\n",
      "epoch 26: loss 0.3567754924297333\n",
      "epoch 27: loss 0.34115415811538696\n",
      "epoch 28: loss 0.39582741260528564\n",
      "epoch 29: loss 0.31558918952941895\n",
      "epoch 30: loss 0.2955613434314728\n",
      "epoch 31: loss 0.3384988605976105\n",
      "epoch 32: loss 0.3484448194503784\n",
      "epoch 33: loss 0.33931419253349304\n",
      "epoch 34: loss 0.3823714554309845\n",
      "epoch 35: loss 0.2550981938838959\n",
      "epoch 36: loss 0.32557451725006104\n",
      "epoch 37: loss 0.32014596462249756\n",
      "epoch 38: loss 0.2221362590789795\n",
      "epoch 39: loss 0.30774807929992676\n",
      "epoch 40: loss 0.299184650182724\n",
      "epoch 41: loss 0.27782899141311646\n",
      "epoch 42: loss 0.2618732750415802\n",
      "epoch 43: loss 0.33033275604248047\n",
      "epoch 44: loss 0.335842490196228\n",
      "epoch 45: loss 0.2745082974433899\n",
      "epoch 46: loss 0.288827121257782\n",
      "epoch 47: loss 0.28075167536735535\n",
      "epoch 48: loss 0.2533462643623352\n",
      "epoch 49: loss 0.33565255999565125\n",
      "epoch 50: loss 0.2784576117992401\n",
      "epoch 51: loss 0.41170036792755127\n",
      "epoch 52: loss 0.2828880846500397\n",
      "epoch 53: loss 0.37114596366882324\n",
      "epoch 54: loss 0.34878700971603394\n",
      "epoch 55: loss 0.2998196482658386\n",
      "epoch 56: loss 0.34983423352241516\n",
      "epoch 57: loss 0.2407733052968979\n",
      "epoch 58: loss 0.32538527250289917\n",
      "epoch 59: loss 0.2594761848449707\n",
      "epoch 60: loss 0.33040547370910645\n",
      "epoch 61: loss 0.21752890944480896\n",
      "epoch 62: loss 0.23657342791557312\n",
      "epoch 63: loss 0.340125173330307\n",
      "epoch 64: loss 0.2817751467227936\n",
      "epoch 65: loss 0.39441215991973877\n",
      "epoch 66: loss 0.33146655559539795\n",
      "epoch 67: loss 0.37482553720474243\n",
      "epoch 68: loss 0.3591782748699188\n",
      "epoch 69: loss 0.40915536880493164\n",
      "epoch 70: loss 0.3752499520778656\n",
      "epoch 71: loss 0.3258410692214966\n",
      "epoch 72: loss 0.2564971446990967\n",
      "epoch 73: loss 0.3297237753868103\n",
      "epoch 74: loss 0.2769485116004944\n",
      "epoch 75: loss 0.3401244878768921\n",
      "epoch 76: loss 0.30286893248558044\n",
      "epoch 77: loss 0.4027700424194336\n",
      "epoch 78: loss 0.30611634254455566\n",
      "epoch 79: loss 0.35873132944107056\n",
      "epoch 80: loss 0.3833514451980591\n",
      "epoch 81: loss 0.2676711678504944\n",
      "epoch 82: loss 0.323367714881897\n",
      "epoch 83: loss 0.40602898597717285\n",
      "epoch 84: loss 0.2687070369720459\n",
      "epoch 85: loss 0.2979205250740051\n",
      "epoch 86: loss 0.34216296672821045\n",
      "epoch 87: loss 0.33941519260406494\n",
      "epoch 88: loss 0.2702353596687317\n",
      "epoch 89: loss 0.37004703283309937\n",
      "epoch 90: loss 0.3134618401527405\n",
      "epoch 91: loss 0.4175029397010803\n",
      "epoch 92: loss 0.3375299572944641\n",
      "epoch 93: loss 0.31281355023384094\n",
      "epoch 94: loss 0.2945459485054016\n",
      "epoch 95: loss 0.27187079191207886\n",
      "epoch 96: loss 0.2394646257162094\n",
      "epoch 97: loss 0.2938748598098755\n",
      "epoch 98: loss 0.31485864520072937\n",
      "epoch 99: loss 0.3624659776687622\n",
      "epoch 100: loss 0.5044981837272644\n",
      "epoch 101: loss 0.3135843873023987\n",
      "epoch 102: loss 0.2724944055080414\n",
      "epoch 103: loss 0.3284510374069214\n",
      "epoch 104: loss 0.2665582597255707\n",
      "epoch 105: loss 0.3839922547340393\n",
      "epoch 106: loss 0.3212217092514038\n",
      "epoch 107: loss 0.2856971025466919\n",
      "epoch 108: loss 0.36172083020210266\n",
      "epoch 109: loss 0.35571935772895813\n",
      "epoch 110: loss 0.3479849398136139\n",
      "epoch 111: loss 0.3699488937854767\n",
      "epoch 112: loss 0.3731938600540161\n",
      "epoch 113: loss 0.28248125314712524\n",
      "epoch 114: loss 0.16741661727428436\n",
      "epoch 115: loss 0.40796804428100586\n",
      "epoch 116: loss 0.30442607402801514\n",
      "epoch 117: loss 0.2708333730697632\n",
      "epoch 118: loss 0.32662510871887207\n",
      "epoch 119: loss 0.3887448310852051\n",
      "epoch 120: loss 0.26508522033691406\n",
      "epoch 121: loss 0.3847805857658386\n",
      "epoch 122: loss 0.24319031834602356\n",
      "epoch 123: loss 0.3926251232624054\n",
      "epoch 124: loss 0.3062843680381775\n",
      "epoch 125: loss 0.3395844101905823\n",
      "epoch 126: loss 0.33937644958496094\n",
      "epoch 127: loss 0.37296369671821594\n",
      "epoch 128: loss 0.2760745882987976\n",
      "epoch 129: loss 0.34571507573127747\n",
      "epoch 130: loss 0.36758875846862793\n",
      "epoch 131: loss 0.32289814949035645\n",
      "epoch 132: loss 0.31567269563674927\n",
      "epoch 133: loss 0.3543606102466583\n",
      "epoch 134: loss 0.3237738609313965\n",
      "epoch 135: loss 0.251793771982193\n",
      "epoch 136: loss 0.2301931083202362\n",
      "epoch 0: loss 0.3416280150413513\n",
      "epoch 1: loss 0.333597868680954\n",
      "epoch 2: loss 0.3777070641517639\n",
      "epoch 3: loss 0.37667316198349\n",
      "epoch 4: loss 0.32878005504608154\n",
      "epoch 5: loss 0.2523226737976074\n",
      "epoch 6: loss 0.28481993079185486\n",
      "epoch 7: loss 0.2245998978614807\n",
      "epoch 8: loss 0.4184168577194214\n",
      "epoch 9: loss 0.43088197708129883\n",
      "epoch 10: loss 0.3472328186035156\n",
      "epoch 11: loss 0.27987271547317505\n",
      "epoch 12: loss 0.30581435561180115\n",
      "epoch 13: loss 0.40648019313812256\n",
      "epoch 14: loss 0.3940012454986572\n",
      "epoch 15: loss 0.27784499526023865\n",
      "epoch 16: loss 0.34973958134651184\n",
      "epoch 17: loss 0.3421427607536316\n",
      "epoch 18: loss 0.39967405796051025\n",
      "epoch 19: loss 0.2747926414012909\n",
      "epoch 20: loss 0.31995582580566406\n",
      "epoch 21: loss 0.3430595397949219\n",
      "epoch 22: loss 0.32413250207901\n",
      "epoch 23: loss 0.37505120038986206\n",
      "epoch 24: loss 0.34560760855674744\n",
      "epoch 25: loss 0.39354902505874634\n",
      "epoch 26: loss 0.3624063730239868\n",
      "epoch 27: loss 0.34663325548171997\n",
      "epoch 28: loss 0.39291784167289734\n",
      "epoch 29: loss 0.31628286838531494\n",
      "epoch 30: loss 0.28077802062034607\n",
      "epoch 31: loss 0.33058834075927734\n",
      "epoch 32: loss 0.34684011340141296\n",
      "epoch 33: loss 0.33992740511894226\n",
      "epoch 34: loss 0.38495123386383057\n",
      "epoch 35: loss 0.2550176680088043\n",
      "epoch 36: loss 0.3283134996891022\n",
      "epoch 37: loss 0.325482577085495\n",
      "epoch 38: loss 0.2244367152452469\n",
      "epoch 39: loss 0.31353500485420227\n",
      "epoch 40: loss 0.30186912417411804\n",
      "epoch 41: loss 0.2770341634750366\n",
      "epoch 42: loss 0.2628564238548279\n",
      "epoch 43: loss 0.3387002944946289\n",
      "epoch 44: loss 0.33504918217658997\n",
      "epoch 45: loss 0.2761469781398773\n",
      "epoch 46: loss 0.2899963855743408\n",
      "epoch 47: loss 0.27949318289756775\n",
      "epoch 48: loss 0.2505268454551697\n",
      "epoch 49: loss 0.3299393057823181\n",
      "epoch 50: loss 0.2803489565849304\n",
      "epoch 51: loss 0.41276392340660095\n",
      "epoch 52: loss 0.28621673583984375\n",
      "epoch 53: loss 0.368678480386734\n",
      "epoch 54: loss 0.34677019715309143\n",
      "epoch 55: loss 0.29951369762420654\n",
      "epoch 56: loss 0.34969204664230347\n",
      "epoch 57: loss 0.24405445158481598\n",
      "epoch 58: loss 0.32557862997055054\n",
      "epoch 59: loss 0.2637421488761902\n",
      "epoch 60: loss 0.33116212487220764\n",
      "epoch 61: loss 0.22016817331314087\n",
      "epoch 62: loss 0.2399885356426239\n",
      "epoch 63: loss 0.341780424118042\n",
      "epoch 64: loss 0.2806299328804016\n",
      "epoch 65: loss 0.39549708366394043\n",
      "epoch 66: loss 0.33110511302948\n",
      "epoch 67: loss 0.3741511106491089\n",
      "epoch 68: loss 0.35754427313804626\n",
      "epoch 69: loss 0.4121466279029846\n",
      "epoch 70: loss 0.37116363644599915\n",
      "epoch 71: loss 0.33258605003356934\n",
      "epoch 72: loss 0.25817158818244934\n",
      "epoch 73: loss 0.3265495002269745\n",
      "epoch 74: loss 0.27386990189552307\n",
      "epoch 75: loss 0.34082508087158203\n",
      "epoch 76: loss 0.3046857416629791\n",
      "epoch 77: loss 0.3996056318283081\n",
      "epoch 78: loss 0.3036115765571594\n",
      "epoch 79: loss 0.3608052134513855\n",
      "epoch 80: loss 0.38692617416381836\n",
      "epoch 81: loss 0.2670554220676422\n",
      "epoch 82: loss 0.3192344307899475\n",
      "epoch 83: loss 0.4084951877593994\n",
      "epoch 84: loss 0.2708694338798523\n",
      "epoch 85: loss 0.2954515218734741\n",
      "epoch 86: loss 0.34285208582878113\n",
      "epoch 87: loss 0.3386828303337097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 88: loss 0.2720802128314972\n",
      "epoch 89: loss 0.37544283270835876\n",
      "epoch 90: loss 0.3197101056575775\n",
      "epoch 91: loss 0.4356890022754669\n",
      "epoch 92: loss 0.3430621027946472\n",
      "epoch 93: loss 0.31448933482170105\n",
      "epoch 94: loss 0.2949090003967285\n",
      "epoch 95: loss 0.26787179708480835\n",
      "epoch 96: loss 0.24846044182777405\n",
      "epoch 97: loss 0.2971915900707245\n",
      "epoch 98: loss 0.3125196695327759\n",
      "epoch 99: loss 0.36129045486450195\n",
      "epoch 100: loss 0.48979872465133667\n",
      "epoch 101: loss 0.30886539816856384\n",
      "epoch 102: loss 0.2704884111881256\n",
      "epoch 103: loss 0.3317188620567322\n",
      "epoch 104: loss 0.26078516244888306\n",
      "epoch 105: loss 0.376644492149353\n",
      "epoch 106: loss 0.3164857029914856\n",
      "epoch 107: loss 0.2791290581226349\n",
      "epoch 108: loss 0.36226004362106323\n",
      "epoch 109: loss 0.35837578773498535\n",
      "epoch 110: loss 0.3493829667568207\n",
      "epoch 111: loss 0.36424610018730164\n",
      "epoch 112: loss 0.36720332503318787\n",
      "epoch 113: loss 0.2767902612686157\n",
      "epoch 114: loss 0.1643429398536682\n",
      "epoch 115: loss 0.4018046259880066\n",
      "epoch 116: loss 0.2951667904853821\n",
      "epoch 117: loss 0.26276829838752747\n",
      "epoch 118: loss 0.32567229866981506\n",
      "epoch 119: loss 0.389554500579834\n",
      "epoch 120: loss 0.26233357191085815\n",
      "epoch 121: loss 0.36973342299461365\n",
      "epoch 122: loss 0.24077993631362915\n",
      "epoch 123: loss 0.3805503249168396\n",
      "epoch 124: loss 0.32730305194854736\n",
      "epoch 125: loss 0.34029585123062134\n",
      "epoch 126: loss 0.3727649748325348\n",
      "epoch 127: loss 0.40838223695755005\n",
      "epoch 128: loss 0.25882548093795776\n",
      "epoch 129: loss 0.349206805229187\n",
      "epoch 130: loss 0.36326923966407776\n",
      "epoch 131: loss 0.3230653405189514\n",
      "epoch 132: loss 0.31885436177253723\n",
      "epoch 133: loss 0.34048956632614136\n",
      "epoch 134: loss 0.31463462114334106\n",
      "epoch 135: loss 0.25294673442840576\n",
      "epoch 136: loss 0.232761412858963\n",
      "epoch 0: loss 0.35458850860595703\n",
      "epoch 1: loss 0.32918012142181396\n",
      "epoch 2: loss 0.4086815416812897\n",
      "epoch 3: loss 0.36048948764801025\n",
      "epoch 4: loss 0.3125241994857788\n",
      "epoch 5: loss 0.2627391815185547\n",
      "epoch 6: loss 0.2771669030189514\n",
      "epoch 7: loss 0.22136427462100983\n",
      "epoch 8: loss 0.405265748500824\n",
      "epoch 9: loss 0.40788328647613525\n",
      "epoch 10: loss 0.33599966764450073\n",
      "epoch 11: loss 0.27828139066696167\n",
      "epoch 12: loss 0.297332227230072\n",
      "epoch 13: loss 0.4216812252998352\n",
      "epoch 14: loss 0.36165153980255127\n",
      "epoch 15: loss 0.28356367349624634\n",
      "epoch 16: loss 0.35128623247146606\n",
      "epoch 17: loss 0.33180731534957886\n",
      "epoch 18: loss 0.38353538513183594\n",
      "epoch 19: loss 0.26370757818222046\n",
      "epoch 20: loss 0.3035408556461334\n",
      "epoch 21: loss 0.33689773082733154\n",
      "epoch 22: loss 0.32142001390457153\n",
      "epoch 23: loss 0.3702729046344757\n",
      "epoch 24: loss 0.3359438180923462\n",
      "epoch 25: loss 0.40070652961730957\n",
      "epoch 26: loss 0.35048121213912964\n",
      "epoch 27: loss 0.3411199450492859\n",
      "epoch 28: loss 0.3979867398738861\n",
      "epoch 29: loss 0.3172576427459717\n",
      "epoch 30: loss 0.29418879747390747\n",
      "epoch 31: loss 0.3361959457397461\n",
      "epoch 32: loss 0.3517366945743561\n",
      "epoch 33: loss 0.3321601450443268\n",
      "epoch 34: loss 0.3789520859718323\n",
      "epoch 35: loss 0.259533166885376\n",
      "epoch 36: loss 0.32322683930397034\n",
      "epoch 37: loss 0.3270730674266815\n",
      "epoch 38: loss 0.22624211013317108\n",
      "epoch 39: loss 0.3126174211502075\n",
      "epoch 40: loss 0.30205458402633667\n",
      "epoch 41: loss 0.27871906757354736\n",
      "epoch 42: loss 0.2575322985649109\n",
      "epoch 43: loss 0.3367239236831665\n",
      "epoch 44: loss 0.3351379334926605\n",
      "epoch 45: loss 0.2735241651535034\n",
      "epoch 46: loss 0.2866872549057007\n",
      "epoch 47: loss 0.2816027104854584\n",
      "epoch 48: loss 0.24892142415046692\n",
      "epoch 49: loss 0.3344160318374634\n",
      "epoch 50: loss 0.28033310174942017\n",
      "epoch 51: loss 0.4104638695716858\n",
      "epoch 52: loss 0.28543490171432495\n",
      "epoch 53: loss 0.36963367462158203\n",
      "epoch 54: loss 0.34537720680236816\n",
      "epoch 55: loss 0.2985009253025055\n",
      "epoch 56: loss 0.34931331872940063\n",
      "epoch 57: loss 0.23995192348957062\n",
      "epoch 58: loss 0.32556402683258057\n",
      "epoch 59: loss 0.26428699493408203\n",
      "epoch 60: loss 0.3276875615119934\n",
      "epoch 61: loss 0.220434308052063\n",
      "epoch 62: loss 0.23877684772014618\n",
      "epoch 63: loss 0.33944931626319885\n",
      "epoch 64: loss 0.2820865511894226\n",
      "epoch 65: loss 0.393515408039093\n",
      "epoch 66: loss 0.3289686441421509\n",
      "epoch 67: loss 0.3784724771976471\n",
      "epoch 68: loss 0.35824376344680786\n",
      "epoch 69: loss 0.40732884407043457\n",
      "epoch 70: loss 0.37813225388526917\n",
      "epoch 71: loss 0.3243960738182068\n",
      "epoch 72: loss 0.2502490282058716\n",
      "epoch 73: loss 0.33080393075942993\n",
      "epoch 74: loss 0.2768119275569916\n",
      "epoch 75: loss 0.34385329484939575\n",
      "epoch 76: loss 0.30349934101104736\n",
      "epoch 77: loss 0.4060039520263672\n",
      "epoch 78: loss 0.30469170212745667\n",
      "epoch 79: loss 0.35934725403785706\n",
      "epoch 80: loss 0.3844694197177887\n",
      "epoch 81: loss 0.2668241262435913\n",
      "epoch 82: loss 0.32150447368621826\n",
      "epoch 83: loss 0.4148338735103607\n",
      "epoch 84: loss 0.2587290406227112\n",
      "epoch 85: loss 0.30032879114151\n",
      "epoch 86: loss 0.3450602889060974\n",
      "epoch 87: loss 0.33793383836746216\n",
      "epoch 88: loss 0.2702369689941406\n",
      "epoch 89: loss 0.36678454279899597\n",
      "epoch 90: loss 0.31268131732940674\n",
      "epoch 91: loss 0.40547800064086914\n",
      "epoch 92: loss 0.3266245126724243\n",
      "epoch 93: loss 0.3129817247390747\n",
      "epoch 94: loss 0.2918933629989624\n",
      "epoch 95: loss 0.273759663105011\n",
      "epoch 96: loss 0.23238250613212585\n",
      "epoch 97: loss 0.29525691270828247\n",
      "epoch 98: loss 0.3144043982028961\n",
      "epoch 99: loss 0.36357152462005615\n",
      "epoch 100: loss 0.5032007694244385\n",
      "epoch 101: loss 0.3119894862174988\n",
      "epoch 102: loss 0.27492350339889526\n",
      "epoch 103: loss 0.33898061513900757\n",
      "epoch 104: loss 0.27081507444381714\n",
      "epoch 105: loss 0.381104052066803\n",
      "epoch 106: loss 0.32904839515686035\n",
      "epoch 107: loss 0.2882991433143616\n",
      "epoch 108: loss 0.3660748600959778\n",
      "epoch 109: loss 0.3566601872444153\n",
      "epoch 110: loss 0.35256636142730713\n",
      "epoch 111: loss 0.3618890643119812\n",
      "epoch 112: loss 0.3637400269508362\n",
      "epoch 113: loss 0.2817545533180237\n",
      "epoch 114: loss 0.17711669206619263\n",
      "epoch 115: loss 0.4002859890460968\n",
      "epoch 116: loss 0.29858988523483276\n",
      "epoch 117: loss 0.26125603914260864\n",
      "epoch 118: loss 0.3269585371017456\n",
      "epoch 119: loss 0.3812961280345917\n",
      "epoch 120: loss 0.26224246621131897\n",
      "epoch 121: loss 0.37372398376464844\n",
      "epoch 122: loss 0.24676299095153809\n",
      "epoch 123: loss 0.3783053755760193\n",
      "epoch 124: loss 0.32656151056289673\n",
      "epoch 125: loss 0.34140074253082275\n",
      "epoch 126: loss 0.3701210021972656\n",
      "epoch 127: loss 0.39748722314834595\n",
      "epoch 128: loss 0.2543379068374634\n",
      "epoch 129: loss 0.34481921792030334\n",
      "epoch 130: loss 0.36178600788116455\n",
      "epoch 131: loss 0.33647966384887695\n",
      "epoch 132: loss 0.31129467487335205\n",
      "epoch 133: loss 0.3413482904434204\n",
      "epoch 134: loss 0.31759458780288696\n",
      "epoch 135: loss 0.24794307351112366\n",
      "epoch 136: loss 0.22973978519439697\n",
      "epoch 0: loss 0.35372427105903625\n",
      "epoch 1: loss 0.3294438421726227\n",
      "epoch 2: loss 0.3975856602191925\n",
      "epoch 3: loss 0.3602447211742401\n",
      "epoch 4: loss 0.3434443175792694\n",
      "epoch 5: loss 0.25261032581329346\n",
      "epoch 6: loss 0.27971047163009644\n",
      "epoch 7: loss 0.22522610425949097\n",
      "epoch 8: loss 0.4228571951389313\n",
      "epoch 9: loss 0.44450992345809937\n",
      "epoch 10: loss 0.35625386238098145\n",
      "epoch 11: loss 0.288026362657547\n",
      "epoch 12: loss 0.30027270317077637\n",
      "epoch 13: loss 0.40880531072616577\n",
      "epoch 14: loss 0.394454687833786\n",
      "epoch 15: loss 0.28655487298965454\n",
      "epoch 16: loss 0.3478184938430786\n",
      "epoch 17: loss 0.3420543670654297\n",
      "epoch 18: loss 0.3956848680973053\n",
      "epoch 19: loss 0.27452099323272705\n",
      "epoch 20: loss 0.31789708137512207\n",
      "epoch 21: loss 0.34731268882751465\n",
      "epoch 22: loss 0.3230980336666107\n",
      "epoch 23: loss 0.3750314712524414\n",
      "epoch 24: loss 0.34941402077674866\n",
      "epoch 25: loss 0.3910842537879944\n",
      "epoch 26: loss 0.3602091670036316\n",
      "epoch 27: loss 0.34592193365097046\n",
      "epoch 28: loss 0.3889833092689514\n",
      "epoch 29: loss 0.3150758743286133\n",
      "epoch 30: loss 0.27604204416275024\n",
      "epoch 31: loss 0.3306286334991455\n",
      "epoch 32: loss 0.3439980745315552\n",
      "epoch 33: loss 0.3391721844673157\n",
      "epoch 34: loss 0.38466599583625793\n",
      "epoch 35: loss 0.2574072480201721\n",
      "epoch 36: loss 0.33236122131347656\n",
      "epoch 37: loss 0.3341728746891022\n",
      "epoch 38: loss 0.23202218115329742\n",
      "epoch 39: loss 0.31624001264572144\n",
      "epoch 40: loss 0.30802276730537415\n",
      "epoch 41: loss 0.278461217880249\n",
      "epoch 42: loss 0.2625444531440735\n",
      "epoch 43: loss 0.3449019193649292\n",
      "epoch 44: loss 0.3336123824119568\n",
      "epoch 45: loss 0.2756688892841339\n",
      "epoch 46: loss 0.29313579201698303\n",
      "epoch 47: loss 0.28270405530929565\n",
      "epoch 48: loss 0.24919191002845764\n",
      "epoch 49: loss 0.32428252696990967\n",
      "epoch 50: loss 0.278892457485199\n",
      "epoch 51: loss 0.41055524349212646\n",
      "epoch 52: loss 0.2875191569328308\n",
      "epoch 53: loss 0.36769431829452515\n",
      "epoch 54: loss 0.34784969687461853\n",
      "epoch 55: loss 0.29997408390045166\n",
      "epoch 56: loss 0.3482329845428467\n",
      "epoch 57: loss 0.24342796206474304\n",
      "epoch 58: loss 0.32429268956184387\n",
      "epoch 59: loss 0.26642724871635437\n",
      "epoch 60: loss 0.32800135016441345\n",
      "epoch 61: loss 0.22018930315971375\n",
      "epoch 62: loss 0.2384990155696869\n",
      "epoch 63: loss 0.3356195390224457\n",
      "epoch 64: loss 0.2819336950778961\n",
      "epoch 65: loss 0.39518624544143677\n",
      "epoch 66: loss 0.3311153054237366\n",
      "epoch 67: loss 0.38001111149787903\n",
      "epoch 68: loss 0.35721755027770996\n",
      "epoch 69: loss 0.413875550031662\n",
      "epoch 70: loss 0.3722518980503082\n",
      "epoch 71: loss 0.3280003070831299\n",
      "epoch 72: loss 0.25478395819664\n",
      "epoch 73: loss 0.3232433795928955\n",
      "epoch 74: loss 0.26912057399749756\n",
      "epoch 75: loss 0.34343522787094116\n",
      "epoch 76: loss 0.3052091598510742\n",
      "epoch 77: loss 0.4017987549304962\n",
      "epoch 78: loss 0.3015569746494293\n",
      "epoch 79: loss 0.3446005582809448\n",
      "epoch 80: loss 0.388033390045166\n",
      "epoch 81: loss 0.26922187209129333\n",
      "epoch 82: loss 0.3236883878707886\n",
      "epoch 83: loss 0.37858718633651733\n",
      "epoch 84: loss 0.3170818090438843\n",
      "epoch 85: loss 0.2979099452495575\n",
      "epoch 86: loss 0.3442205786705017\n",
      "epoch 87: loss 0.3317576050758362\n",
      "epoch 88: loss 0.29244643449783325\n",
      "epoch 89: loss 0.39339208602905273\n",
      "epoch 90: loss 0.32814109325408936\n",
      "epoch 91: loss 0.44152846932411194\n",
      "epoch 92: loss 0.3584766387939453\n",
      "epoch 93: loss 0.31287682056427\n",
      "epoch 94: loss 0.2934049963951111\n",
      "epoch 95: loss 0.23479685187339783\n",
      "epoch 96: loss 0.2736385464668274\n",
      "epoch 97: loss 0.3012576103210449\n",
      "epoch 98: loss 0.3125462830066681\n",
      "epoch 99: loss 0.3536786437034607\n",
      "epoch 100: loss 0.4734560549259186\n",
      "epoch 101: loss 0.3058675527572632\n",
      "epoch 102: loss 0.26812252402305603\n",
      "epoch 103: loss 0.34069639444351196\n",
      "epoch 104: loss 0.26738208532333374\n",
      "epoch 105: loss 0.37297114729881287\n",
      "epoch 106: loss 0.3248218297958374\n",
      "epoch 107: loss 0.27975666522979736\n",
      "epoch 108: loss 0.3544710874557495\n",
      "epoch 109: loss 0.3563135266304016\n",
      "epoch 110: loss 0.35064250230789185\n",
      "epoch 111: loss 0.37996599078178406\n",
      "epoch 112: loss 0.36536073684692383\n",
      "epoch 113: loss 0.2833114266395569\n",
      "epoch 114: loss 0.17403921484947205\n",
      "epoch 115: loss 0.40846049785614014\n",
      "epoch 116: loss 0.29922524094581604\n",
      "epoch 117: loss 0.2585344910621643\n",
      "epoch 118: loss 0.3222543001174927\n",
      "epoch 119: loss 0.3844412565231323\n",
      "epoch 120: loss 0.26381832361221313\n",
      "epoch 121: loss 0.36175358295440674\n",
      "epoch 122: loss 0.24466371536254883\n",
      "epoch 123: loss 0.38127419352531433\n",
      "epoch 124: loss 0.3261287808418274\n",
      "epoch 125: loss 0.33652228116989136\n",
      "epoch 126: loss 0.37765538692474365\n",
      "epoch 127: loss 0.42323794960975647\n",
      "epoch 128: loss 0.27507734298706055\n",
      "epoch 129: loss 0.34551936388015747\n",
      "epoch 130: loss 0.39310434460639954\n",
      "epoch 131: loss 0.3213571310043335\n",
      "epoch 132: loss 0.3179813027381897\n",
      "epoch 133: loss 0.34886687994003296\n",
      "epoch 134: loss 0.3220463693141937\n",
      "epoch 135: loss 0.26831769943237305\n",
      "epoch 136: loss 0.2603248953819275\n",
      "epoch 0: loss 0.35728585720062256\n",
      "epoch 1: loss 0.33605778217315674\n",
      "epoch 2: loss 0.3918454647064209\n",
      "epoch 3: loss 0.358650803565979\n",
      "epoch 4: loss 0.3150377869606018\n",
      "epoch 5: loss 0.2607612907886505\n",
      "epoch 6: loss 0.27658167481422424\n",
      "epoch 7: loss 0.21879178285598755\n",
      "epoch 8: loss 0.40356940031051636\n",
      "epoch 9: loss 0.40403956174850464\n",
      "epoch 10: loss 0.323458731174469\n",
      "epoch 11: loss 0.263301819562912\n",
      "epoch 12: loss 0.30182021856307983\n",
      "epoch 13: loss 0.41323959827423096\n",
      "epoch 14: loss 0.38166868686676025\n",
      "epoch 15: loss 0.27239423990249634\n",
      "epoch 16: loss 0.35013774037361145\n",
      "epoch 17: loss 0.3383050262928009\n",
      "epoch 18: loss 0.40607088804244995\n",
      "epoch 19: loss 0.26427581906318665\n",
      "epoch 20: loss 0.3076915442943573\n",
      "epoch 21: loss 0.3411426842212677\n",
      "epoch 22: loss 0.3211718797683716\n",
      "epoch 23: loss 0.37168392539024353\n",
      "epoch 24: loss 0.3332197964191437\n",
      "epoch 25: loss 0.4141054153442383\n",
      "epoch 26: loss 0.3322405219078064\n",
      "epoch 27: loss 0.3409775197505951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: loss 0.39500874280929565\n",
      "epoch 29: loss 0.3187498152256012\n",
      "epoch 30: loss 0.2912254333496094\n",
      "epoch 31: loss 0.3317170739173889\n",
      "epoch 32: loss 0.3431183397769928\n",
      "epoch 33: loss 0.33981895446777344\n",
      "epoch 34: loss 0.3805761933326721\n",
      "epoch 35: loss 0.2583531439304352\n",
      "epoch 36: loss 0.3269386887550354\n",
      "epoch 37: loss 0.32295623421669006\n",
      "epoch 38: loss 0.2285698652267456\n",
      "epoch 39: loss 0.3145425319671631\n",
      "epoch 40: loss 0.30731743574142456\n",
      "epoch 41: loss 0.27899640798568726\n",
      "epoch 42: loss 0.24898648262023926\n",
      "epoch 43: loss 0.3312680125236511\n",
      "epoch 44: loss 0.33386415243148804\n",
      "epoch 45: loss 0.27291902899742126\n",
      "epoch 46: loss 0.2893338203430176\n",
      "epoch 47: loss 0.27936601638793945\n",
      "epoch 48: loss 0.2525605261325836\n",
      "epoch 49: loss 0.33670222759246826\n",
      "epoch 50: loss 0.2800825834274292\n",
      "epoch 51: loss 0.4186728298664093\n",
      "epoch 52: loss 0.29410916566848755\n",
      "epoch 53: loss 0.3743478059768677\n",
      "epoch 54: loss 0.3474254012107849\n",
      "epoch 55: loss 0.30255287885665894\n",
      "epoch 56: loss 0.3466782569885254\n",
      "epoch 57: loss 0.24075758457183838\n",
      "epoch 58: loss 0.3236251473426819\n",
      "epoch 59: loss 0.26791656017303467\n",
      "epoch 60: loss 0.3268243074417114\n",
      "epoch 61: loss 0.22047625482082367\n",
      "epoch 62: loss 0.23916950821876526\n",
      "epoch 63: loss 0.3364053964614868\n",
      "epoch 64: loss 0.2793703079223633\n",
      "epoch 65: loss 0.3925844430923462\n",
      "epoch 66: loss 0.3283557891845703\n",
      "epoch 67: loss 0.3803781569004059\n",
      "epoch 68: loss 0.3562791645526886\n",
      "epoch 69: loss 0.41784003376960754\n",
      "epoch 70: loss 0.36954694986343384\n",
      "epoch 71: loss 0.33528563380241394\n",
      "epoch 72: loss 0.2610313296318054\n",
      "epoch 73: loss 0.32159286737442017\n",
      "epoch 74: loss 0.2704835832118988\n",
      "epoch 75: loss 0.3425806164741516\n",
      "epoch 76: loss 0.3057992458343506\n",
      "epoch 77: loss 0.40066272020339966\n",
      "epoch 78: loss 0.3030032515525818\n",
      "epoch 79: loss 0.35410749912261963\n",
      "epoch 80: loss 0.39270490407943726\n",
      "epoch 81: loss 0.2718420922756195\n",
      "epoch 82: loss 0.3248767554759979\n",
      "epoch 83: loss 0.37864112854003906\n",
      "epoch 84: loss 0.3349546194076538\n",
      "epoch 85: loss 0.2970420718193054\n",
      "epoch 86: loss 0.34173107147216797\n",
      "epoch 87: loss 0.33467262983322144\n",
      "epoch 88: loss 0.2874438762664795\n",
      "epoch 89: loss 0.38567429780960083\n",
      "epoch 90: loss 0.3197566866874695\n",
      "epoch 91: loss 0.4421606659889221\n",
      "epoch 92: loss 0.3707961440086365\n",
      "epoch 93: loss 0.31029772758483887\n",
      "epoch 94: loss 0.31159138679504395\n",
      "epoch 95: loss 0.22109191119670868\n",
      "epoch 96: loss 0.25805655121803284\n",
      "epoch 97: loss 0.32437053322792053\n",
      "epoch 98: loss 0.33853498101234436\n",
      "epoch 99: loss 0.3389209806919098\n",
      "epoch 100: loss 0.453913152217865\n",
      "epoch 101: loss 0.309676855802536\n",
      "epoch 102: loss 0.26045531034469604\n",
      "epoch 103: loss 0.3087129592895508\n",
      "epoch 104: loss 0.25768691301345825\n",
      "epoch 105: loss 0.37042850255966187\n",
      "epoch 106: loss 0.3117824196815491\n",
      "epoch 107: loss 0.2741941213607788\n",
      "epoch 108: loss 0.34314149618148804\n",
      "epoch 109: loss 0.3527710437774658\n",
      "epoch 110: loss 0.3429757356643677\n",
      "epoch 111: loss 0.39503058791160583\n",
      "epoch 112: loss 0.37181001901626587\n",
      "epoch 113: loss 0.2729203402996063\n",
      "epoch 114: loss 0.15689323842525482\n",
      "epoch 115: loss 0.40164443850517273\n",
      "epoch 116: loss 0.2874518930912018\n",
      "epoch 117: loss 0.2486509531736374\n",
      "epoch 118: loss 0.32189321517944336\n",
      "epoch 119: loss 0.3963735103607178\n",
      "epoch 120: loss 0.2710333466529846\n",
      "epoch 121: loss 0.3562459945678711\n",
      "epoch 122: loss 0.24311941862106323\n",
      "epoch 123: loss 0.3911730647087097\n",
      "epoch 124: loss 0.298032283782959\n",
      "epoch 125: loss 0.33946043252944946\n",
      "epoch 126: loss 0.3490165174007416\n",
      "epoch 127: loss 0.36623990535736084\n",
      "epoch 128: loss 0.30100521445274353\n",
      "epoch 129: loss 0.33962029218673706\n",
      "epoch 130: loss 0.36896955966949463\n",
      "epoch 131: loss 0.32270348072052\n",
      "epoch 132: loss 0.32437068223953247\n",
      "epoch 133: loss 0.36912521719932556\n",
      "epoch 134: loss 0.3197525143623352\n",
      "epoch 135: loss 0.25261497497558594\n",
      "epoch 136: loss 0.2291628122329712\n",
      "epoch 0: loss 0.3361656069755554\n",
      "epoch 1: loss 0.33495908975601196\n",
      "epoch 2: loss 0.3750734329223633\n",
      "epoch 3: loss 0.3679625988006592\n",
      "epoch 4: loss 0.3162567913532257\n",
      "epoch 5: loss 0.25452229380607605\n",
      "epoch 6: loss 0.27615076303482056\n",
      "epoch 7: loss 0.22153931856155396\n",
      "epoch 8: loss 0.41746243834495544\n",
      "epoch 9: loss 0.43811115622520447\n",
      "epoch 10: loss 0.3393476903438568\n",
      "epoch 11: loss 0.2741304636001587\n",
      "epoch 12: loss 0.32056525349617004\n",
      "epoch 13: loss 0.40607723593711853\n",
      "epoch 14: loss 0.3931956887245178\n",
      "epoch 15: loss 0.27509233355522156\n",
      "epoch 16: loss 0.34219133853912354\n",
      "epoch 17: loss 0.35711830854415894\n",
      "epoch 18: loss 0.41576963663101196\n",
      "epoch 19: loss 0.2851119637489319\n",
      "epoch 20: loss 0.3227115869522095\n",
      "epoch 21: loss 0.34459808468818665\n",
      "epoch 22: loss 0.3270620107650757\n",
      "epoch 23: loss 0.38273856043815613\n",
      "epoch 24: loss 0.35007280111312866\n",
      "epoch 25: loss 0.4062349498271942\n",
      "epoch 26: loss 0.3522486090660095\n",
      "epoch 27: loss 0.34796634316444397\n",
      "epoch 28: loss 0.3935316205024719\n",
      "epoch 29: loss 0.31902825832366943\n",
      "epoch 30: loss 0.2725304067134857\n",
      "epoch 31: loss 0.3307402729988098\n",
      "epoch 32: loss 0.3468238115310669\n",
      "epoch 33: loss 0.346707820892334\n",
      "epoch 34: loss 0.39829862117767334\n",
      "epoch 35: loss 0.2511630058288574\n",
      "epoch 36: loss 0.3323466181755066\n",
      "epoch 37: loss 0.32545197010040283\n",
      "epoch 38: loss 0.22514790296554565\n",
      "epoch 39: loss 0.31557604670524597\n",
      "epoch 40: loss 0.3037151098251343\n",
      "epoch 41: loss 0.27733418345451355\n",
      "epoch 42: loss 0.2622005045413971\n",
      "epoch 43: loss 0.34034284949302673\n",
      "epoch 44: loss 0.3387787342071533\n",
      "epoch 45: loss 0.2781062126159668\n",
      "epoch 46: loss 0.29311931133270264\n",
      "epoch 47: loss 0.28360840678215027\n",
      "epoch 48: loss 0.2520752251148224\n",
      "epoch 49: loss 0.323517382144928\n",
      "epoch 50: loss 0.2774714529514313\n",
      "epoch 51: loss 0.4085383415222168\n",
      "epoch 52: loss 0.29124486446380615\n",
      "epoch 53: loss 0.3635709285736084\n",
      "epoch 54: loss 0.34405604004859924\n",
      "epoch 55: loss 0.29721498489379883\n",
      "epoch 56: loss 0.3510737121105194\n",
      "epoch 57: loss 0.2452368140220642\n",
      "epoch 58: loss 0.3234430253505707\n",
      "epoch 59: loss 0.2641194760799408\n",
      "epoch 60: loss 0.32822006940841675\n",
      "epoch 61: loss 0.21806205809116364\n",
      "epoch 62: loss 0.23836779594421387\n",
      "epoch 63: loss 0.34468477964401245\n",
      "epoch 64: loss 0.27995312213897705\n",
      "epoch 65: loss 0.3955407738685608\n",
      "epoch 66: loss 0.3288033604621887\n",
      "epoch 67: loss 0.37733370065689087\n",
      "epoch 68: loss 0.3628377914428711\n",
      "epoch 69: loss 0.40363848209381104\n",
      "epoch 70: loss 0.3847227394580841\n",
      "epoch 71: loss 0.31724071502685547\n",
      "epoch 72: loss 0.25035983324050903\n",
      "epoch 73: loss 0.33363842964172363\n",
      "epoch 74: loss 0.2726469039916992\n",
      "epoch 75: loss 0.34617364406585693\n",
      "epoch 76: loss 0.30525338649749756\n",
      "epoch 77: loss 0.4066445231437683\n",
      "epoch 78: loss 0.30345219373703003\n",
      "epoch 79: loss 0.33729249238967896\n",
      "epoch 80: loss 0.3805691599845886\n",
      "epoch 81: loss 0.26489925384521484\n",
      "epoch 82: loss 0.31853675842285156\n",
      "epoch 83: loss 0.3932316303253174\n",
      "epoch 84: loss 0.2776367664337158\n",
      "epoch 85: loss 0.2983359694480896\n",
      "epoch 86: loss 0.33757537603378296\n",
      "epoch 87: loss 0.33956193923950195\n",
      "epoch 88: loss 0.2690553367137909\n",
      "epoch 89: loss 0.3654506206512451\n",
      "epoch 90: loss 0.31210196018218994\n",
      "epoch 91: loss 0.430184543132782\n",
      "epoch 92: loss 0.36057746410369873\n",
      "epoch 93: loss 0.3120041787624359\n",
      "epoch 94: loss 0.2911890149116516\n",
      "epoch 95: loss 0.22536081075668335\n",
      "epoch 96: loss 0.255290687084198\n",
      "epoch 97: loss 0.3135315775871277\n",
      "epoch 98: loss 0.31803417205810547\n",
      "epoch 99: loss 0.33894282579421997\n",
      "epoch 100: loss 0.4583238959312439\n",
      "epoch 101: loss 0.3059873580932617\n",
      "epoch 102: loss 0.2601676285266876\n",
      "epoch 103: loss 0.32385730743408203\n",
      "epoch 104: loss 0.26079729199409485\n",
      "epoch 105: loss 0.3729974031448364\n",
      "epoch 106: loss 0.31953907012939453\n",
      "epoch 107: loss 0.27491140365600586\n",
      "epoch 108: loss 0.3497249484062195\n",
      "epoch 109: loss 0.3530620336532593\n",
      "epoch 110: loss 0.34725654125213623\n",
      "epoch 111: loss 0.379569947719574\n",
      "epoch 112: loss 0.3644478917121887\n",
      "epoch 113: loss 0.2773585319519043\n",
      "epoch 114: loss 0.16441388428211212\n",
      "epoch 115: loss 0.4004961848258972\n",
      "epoch 116: loss 0.29250258207321167\n",
      "epoch 117: loss 0.25639450550079346\n",
      "epoch 118: loss 0.3182629942893982\n",
      "epoch 119: loss 0.3872617185115814\n",
      "epoch 120: loss 0.267255574464798\n",
      "epoch 121: loss 0.3543950319290161\n",
      "epoch 122: loss 0.24058815836906433\n",
      "epoch 123: loss 0.3863852024078369\n",
      "epoch 124: loss 0.30105268955230713\n",
      "epoch 125: loss 0.3372448682785034\n",
      "epoch 126: loss 0.3536255955696106\n",
      "epoch 127: loss 0.37404394149780273\n",
      "epoch 128: loss 0.27596601843833923\n",
      "epoch 129: loss 0.3390391170978546\n",
      "epoch 130: loss 0.3652949333190918\n",
      "epoch 131: loss 0.32184070348739624\n",
      "epoch 132: loss 0.31525158882141113\n",
      "epoch 133: loss 0.3474266231060028\n",
      "epoch 134: loss 0.31072843074798584\n",
      "epoch 135: loss 0.24734321236610413\n",
      "epoch 136: loss 0.2249464988708496\n",
      "epoch 0: loss 0.3331570029258728\n",
      "epoch 1: loss 0.33736246824264526\n",
      "epoch 2: loss 0.3727284073829651\n",
      "epoch 3: loss 0.36358869075775146\n",
      "epoch 4: loss 0.31677183508872986\n",
      "epoch 5: loss 0.24518662691116333\n",
      "epoch 6: loss 0.27395933866500854\n",
      "epoch 7: loss 0.217067688703537\n",
      "epoch 8: loss 0.4169926941394806\n",
      "epoch 9: loss 0.4195610284805298\n",
      "epoch 10: loss 0.3297755718231201\n",
      "epoch 11: loss 0.26408064365386963\n",
      "epoch 12: loss 0.31832975149154663\n",
      "epoch 13: loss 0.4059368669986725\n",
      "epoch 14: loss 0.3905538320541382\n",
      "epoch 15: loss 0.26963162422180176\n",
      "epoch 16: loss 0.34487253427505493\n",
      "epoch 17: loss 0.35247233510017395\n",
      "epoch 18: loss 0.4178358316421509\n",
      "epoch 19: loss 0.2820205092430115\n",
      "epoch 20: loss 0.32351601123809814\n",
      "epoch 21: loss 0.34417450428009033\n",
      "epoch 22: loss 0.3253251910209656\n",
      "epoch 23: loss 0.38451826572418213\n",
      "epoch 24: loss 0.34702277183532715\n",
      "epoch 25: loss 0.42032718658447266\n",
      "epoch 26: loss 0.34719425439834595\n",
      "epoch 27: loss 0.3455251157283783\n",
      "epoch 28: loss 0.39729738235473633\n",
      "epoch 29: loss 0.3188494145870209\n",
      "epoch 30: loss 0.28121644258499146\n",
      "epoch 31: loss 0.32849952578544617\n",
      "epoch 32: loss 0.3509055972099304\n",
      "epoch 33: loss 0.3382801413536072\n",
      "epoch 34: loss 0.3891211748123169\n",
      "epoch 35: loss 0.2547625005245209\n",
      "epoch 36: loss 0.3297620713710785\n",
      "epoch 37: loss 0.3287069797515869\n",
      "epoch 38: loss 0.22893916070461273\n",
      "epoch 39: loss 0.3185412883758545\n",
      "epoch 40: loss 0.30115586519241333\n",
      "epoch 41: loss 0.2730981707572937\n",
      "epoch 42: loss 0.2638721466064453\n",
      "epoch 43: loss 0.34334081411361694\n",
      "epoch 44: loss 0.3412141799926758\n",
      "epoch 45: loss 0.2753702998161316\n",
      "epoch 46: loss 0.29463014006614685\n",
      "epoch 47: loss 0.28431466221809387\n",
      "epoch 48: loss 0.2493237555027008\n",
      "epoch 49: loss 0.3234344720840454\n",
      "epoch 50: loss 0.2788692116737366\n",
      "epoch 51: loss 0.406320184469223\n",
      "epoch 52: loss 0.2907257080078125\n",
      "epoch 53: loss 0.3642629384994507\n",
      "epoch 54: loss 0.3437088131904602\n",
      "epoch 55: loss 0.2985091805458069\n",
      "epoch 56: loss 0.347461462020874\n",
      "epoch 57: loss 0.24213004112243652\n",
      "epoch 58: loss 0.32550227642059326\n",
      "epoch 59: loss 0.2631077170372009\n",
      "epoch 60: loss 0.32614994049072266\n",
      "epoch 61: loss 0.21869516372680664\n",
      "epoch 62: loss 0.23802690207958221\n",
      "epoch 63: loss 0.3381651043891907\n",
      "epoch 64: loss 0.28111183643341064\n",
      "epoch 65: loss 0.39333945512771606\n",
      "epoch 66: loss 0.32926109433174133\n",
      "epoch 67: loss 0.378570556640625\n",
      "epoch 68: loss 0.35833874344825745\n",
      "epoch 69: loss 0.40712571144104004\n",
      "epoch 70: loss 0.3762887716293335\n",
      "epoch 71: loss 0.3230735957622528\n",
      "epoch 72: loss 0.25100940465927124\n",
      "epoch 73: loss 0.3274076282978058\n",
      "epoch 74: loss 0.2712247371673584\n",
      "epoch 75: loss 0.3460027575492859\n",
      "epoch 76: loss 0.30637580156326294\n",
      "epoch 77: loss 0.4057120680809021\n",
      "epoch 78: loss 0.3029770255088806\n",
      "epoch 79: loss 0.33562177419662476\n",
      "epoch 80: loss 0.38106775283813477\n",
      "epoch 81: loss 0.26654112339019775\n",
      "epoch 82: loss 0.3174445629119873\n",
      "epoch 83: loss 0.3952978849411011\n",
      "epoch 84: loss 0.278370201587677\n",
      "epoch 85: loss 0.29756152629852295\n",
      "epoch 86: loss 0.33631637692451477\n",
      "epoch 87: loss 0.3377189338207245\n",
      "epoch 88: loss 0.2711029648780823\n",
      "epoch 89: loss 0.36719176173210144\n",
      "epoch 90: loss 0.31305813789367676\n",
      "epoch 91: loss 0.4303819239139557\n",
      "epoch 92: loss 0.3576284348964691\n",
      "epoch 93: loss 0.31296032667160034\n",
      "epoch 94: loss 0.2961307466030121\n",
      "epoch 95: loss 0.23062556982040405\n",
      "epoch 96: loss 0.2592054605484009\n",
      "epoch 97: loss 0.3073402941226959\n",
      "epoch 98: loss 0.3152598738670349\n",
      "epoch 99: loss 0.34212419390678406\n",
      "epoch 100: loss 0.4699718654155731\n",
      "epoch 101: loss 0.30636924505233765\n",
      "epoch 102: loss 0.2580980658531189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 103: loss 0.32469677925109863\n",
      "epoch 104: loss 0.2594423294067383\n",
      "epoch 105: loss 0.3786991834640503\n",
      "epoch 106: loss 0.3144916296005249\n",
      "epoch 107: loss 0.26945239305496216\n",
      "epoch 108: loss 0.3487488031387329\n",
      "epoch 109: loss 0.35912948846817017\n",
      "epoch 110: loss 0.3429388999938965\n",
      "epoch 111: loss 0.3762739896774292\n",
      "epoch 112: loss 0.36604875326156616\n",
      "epoch 113: loss 0.27932727336883545\n",
      "epoch 114: loss 0.16472649574279785\n",
      "epoch 115: loss 0.4042063355445862\n",
      "epoch 116: loss 0.294323205947876\n",
      "epoch 117: loss 0.25992676615715027\n",
      "epoch 118: loss 0.3185179829597473\n",
      "epoch 119: loss 0.38715660572052\n",
      "epoch 120: loss 0.26840293407440186\n",
      "epoch 121: loss 0.35427433252334595\n",
      "epoch 122: loss 0.24284479022026062\n",
      "epoch 123: loss 0.386465847492218\n",
      "epoch 124: loss 0.30028778314590454\n",
      "epoch 125: loss 0.33562085032463074\n",
      "epoch 126: loss 0.35476312041282654\n",
      "epoch 127: loss 0.3786686062812805\n",
      "epoch 128: loss 0.264026939868927\n",
      "epoch 129: loss 0.3396603763103485\n",
      "epoch 130: loss 0.36423739790916443\n",
      "epoch 131: loss 0.3215544819831848\n",
      "epoch 132: loss 0.31397882103919983\n",
      "epoch 133: loss 0.34448862075805664\n",
      "epoch 134: loss 0.3089551329612732\n",
      "epoch 135: loss 0.24809494614601135\n",
      "epoch 136: loss 0.2236270308494568\n",
      "epoch 0: loss 0.334544837474823\n",
      "epoch 1: loss 0.33634838461875916\n",
      "epoch 2: loss 0.37923115491867065\n",
      "epoch 3: loss 0.367709755897522\n",
      "epoch 4: loss 0.32269418239593506\n",
      "epoch 5: loss 0.24416980147361755\n",
      "epoch 6: loss 0.2813425064086914\n",
      "epoch 7: loss 0.22239762544631958\n",
      "epoch 8: loss 0.4266417324542999\n",
      "epoch 9: loss 0.43648022413253784\n",
      "epoch 10: loss 0.34205377101898193\n",
      "epoch 11: loss 0.26978400349617004\n",
      "epoch 12: loss 0.3273933231830597\n",
      "epoch 13: loss 0.40607497096061707\n",
      "epoch 14: loss 0.36674171686172485\n",
      "epoch 15: loss 0.26999151706695557\n",
      "epoch 16: loss 0.3446590006351471\n",
      "epoch 17: loss 0.34878361225128174\n",
      "epoch 18: loss 0.4049760103225708\n",
      "epoch 19: loss 0.276848167181015\n",
      "epoch 20: loss 0.3156432509422302\n",
      "epoch 21: loss 0.3394472599029541\n",
      "epoch 22: loss 0.32291197776794434\n",
      "epoch 23: loss 0.36946970224380493\n",
      "epoch 24: loss 0.3497512936592102\n",
      "epoch 25: loss 0.3991643786430359\n",
      "epoch 26: loss 0.3204752802848816\n",
      "epoch 27: loss 0.3416510820388794\n",
      "epoch 28: loss 0.39185190200805664\n",
      "epoch 29: loss 0.31833940744400024\n",
      "epoch 30: loss 0.2739253640174866\n",
      "epoch 31: loss 0.3294484317302704\n",
      "epoch 32: loss 0.3378995656967163\n",
      "epoch 33: loss 0.3463420867919922\n",
      "epoch 34: loss 0.3913334012031555\n",
      "epoch 35: loss 0.25364068150520325\n",
      "epoch 36: loss 0.32672354578971863\n",
      "epoch 37: loss 0.3232545852661133\n",
      "epoch 38: loss 0.223799467086792\n",
      "epoch 39: loss 0.31715622544288635\n",
      "epoch 40: loss 0.3135010600090027\n",
      "epoch 41: loss 0.2776280641555786\n",
      "epoch 42: loss 0.25009065866470337\n",
      "epoch 43: loss 0.328485369682312\n",
      "epoch 44: loss 0.33547699451446533\n",
      "epoch 45: loss 0.27613967657089233\n",
      "epoch 46: loss 0.29181575775146484\n",
      "epoch 47: loss 0.2814813256263733\n",
      "epoch 48: loss 0.25077372789382935\n",
      "epoch 49: loss 0.3247590661048889\n",
      "epoch 50: loss 0.2807594835758209\n",
      "epoch 51: loss 0.4169301390647888\n",
      "epoch 52: loss 0.2988041639328003\n",
      "epoch 53: loss 0.3641066253185272\n",
      "epoch 54: loss 0.34396055340766907\n",
      "epoch 55: loss 0.30097225308418274\n",
      "epoch 56: loss 0.34489673376083374\n",
      "epoch 57: loss 0.2417450249195099\n",
      "epoch 58: loss 0.32346394658088684\n",
      "epoch 59: loss 0.26590028405189514\n",
      "epoch 60: loss 0.3253059387207031\n",
      "epoch 61: loss 0.21910421550273895\n",
      "epoch 62: loss 0.2385231852531433\n",
      "epoch 63: loss 0.3339492678642273\n",
      "epoch 64: loss 0.28022101521492004\n",
      "epoch 65: loss 0.3916229009628296\n",
      "epoch 66: loss 0.3287656903266907\n",
      "epoch 67: loss 0.3774169087409973\n",
      "epoch 68: loss 0.35635942220687866\n",
      "epoch 69: loss 0.41039127111434937\n",
      "epoch 70: loss 0.37516409158706665\n",
      "epoch 71: loss 0.3221299648284912\n",
      "epoch 72: loss 0.25081950426101685\n",
      "epoch 73: loss 0.3266102969646454\n",
      "epoch 74: loss 0.26944661140441895\n",
      "epoch 75: loss 0.34426671266555786\n",
      "epoch 76: loss 0.30569666624069214\n",
      "epoch 77: loss 0.40307343006134033\n",
      "epoch 78: loss 0.302827388048172\n",
      "epoch 79: loss 0.3359310030937195\n",
      "epoch 80: loss 0.3806246221065521\n",
      "epoch 81: loss 0.2659333348274231\n",
      "epoch 82: loss 0.31492263078689575\n",
      "epoch 83: loss 0.39872536063194275\n",
      "epoch 84: loss 0.25931504368782043\n",
      "epoch 85: loss 0.29691416025161743\n",
      "epoch 86: loss 0.3356616199016571\n",
      "epoch 87: loss 0.33939361572265625\n",
      "epoch 88: loss 0.2679901123046875\n",
      "epoch 89: loss 0.3605058193206787\n",
      "epoch 90: loss 0.3057703971862793\n",
      "epoch 91: loss 0.4138747453689575\n",
      "epoch 92: loss 0.35401785373687744\n",
      "epoch 93: loss 0.3130219578742981\n",
      "epoch 94: loss 0.29020312428474426\n",
      "epoch 95: loss 0.21820706129074097\n",
      "epoch 96: loss 0.25302496552467346\n",
      "epoch 97: loss 0.3078940510749817\n",
      "epoch 98: loss 0.31690776348114014\n",
      "epoch 99: loss 0.3386610448360443\n",
      "epoch 100: loss 0.4619486629962921\n",
      "epoch 101: loss 0.304622620344162\n",
      "epoch 102: loss 0.2590819299221039\n",
      "epoch 103: loss 0.32675737142562866\n",
      "epoch 104: loss 0.26143044233322144\n",
      "epoch 105: loss 0.37448650598526\n",
      "epoch 106: loss 0.31674206256866455\n",
      "epoch 107: loss 0.2724795937538147\n",
      "epoch 108: loss 0.34900057315826416\n",
      "epoch 109: loss 0.35423874855041504\n",
      "epoch 110: loss 0.3445911705493927\n",
      "epoch 111: loss 0.3795101046562195\n",
      "epoch 112: loss 0.36190253496170044\n",
      "epoch 113: loss 0.27985551953315735\n",
      "epoch 114: loss 0.16444388031959534\n",
      "epoch 115: loss 0.4060501456260681\n",
      "epoch 116: loss 0.29374343156814575\n",
      "epoch 117: loss 0.25664591789245605\n",
      "epoch 118: loss 0.31780847907066345\n",
      "epoch 119: loss 0.38927578926086426\n",
      "epoch 120: loss 0.2711330056190491\n",
      "epoch 121: loss 0.3495626449584961\n",
      "epoch 122: loss 0.23934467136859894\n",
      "epoch 123: loss 0.38826876878738403\n",
      "epoch 124: loss 0.29634353518486023\n",
      "epoch 125: loss 0.3358061611652374\n",
      "epoch 126: loss 0.34649890661239624\n",
      "epoch 127: loss 0.3691319525241852\n",
      "epoch 128: loss 0.2916181683540344\n",
      "epoch 129: loss 0.3402469754219055\n",
      "epoch 130: loss 0.3665921688079834\n",
      "epoch 131: loss 0.3241661787033081\n",
      "epoch 132: loss 0.3211515545845032\n",
      "epoch 133: loss 0.3702999949455261\n",
      "epoch 134: loss 0.32250741124153137\n",
      "epoch 135: loss 0.25272926688194275\n",
      "epoch 136: loss 0.23432210087776184\n",
      "epoch 0: loss 0.36096781492233276\n",
      "epoch 1: loss 0.32828477025032043\n",
      "epoch 2: loss 0.39707815647125244\n",
      "epoch 3: loss 0.360910564661026\n",
      "epoch 4: loss 0.36528289318084717\n",
      "epoch 5: loss 0.2567826509475708\n",
      "epoch 6: loss 0.2826460003852844\n",
      "epoch 7: loss 0.22681045532226562\n",
      "epoch 8: loss 0.41906434297561646\n",
      "epoch 9: loss 0.45075371861457825\n",
      "epoch 10: loss 0.3677459955215454\n",
      "epoch 11: loss 0.2926729917526245\n",
      "epoch 12: loss 0.3119373321533203\n",
      "epoch 13: loss 0.427463561296463\n",
      "epoch 14: loss 0.36224353313446045\n",
      "epoch 15: loss 0.2937457263469696\n",
      "epoch 16: loss 0.3689970076084137\n",
      "epoch 17: loss 0.34072232246398926\n",
      "epoch 18: loss 0.3706543445587158\n",
      "epoch 19: loss 0.26730650663375854\n",
      "epoch 20: loss 0.3017442524433136\n",
      "epoch 21: loss 0.3339097797870636\n",
      "epoch 22: loss 0.31911617517471313\n",
      "epoch 23: loss 0.36952584981918335\n",
      "epoch 24: loss 0.349997878074646\n",
      "epoch 25: loss 0.38494837284088135\n",
      "epoch 26: loss 0.323690265417099\n",
      "epoch 27: loss 0.3418903052806854\n",
      "epoch 28: loss 0.3932819962501526\n",
      "epoch 29: loss 0.3154242932796478\n",
      "epoch 30: loss 0.276960551738739\n",
      "epoch 31: loss 0.33753830194473267\n",
      "epoch 32: loss 0.3373669385910034\n",
      "epoch 33: loss 0.34485679864883423\n",
      "epoch 34: loss 0.3933272063732147\n",
      "epoch 35: loss 0.254475474357605\n",
      "epoch 36: loss 0.33323442935943604\n",
      "epoch 37: loss 0.33635666966438293\n",
      "epoch 38: loss 0.22698792815208435\n",
      "epoch 39: loss 0.32564249634742737\n",
      "epoch 40: loss 0.3266773223876953\n",
      "epoch 41: loss 0.28525248169898987\n",
      "epoch 42: loss 0.2545017600059509\n",
      "epoch 43: loss 0.3255881667137146\n",
      "epoch 44: loss 0.33644789457321167\n",
      "epoch 45: loss 0.28398868441581726\n",
      "epoch 46: loss 0.29423195123672485\n",
      "epoch 47: loss 0.2809511423110962\n",
      "epoch 48: loss 0.24369321763515472\n",
      "epoch 49: loss 0.3203703761100769\n",
      "epoch 50: loss 0.2864633798599243\n",
      "epoch 51: loss 0.4237457513809204\n",
      "epoch 52: loss 0.33129197359085083\n",
      "epoch 53: loss 0.3659239411354065\n",
      "epoch 54: loss 0.3378632068634033\n",
      "epoch 55: loss 0.30274105072021484\n",
      "epoch 56: loss 0.3460959792137146\n",
      "epoch 57: loss 0.25122541189193726\n",
      "epoch 58: loss 0.3270983397960663\n",
      "epoch 59: loss 0.2619301676750183\n",
      "epoch 60: loss 0.33273208141326904\n",
      "epoch 61: loss 0.2254471331834793\n",
      "epoch 62: loss 0.24224813282489777\n",
      "epoch 63: loss 0.33307576179504395\n",
      "epoch 64: loss 0.2840438783168793\n",
      "epoch 65: loss 0.38888680934906006\n",
      "epoch 66: loss 0.32998034358024597\n",
      "epoch 67: loss 0.3767429292201996\n",
      "epoch 68: loss 0.35685116052627563\n",
      "epoch 69: loss 0.4175945818424225\n",
      "epoch 70: loss 0.3694409132003784\n",
      "epoch 71: loss 0.316840797662735\n",
      "epoch 72: loss 0.25756195187568665\n",
      "epoch 73: loss 0.3202299177646637\n",
      "epoch 74: loss 0.26415759325027466\n",
      "epoch 75: loss 0.3378422260284424\n",
      "epoch 76: loss 0.3144807815551758\n",
      "epoch 77: loss 0.39765334129333496\n",
      "epoch 78: loss 0.3165576756000519\n",
      "epoch 79: loss 0.31622394919395447\n",
      "epoch 80: loss 0.37690871953964233\n",
      "epoch 81: loss 0.2682855427265167\n",
      "epoch 82: loss 0.33155328035354614\n",
      "epoch 83: loss 0.3769366443157196\n",
      "epoch 84: loss 0.27528104186058044\n",
      "epoch 85: loss 0.3070075511932373\n",
      "epoch 86: loss 0.33023762702941895\n",
      "epoch 87: loss 0.34038791060447693\n",
      "epoch 88: loss 0.269531786441803\n",
      "epoch 89: loss 0.3138921558856964\n",
      "epoch 90: loss 0.2820843458175659\n",
      "epoch 91: loss 0.33606982231140137\n",
      "epoch 92: loss 0.2881029546260834\n",
      "epoch 93: loss 0.3167300224304199\n",
      "epoch 94: loss 0.30244937539100647\n",
      "epoch 95: loss 0.21631765365600586\n",
      "epoch 96: loss 0.2208918333053589\n",
      "epoch 97: loss 0.28630849719047546\n",
      "epoch 98: loss 0.3117583394050598\n",
      "epoch 99: loss 0.3531872630119324\n",
      "epoch 100: loss 0.48161959648132324\n",
      "epoch 101: loss 0.30529966950416565\n",
      "epoch 102: loss 0.26108765602111816\n",
      "epoch 103: loss 0.3211272656917572\n",
      "epoch 104: loss 0.26292553544044495\n",
      "epoch 105: loss 0.36846938729286194\n",
      "epoch 106: loss 0.3130034804344177\n",
      "epoch 107: loss 0.27804142236709595\n",
      "epoch 108: loss 0.3481341004371643\n",
      "epoch 109: loss 0.34579211473464966\n",
      "epoch 110: loss 0.3496554493904114\n",
      "epoch 111: loss 0.36479806900024414\n",
      "epoch 112: loss 0.3586139678955078\n",
      "epoch 113: loss 0.2788546681404114\n",
      "epoch 114: loss 0.163682758808136\n",
      "epoch 115: loss 0.3835846185684204\n",
      "epoch 116: loss 0.2815943956375122\n",
      "epoch 117: loss 0.2683101296424866\n",
      "epoch 118: loss 0.31741952896118164\n",
      "epoch 119: loss 0.3880046308040619\n",
      "epoch 120: loss 0.2661363482475281\n",
      "epoch 121: loss 0.3885743021965027\n",
      "epoch 122: loss 0.249166339635849\n",
      "epoch 123: loss 0.4055083990097046\n",
      "epoch 124: loss 0.2977433502674103\n",
      "epoch 125: loss 0.33104345202445984\n",
      "epoch 126: loss 0.3439335525035858\n",
      "epoch 127: loss 0.37830305099487305\n",
      "epoch 128: loss 0.29182112216949463\n",
      "epoch 129: loss 0.3381090760231018\n",
      "epoch 130: loss 0.37135905027389526\n",
      "epoch 131: loss 0.3241325616836548\n",
      "epoch 132: loss 0.3347546458244324\n",
      "epoch 133: loss 0.3843139708042145\n",
      "epoch 134: loss 0.3176874816417694\n",
      "epoch 135: loss 0.27172067761421204\n",
      "epoch 136: loss 0.24870382249355316\n",
      "epoch 0: loss 0.3746182918548584\n",
      "epoch 1: loss 0.33735865354537964\n",
      "epoch 2: loss 0.4211490750312805\n",
      "epoch 3: loss 0.37364447116851807\n",
      "epoch 4: loss 0.3211171329021454\n",
      "epoch 5: loss 0.26972129940986633\n",
      "epoch 6: loss 0.29531002044677734\n",
      "epoch 7: loss 0.2256172001361847\n",
      "epoch 8: loss 0.4058598279953003\n",
      "epoch 9: loss 0.38913923501968384\n",
      "epoch 10: loss 0.31884706020355225\n",
      "epoch 11: loss 0.2698231339454651\n",
      "epoch 12: loss 0.3140820264816284\n",
      "epoch 13: loss 0.43103766441345215\n",
      "epoch 14: loss 0.35983097553253174\n",
      "epoch 15: loss 0.2781890332698822\n",
      "epoch 16: loss 0.35946929454803467\n",
      "epoch 17: loss 0.33032894134521484\n",
      "epoch 18: loss 0.37566784024238586\n",
      "epoch 19: loss 0.25322696566581726\n",
      "epoch 20: loss 0.2922130227088928\n",
      "epoch 21: loss 0.32671573758125305\n",
      "epoch 22: loss 0.3175795376300812\n",
      "epoch 23: loss 0.36936336755752563\n",
      "epoch 24: loss 0.3390847444534302\n",
      "epoch 25: loss 0.39462268352508545\n",
      "epoch 26: loss 0.3587750196456909\n",
      "epoch 27: loss 0.33493220806121826\n",
      "epoch 28: loss 0.4073810279369354\n",
      "epoch 29: loss 0.31701070070266724\n",
      "epoch 30: loss 0.30392199754714966\n",
      "epoch 31: loss 0.3390316367149353\n",
      "epoch 32: loss 0.35289353132247925\n",
      "epoch 33: loss 0.33108752965927124\n",
      "epoch 34: loss 0.3759136199951172\n",
      "epoch 35: loss 0.2647002935409546\n",
      "epoch 36: loss 0.3237181603908539\n",
      "epoch 37: loss 0.32532644271850586\n",
      "epoch 38: loss 0.22436347603797913\n",
      "epoch 39: loss 0.31039416790008545\n",
      "epoch 40: loss 0.3022547960281372\n",
      "epoch 41: loss 0.2819942831993103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 42: loss 0.26023560762405396\n",
      "epoch 43: loss 0.3322659134864807\n",
      "epoch 44: loss 0.33243003487586975\n",
      "epoch 45: loss 0.2768031656742096\n",
      "epoch 46: loss 0.2839100956916809\n",
      "epoch 47: loss 0.2807092070579529\n",
      "epoch 48: loss 0.2437150478363037\n",
      "epoch 49: loss 0.33423489332199097\n",
      "epoch 50: loss 0.2858967185020447\n",
      "epoch 51: loss 0.41472381353378296\n",
      "epoch 52: loss 0.2888224124908447\n",
      "epoch 53: loss 0.3628762364387512\n",
      "epoch 54: loss 0.3486204743385315\n",
      "epoch 55: loss 0.29578518867492676\n",
      "epoch 56: loss 0.35135817527770996\n",
      "epoch 57: loss 0.24535444378852844\n",
      "epoch 58: loss 0.32242700457572937\n",
      "epoch 59: loss 0.26252174377441406\n",
      "epoch 60: loss 0.32450157403945923\n",
      "epoch 61: loss 0.22129356861114502\n",
      "epoch 62: loss 0.24267132580280304\n",
      "epoch 63: loss 0.3428137004375458\n",
      "epoch 64: loss 0.28382015228271484\n",
      "epoch 65: loss 0.3971724510192871\n",
      "epoch 66: loss 0.32827433943748474\n",
      "epoch 67: loss 0.37049371004104614\n",
      "epoch 68: loss 0.3566201329231262\n",
      "epoch 69: loss 0.40475571155548096\n",
      "epoch 70: loss 0.39346784353256226\n",
      "epoch 71: loss 0.3194999694824219\n",
      "epoch 72: loss 0.24927076697349548\n",
      "epoch 73: loss 0.33656221628189087\n",
      "epoch 74: loss 0.28241991996765137\n",
      "epoch 75: loss 0.34262049198150635\n",
      "epoch 76: loss 0.3020467460155487\n",
      "epoch 77: loss 0.41315340995788574\n",
      "epoch 78: loss 0.3037988543510437\n",
      "epoch 79: loss 0.3540242910385132\n",
      "epoch 80: loss 0.37753069400787354\n",
      "epoch 81: loss 0.2699359953403473\n",
      "epoch 82: loss 0.33214789628982544\n",
      "epoch 83: loss 0.3995458781719208\n",
      "epoch 84: loss 0.2590128183364868\n",
      "epoch 85: loss 0.29640993475914\n",
      "epoch 86: loss 0.34037190675735474\n",
      "epoch 87: loss 0.33953073620796204\n",
      "epoch 88: loss 0.26939675211906433\n",
      "epoch 89: loss 0.3625675439834595\n",
      "epoch 90: loss 0.3100939393043518\n",
      "epoch 91: loss 0.4082311987876892\n",
      "epoch 92: loss 0.3345704674720764\n",
      "epoch 93: loss 0.3119409680366516\n",
      "epoch 94: loss 0.28841710090637207\n",
      "epoch 95: loss 0.2599951922893524\n",
      "epoch 96: loss 0.25101959705352783\n",
      "epoch 97: loss 0.2973847985267639\n",
      "epoch 98: loss 0.30973154306411743\n",
      "epoch 99: loss 0.3535477817058563\n",
      "epoch 100: loss 0.48320114612579346\n",
      "epoch 101: loss 0.3073229193687439\n",
      "epoch 102: loss 0.26556891202926636\n",
      "epoch 103: loss 0.32410380244255066\n",
      "epoch 104: loss 0.2618655562400818\n",
      "epoch 105: loss 0.38225558400154114\n",
      "epoch 106: loss 0.31836938858032227\n",
      "epoch 107: loss 0.27637189626693726\n",
      "epoch 108: loss 0.3574216961860657\n",
      "epoch 109: loss 0.35378989577293396\n",
      "epoch 110: loss 0.3432649075984955\n",
      "epoch 111: loss 0.36809074878692627\n",
      "epoch 112: loss 0.36692261695861816\n",
      "epoch 113: loss 0.28020942211151123\n",
      "epoch 114: loss 0.16987261176109314\n",
      "epoch 115: loss 0.4055619537830353\n",
      "epoch 116: loss 0.30124402046203613\n",
      "epoch 117: loss 0.25755852460861206\n",
      "epoch 118: loss 0.3265525698661804\n",
      "epoch 119: loss 0.3834661841392517\n",
      "epoch 120: loss 0.26163220405578613\n",
      "epoch 121: loss 0.37731635570526123\n",
      "epoch 122: loss 0.24297359585762024\n",
      "epoch 123: loss 0.37847164273262024\n",
      "epoch 124: loss 0.3531841039657593\n",
      "epoch 125: loss 0.3428197503089905\n",
      "epoch 126: loss 0.3774639964103699\n",
      "epoch 127: loss 0.4374305009841919\n",
      "epoch 128: loss 0.3029959797859192\n",
      "epoch 129: loss 0.3413400650024414\n",
      "epoch 130: loss 0.451363742351532\n",
      "epoch 131: loss 0.3635703921318054\n",
      "epoch 132: loss 0.38335534930229187\n",
      "epoch 133: loss 0.3770410418510437\n",
      "epoch 134: loss 0.32256513833999634\n",
      "epoch 135: loss 0.28600507974624634\n",
      "epoch 136: loss 0.2718948721885681\n",
      "epoch 0: loss 0.36223217844963074\n",
      "epoch 1: loss 0.3485282361507416\n",
      "epoch 2: loss 0.3864192068576813\n",
      "epoch 3: loss 0.370687335729599\n",
      "epoch 4: loss 0.3240734338760376\n",
      "epoch 5: loss 0.27163225412368774\n",
      "epoch 6: loss 0.28591179847717285\n",
      "epoch 7: loss 0.22891192138195038\n",
      "epoch 8: loss 0.40381842851638794\n",
      "epoch 9: loss 0.405194491147995\n",
      "epoch 10: loss 0.33360517024993896\n",
      "epoch 11: loss 0.27448028326034546\n",
      "epoch 12: loss 0.2920977473258972\n",
      "epoch 13: loss 0.4243519604206085\n",
      "epoch 14: loss 0.3710535168647766\n",
      "epoch 15: loss 0.2863263785839081\n",
      "epoch 16: loss 0.3429826498031616\n",
      "epoch 17: loss 0.33122754096984863\n",
      "epoch 18: loss 0.3985101878643036\n",
      "epoch 19: loss 0.2589700520038605\n",
      "epoch 20: loss 0.3088531494140625\n",
      "epoch 21: loss 0.34720948338508606\n",
      "epoch 22: loss 0.3212822377681732\n",
      "epoch 23: loss 0.3745255470275879\n",
      "epoch 24: loss 0.3408556878566742\n",
      "epoch 25: loss 0.4007778763771057\n",
      "epoch 26: loss 0.3570324778556824\n",
      "epoch 27: loss 0.3405872881412506\n",
      "epoch 28: loss 0.394184410572052\n",
      "epoch 29: loss 0.31639963388442993\n",
      "epoch 30: loss 0.2826158404350281\n",
      "epoch 31: loss 0.331134557723999\n",
      "epoch 32: loss 0.3405342698097229\n",
      "epoch 33: loss 0.33944928646087646\n",
      "epoch 34: loss 0.38099631667137146\n",
      "epoch 35: loss 0.25808703899383545\n",
      "epoch 36: loss 0.327725887298584\n",
      "epoch 37: loss 0.32483047246932983\n",
      "epoch 38: loss 0.22672542929649353\n",
      "epoch 39: loss 0.3146677613258362\n",
      "epoch 40: loss 0.31123632192611694\n",
      "epoch 41: loss 0.28203675150871277\n",
      "epoch 42: loss 0.253293514251709\n",
      "epoch 43: loss 0.33354535698890686\n",
      "epoch 44: loss 0.3338088095188141\n",
      "epoch 45: loss 0.2765369713306427\n",
      "epoch 46: loss 0.2907257378101349\n",
      "epoch 47: loss 0.27964329719543457\n",
      "epoch 48: loss 0.24984532594680786\n",
      "epoch 49: loss 0.32645827531814575\n",
      "epoch 50: loss 0.2812656760215759\n",
      "epoch 51: loss 0.415736585855484\n",
      "epoch 52: loss 0.29139819741249084\n",
      "epoch 53: loss 0.3668871819972992\n",
      "epoch 54: loss 0.3479211628437042\n",
      "epoch 55: loss 0.30015599727630615\n",
      "epoch 56: loss 0.3469023108482361\n",
      "epoch 57: loss 0.24088984727859497\n",
      "epoch 58: loss 0.32241785526275635\n",
      "epoch 59: loss 0.2657395005226135\n",
      "epoch 60: loss 0.326202929019928\n",
      "epoch 61: loss 0.21745656430721283\n",
      "epoch 62: loss 0.23760052025318146\n",
      "epoch 63: loss 0.33254921436309814\n",
      "epoch 64: loss 0.278759241104126\n",
      "epoch 65: loss 0.3925563395023346\n",
      "epoch 66: loss 0.3299194574356079\n",
      "epoch 67: loss 0.37991732358932495\n",
      "epoch 68: loss 0.35651513934135437\n",
      "epoch 69: loss 0.41539454460144043\n",
      "epoch 70: loss 0.36779797077178955\n",
      "epoch 71: loss 0.32822057604789734\n",
      "epoch 72: loss 0.2568558156490326\n",
      "epoch 73: loss 0.3189297318458557\n",
      "epoch 74: loss 0.2676827013492584\n",
      "epoch 75: loss 0.3437674939632416\n",
      "epoch 76: loss 0.30578508973121643\n",
      "epoch 77: loss 0.399596244096756\n",
      "epoch 78: loss 0.30118516087532043\n",
      "epoch 79: loss 0.34043359756469727\n",
      "epoch 80: loss 0.38867422938346863\n",
      "epoch 81: loss 0.27088838815689087\n",
      "epoch 82: loss 0.3233014941215515\n",
      "epoch 83: loss 0.3747822642326355\n",
      "epoch 84: loss 0.32616978883743286\n",
      "epoch 85: loss 0.2943498492240906\n",
      "epoch 86: loss 0.34430131316185\n",
      "epoch 87: loss 0.3330955505371094\n",
      "epoch 88: loss 0.2933531105518341\n",
      "epoch 89: loss 0.39143726229667664\n",
      "epoch 90: loss 0.325123131275177\n",
      "epoch 91: loss 0.4439149796962738\n",
      "epoch 92: loss 0.3596148192882538\n",
      "epoch 93: loss 0.31414657831192017\n",
      "epoch 94: loss 0.29281383752822876\n",
      "epoch 95: loss 0.23983550071716309\n",
      "epoch 96: loss 0.2800603210926056\n",
      "epoch 97: loss 0.30442264676094055\n",
      "epoch 98: loss 0.3119964003562927\n",
      "epoch 99: loss 0.34870272874832153\n",
      "epoch 100: loss 0.46412768959999084\n",
      "epoch 101: loss 0.30559223890304565\n",
      "epoch 102: loss 0.2660357654094696\n",
      "epoch 103: loss 0.33621662855148315\n",
      "epoch 104: loss 0.2618519365787506\n",
      "epoch 105: loss 0.3737848997116089\n",
      "epoch 106: loss 0.3244361877441406\n",
      "epoch 107: loss 0.27774345874786377\n",
      "epoch 108: loss 0.3584202826023102\n",
      "epoch 109: loss 0.3577016592025757\n",
      "epoch 110: loss 0.3488066494464874\n",
      "epoch 111: loss 0.37750041484832764\n",
      "epoch 112: loss 0.3638894557952881\n",
      "epoch 113: loss 0.2810620069503784\n",
      "epoch 114: loss 0.1693440079689026\n",
      "epoch 115: loss 0.404538631439209\n",
      "epoch 116: loss 0.29739105701446533\n",
      "epoch 117: loss 0.2598777711391449\n",
      "epoch 118: loss 0.32167744636535645\n",
      "epoch 119: loss 0.3825811743736267\n",
      "epoch 120: loss 0.2664962112903595\n",
      "epoch 121: loss 0.35599982738494873\n",
      "epoch 122: loss 0.2443772554397583\n",
      "epoch 123: loss 0.38169896602630615\n",
      "epoch 124: loss 0.3132008910179138\n",
      "epoch 125: loss 0.3364853858947754\n",
      "epoch 126: loss 0.370064914226532\n",
      "epoch 127: loss 0.4055377244949341\n",
      "epoch 128: loss 0.258059561252594\n",
      "epoch 129: loss 0.34734123945236206\n",
      "epoch 130: loss 0.3649141788482666\n",
      "epoch 131: loss 0.3325101137161255\n",
      "epoch 132: loss 0.32254478335380554\n",
      "epoch 133: loss 0.33632415533065796\n",
      "epoch 134: loss 0.3145279586315155\n",
      "epoch 135: loss 0.2472422868013382\n",
      "epoch 136: loss 0.22602537274360657\n",
      "epoch 0: loss 0.35344210267066956\n",
      "epoch 1: loss 0.32953891158103943\n",
      "epoch 2: loss 0.4050185978412628\n",
      "epoch 3: loss 0.36033347249031067\n",
      "epoch 4: loss 0.3458332419395447\n",
      "epoch 5: loss 0.250327467918396\n",
      "epoch 6: loss 0.2774311304092407\n",
      "epoch 7: loss 0.22293546795845032\n",
      "epoch 8: loss 0.41428709030151367\n",
      "epoch 9: loss 0.42330098152160645\n",
      "epoch 10: loss 0.34586870670318604\n",
      "epoch 11: loss 0.28260400891304016\n",
      "epoch 12: loss 0.2971958518028259\n",
      "epoch 13: loss 0.4129178524017334\n",
      "epoch 14: loss 0.36877354979515076\n",
      "epoch 15: loss 0.283741295337677\n",
      "epoch 16: loss 0.3516000211238861\n",
      "epoch 17: loss 0.3305177688598633\n",
      "epoch 18: loss 0.3810230493545532\n",
      "epoch 19: loss 0.26183584332466125\n",
      "epoch 20: loss 0.3038496971130371\n",
      "epoch 21: loss 0.338476300239563\n",
      "epoch 22: loss 0.32125282287597656\n",
      "epoch 23: loss 0.3714478015899658\n",
      "epoch 24: loss 0.34302961826324463\n",
      "epoch 25: loss 0.3924456536769867\n",
      "epoch 26: loss 0.36196058988571167\n",
      "epoch 27: loss 0.3417104482650757\n",
      "epoch 28: loss 0.3904542326927185\n",
      "epoch 29: loss 0.3142087161540985\n",
      "epoch 30: loss 0.2785445749759674\n",
      "epoch 31: loss 0.32934218645095825\n",
      "epoch 32: loss 0.34026920795440674\n",
      "epoch 33: loss 0.3394342064857483\n",
      "epoch 34: loss 0.38317394256591797\n",
      "epoch 35: loss 0.25634461641311646\n",
      "epoch 36: loss 0.3301330506801605\n",
      "epoch 37: loss 0.32780417799949646\n",
      "epoch 38: loss 0.22589285671710968\n",
      "epoch 39: loss 0.3156159818172455\n",
      "epoch 40: loss 0.3095128536224365\n",
      "epoch 41: loss 0.2801142930984497\n",
      "epoch 42: loss 0.25122570991516113\n",
      "epoch 43: loss 0.3367004692554474\n",
      "epoch 44: loss 0.3331747055053711\n",
      "epoch 45: loss 0.27669480443000793\n",
      "epoch 46: loss 0.289884090423584\n",
      "epoch 47: loss 0.28169453144073486\n",
      "epoch 48: loss 0.24959808588027954\n",
      "epoch 49: loss 0.32311582565307617\n",
      "epoch 50: loss 0.27828148007392883\n",
      "epoch 51: loss 0.40612250566482544\n",
      "epoch 52: loss 0.29086294770240784\n",
      "epoch 53: loss 0.36211317777633667\n",
      "epoch 54: loss 0.3467784821987152\n",
      "epoch 55: loss 0.3021593689918518\n",
      "epoch 56: loss 0.3433518409729004\n",
      "epoch 57: loss 0.24306470155715942\n",
      "epoch 58: loss 0.320428729057312\n",
      "epoch 59: loss 0.2672196924686432\n",
      "epoch 60: loss 0.3257657289505005\n",
      "epoch 61: loss 0.21787071228027344\n",
      "epoch 62: loss 0.23724325001239777\n",
      "epoch 63: loss 0.3310306668281555\n",
      "epoch 64: loss 0.2777528762817383\n",
      "epoch 65: loss 0.38910919427871704\n",
      "epoch 66: loss 0.3292040228843689\n",
      "epoch 67: loss 0.38042154908180237\n",
      "epoch 68: loss 0.35658618807792664\n",
      "epoch 69: loss 0.41747045516967773\n",
      "epoch 70: loss 0.36580127477645874\n",
      "epoch 71: loss 0.32916998863220215\n",
      "epoch 72: loss 0.2571409344673157\n",
      "epoch 73: loss 0.3189605474472046\n",
      "epoch 74: loss 0.26684629917144775\n",
      "epoch 75: loss 0.34411853551864624\n",
      "epoch 76: loss 0.30646753311157227\n",
      "epoch 77: loss 0.40154534578323364\n",
      "epoch 78: loss 0.3015492856502533\n",
      "epoch 79: loss 0.34171125292778015\n",
      "epoch 80: loss 0.3888076841831207\n",
      "epoch 81: loss 0.2700400650501251\n",
      "epoch 82: loss 0.32094883918762207\n",
      "epoch 83: loss 0.37806209921836853\n",
      "epoch 84: loss 0.3164895176887512\n",
      "epoch 85: loss 0.2958828806877136\n",
      "epoch 86: loss 0.3422832489013672\n",
      "epoch 87: loss 0.3306956887245178\n",
      "epoch 88: loss 0.29467323422431946\n",
      "epoch 89: loss 0.3883240222930908\n",
      "epoch 90: loss 0.3249560594558716\n",
      "epoch 91: loss 0.4448471963405609\n",
      "epoch 92: loss 0.37331563234329224\n",
      "epoch 93: loss 0.3093094527721405\n",
      "epoch 94: loss 0.30191242694854736\n",
      "epoch 95: loss 0.2081288844347\n",
      "epoch 96: loss 0.24896112084388733\n",
      "epoch 97: loss 0.32177403569221497\n",
      "epoch 98: loss 0.33288922905921936\n",
      "epoch 99: loss 0.32935911417007446\n",
      "epoch 100: loss 0.44559192657470703\n",
      "epoch 101: loss 0.3090822398662567\n",
      "epoch 102: loss 0.26261061429977417\n",
      "epoch 103: loss 0.30990034341812134\n",
      "epoch 104: loss 0.25761985778808594\n",
      "epoch 105: loss 0.36913540959358215\n",
      "epoch 106: loss 0.3123526871204376\n",
      "epoch 107: loss 0.26878154277801514\n",
      "epoch 108: loss 0.3448944687843323\n",
      "epoch 109: loss 0.3476719856262207\n",
      "epoch 110: loss 0.34345102310180664\n",
      "epoch 111: loss 0.38460350036621094\n",
      "epoch 112: loss 0.3655495345592499\n",
      "epoch 113: loss 0.2715856432914734\n",
      "epoch 114: loss 0.15409809350967407\n",
      "epoch 115: loss 0.3967897295951843\n",
      "epoch 116: loss 0.2840123474597931\n",
      "epoch 117: loss 0.24808670580387115\n",
      "epoch 118: loss 0.32116276025772095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 119: loss 0.3958311080932617\n",
      "epoch 120: loss 0.26779037714004517\n",
      "epoch 121: loss 0.3549817204475403\n",
      "epoch 122: loss 0.23806798458099365\n",
      "epoch 123: loss 0.3853367567062378\n",
      "epoch 124: loss 0.30112555623054504\n",
      "epoch 125: loss 0.33307403326034546\n",
      "epoch 126: loss 0.3555159866809845\n",
      "epoch 127: loss 0.3763997554779053\n",
      "epoch 128: loss 0.26240074634552\n",
      "epoch 129: loss 0.33623069524765015\n",
      "epoch 130: loss 0.3617890477180481\n",
      "epoch 131: loss 0.31984177231788635\n",
      "epoch 132: loss 0.31883853673934937\n",
      "epoch 133: loss 0.3463919162750244\n",
      "epoch 134: loss 0.3073921799659729\n",
      "epoch 135: loss 0.24550306797027588\n",
      "epoch 136: loss 0.22249804437160492\n",
      "epoch 0: loss 0.3326514959335327\n",
      "epoch 1: loss 0.3335171043872833\n",
      "epoch 2: loss 0.3718357980251312\n",
      "epoch 3: loss 0.36087214946746826\n",
      "epoch 4: loss 0.3147544264793396\n",
      "epoch 5: loss 0.24715012311935425\n",
      "epoch 6: loss 0.27570611238479614\n",
      "epoch 7: loss 0.22053591907024384\n",
      "epoch 8: loss 0.4266131520271301\n",
      "epoch 9: loss 0.43623751401901245\n",
      "epoch 10: loss 0.34644603729248047\n",
      "epoch 11: loss 0.2761153280735016\n",
      "epoch 12: loss 0.3120311200618744\n",
      "epoch 13: loss 0.40187978744506836\n",
      "epoch 14: loss 0.403189480304718\n",
      "epoch 15: loss 0.2687556743621826\n",
      "epoch 16: loss 0.3421363830566406\n",
      "epoch 17: loss 0.36938127875328064\n",
      "epoch 18: loss 0.4279099404811859\n",
      "epoch 19: loss 0.3050701320171356\n",
      "epoch 20: loss 0.34183788299560547\n",
      "epoch 21: loss 0.3578166365623474\n",
      "epoch 22: loss 0.33456453680992126\n",
      "epoch 23: loss 0.3934095501899719\n",
      "epoch 24: loss 0.36000993847846985\n",
      "epoch 25: loss 0.4094995856285095\n",
      "epoch 26: loss 0.3456019461154938\n",
      "epoch 27: loss 0.3617083728313446\n",
      "epoch 28: loss 0.39189594984054565\n",
      "epoch 29: loss 0.32220274209976196\n",
      "epoch 30: loss 0.2681158483028412\n",
      "epoch 31: loss 0.3312552869319916\n",
      "epoch 32: loss 0.35540449619293213\n",
      "epoch 33: loss 0.3422461152076721\n",
      "epoch 34: loss 0.39391249418258667\n",
      "epoch 35: loss 0.26924610137939453\n",
      "epoch 36: loss 0.33896109461784363\n",
      "epoch 37: loss 0.3453747034072876\n",
      "epoch 38: loss 0.2365134060382843\n",
      "epoch 39: loss 0.3193534016609192\n",
      "epoch 40: loss 0.29847341775894165\n",
      "epoch 41: loss 0.2786562442779541\n",
      "epoch 42: loss 0.2973559498786926\n",
      "epoch 43: loss 0.34828174114227295\n",
      "epoch 44: loss 0.3515448570251465\n",
      "epoch 45: loss 0.28385239839553833\n",
      "epoch 46: loss 0.30587154626846313\n",
      "epoch 47: loss 0.2982778549194336\n",
      "epoch 48: loss 0.25732624530792236\n",
      "epoch 49: loss 0.323885440826416\n",
      "epoch 50: loss 0.28210577368736267\n",
      "epoch 51: loss 0.399120956659317\n",
      "epoch 52: loss 0.29210925102233887\n",
      "epoch 53: loss 0.3656213879585266\n",
      "epoch 54: loss 0.3435658812522888\n",
      "epoch 55: loss 0.3009781837463379\n",
      "epoch 56: loss 0.34646862745285034\n",
      "epoch 57: loss 0.26251429319381714\n",
      "epoch 58: loss 0.3341454565525055\n",
      "epoch 59: loss 0.2650630474090576\n",
      "epoch 60: loss 0.33948445320129395\n",
      "epoch 61: loss 0.22267338633537292\n",
      "epoch 62: loss 0.23819030821323395\n",
      "epoch 63: loss 0.35744932293891907\n",
      "epoch 64: loss 0.30125224590301514\n",
      "epoch 65: loss 0.40547701716423035\n",
      "epoch 66: loss 0.33093175292015076\n",
      "epoch 67: loss 0.3711037039756775\n",
      "epoch 68: loss 0.36701858043670654\n",
      "epoch 69: loss 0.3989546298980713\n",
      "epoch 70: loss 0.38222986459732056\n",
      "epoch 71: loss 0.32186856865882874\n",
      "epoch 72: loss 0.254181832075119\n",
      "epoch 73: loss 0.3358439803123474\n",
      "epoch 74: loss 0.2772907614707947\n",
      "epoch 75: loss 0.3442060947418213\n",
      "epoch 76: loss 0.3056022524833679\n",
      "epoch 77: loss 0.40929439663887024\n",
      "epoch 78: loss 0.31142503023147583\n",
      "epoch 79: loss 0.34161680936813354\n",
      "epoch 80: loss 0.38243651390075684\n",
      "epoch 81: loss 0.2685112953186035\n",
      "epoch 82: loss 0.3242257833480835\n",
      "epoch 83: loss 0.3776906132698059\n",
      "epoch 84: loss 0.290018230676651\n",
      "epoch 85: loss 0.30559563636779785\n",
      "epoch 86: loss 0.33345645666122437\n",
      "epoch 87: loss 0.3382670283317566\n",
      "epoch 88: loss 0.27396059036254883\n",
      "epoch 89: loss 0.36765581369400024\n",
      "epoch 90: loss 0.3079679608345032\n",
      "epoch 91: loss 0.42948681116104126\n",
      "epoch 92: loss 0.3700295686721802\n",
      "epoch 93: loss 0.30839043855667114\n",
      "epoch 94: loss 0.2972164452075958\n",
      "epoch 95: loss 0.21338006854057312\n",
      "epoch 96: loss 0.24389171600341797\n",
      "epoch 97: loss 0.31416624784469604\n",
      "epoch 98: loss 0.31984931230545044\n",
      "epoch 99: loss 0.3311302065849304\n",
      "epoch 100: loss 0.46287211775779724\n",
      "epoch 101: loss 0.30480143427848816\n",
      "epoch 102: loss 0.2575492560863495\n",
      "epoch 103: loss 0.30149558186531067\n",
      "epoch 104: loss 0.2516136169433594\n",
      "epoch 105: loss 0.3716716170310974\n",
      "epoch 106: loss 0.3129032552242279\n",
      "epoch 107: loss 0.2649080753326416\n",
      "epoch 108: loss 0.3434356451034546\n",
      "epoch 109: loss 0.3476475179195404\n",
      "epoch 110: loss 0.343600869178772\n",
      "epoch 111: loss 0.38237565755844116\n",
      "epoch 112: loss 0.3671155273914337\n",
      "epoch 113: loss 0.27691662311553955\n",
      "epoch 114: loss 0.15853075683116913\n",
      "epoch 115: loss 0.3965889513492584\n",
      "epoch 116: loss 0.2867291271686554\n",
      "epoch 117: loss 0.24903757870197296\n",
      "epoch 118: loss 0.3200529217720032\n",
      "epoch 119: loss 0.39408010244369507\n",
      "epoch 120: loss 0.26328033208847046\n",
      "epoch 121: loss 0.3561088442802429\n",
      "epoch 122: loss 0.2410551756620407\n",
      "epoch 123: loss 0.384655237197876\n",
      "epoch 124: loss 0.30564379692077637\n",
      "epoch 125: loss 0.3371385931968689\n",
      "epoch 126: loss 0.3600383996963501\n",
      "epoch 127: loss 0.3838593065738678\n",
      "epoch 128: loss 0.25731390714645386\n",
      "epoch 129: loss 0.33410584926605225\n",
      "epoch 130: loss 0.36152827739715576\n",
      "epoch 131: loss 0.32357507944107056\n",
      "epoch 132: loss 0.319549560546875\n",
      "epoch 133: loss 0.3472817838191986\n",
      "epoch 134: loss 0.31016334891319275\n",
      "epoch 135: loss 0.24542781710624695\n",
      "epoch 136: loss 0.22297701239585876\n",
      "epoch 0: loss 0.33818358182907104\n",
      "epoch 1: loss 0.3300527334213257\n",
      "epoch 2: loss 0.3829991817474365\n",
      "epoch 3: loss 0.3608689308166504\n",
      "epoch 4: loss 0.3327949345111847\n",
      "epoch 5: loss 0.24858906865119934\n",
      "epoch 6: loss 0.27946192026138306\n",
      "epoch 7: loss 0.22489480674266815\n",
      "epoch 8: loss 0.4345765709877014\n",
      "epoch 9: loss 0.4595698118209839\n",
      "epoch 10: loss 0.35927635431289673\n",
      "epoch 11: loss 0.28333720564842224\n",
      "epoch 12: loss 0.3126532733440399\n",
      "epoch 13: loss 0.4064144194126129\n",
      "epoch 14: loss 0.4063563346862793\n",
      "epoch 15: loss 0.2756877839565277\n",
      "epoch 16: loss 0.3477330803871155\n",
      "epoch 17: loss 0.34637218713760376\n",
      "epoch 18: loss 0.40100616216659546\n",
      "epoch 19: loss 0.2882266044616699\n",
      "epoch 20: loss 0.3267064094543457\n",
      "epoch 21: loss 0.35120344161987305\n",
      "epoch 22: loss 0.330907940864563\n",
      "epoch 23: loss 0.38339295983314514\n",
      "epoch 24: loss 0.35705095529556274\n",
      "epoch 25: loss 0.3932304084300995\n",
      "epoch 26: loss 0.3373306393623352\n",
      "epoch 27: loss 0.353730171918869\n",
      "epoch 28: loss 0.3907313942909241\n",
      "epoch 29: loss 0.3189306855201721\n",
      "epoch 30: loss 0.2700731158256531\n",
      "epoch 31: loss 0.3327258229255676\n",
      "epoch 32: loss 0.34210318326950073\n",
      "epoch 33: loss 0.342216819524765\n",
      "epoch 34: loss 0.3920263946056366\n",
      "epoch 35: loss 0.2631976306438446\n",
      "epoch 36: loss 0.33869728446006775\n",
      "epoch 37: loss 0.34756726026535034\n",
      "epoch 38: loss 0.2427223026752472\n",
      "epoch 39: loss 0.3246404528617859\n",
      "epoch 40: loss 0.3154686987400055\n",
      "epoch 41: loss 0.27611982822418213\n",
      "epoch 42: loss 0.25990742444992065\n",
      "epoch 43: loss 0.34376201033592224\n",
      "epoch 44: loss 0.34563305974006653\n",
      "epoch 45: loss 0.28040364384651184\n",
      "epoch 46: loss 0.29445090889930725\n",
      "epoch 47: loss 0.2842904329299927\n",
      "epoch 48: loss 0.24696800112724304\n",
      "epoch 49: loss 0.31675446033477783\n",
      "epoch 50: loss 0.28124552965164185\n",
      "epoch 51: loss 0.4134010672569275\n",
      "epoch 52: loss 0.31381356716156006\n",
      "epoch 53: loss 0.36062091588974\n",
      "epoch 54: loss 0.33847224712371826\n",
      "epoch 55: loss 0.30316001176834106\n",
      "epoch 56: loss 0.3460427522659302\n",
      "epoch 57: loss 0.24704235792160034\n",
      "epoch 58: loss 0.3298810124397278\n",
      "epoch 59: loss 0.26056474447250366\n",
      "epoch 60: loss 0.33366698026657104\n",
      "epoch 61: loss 0.22147005796432495\n",
      "epoch 62: loss 0.2385154366493225\n",
      "epoch 63: loss 0.3360052704811096\n",
      "epoch 64: loss 0.2810666561126709\n",
      "epoch 65: loss 0.38871315121650696\n",
      "epoch 66: loss 0.3289731740951538\n",
      "epoch 67: loss 0.3774142265319824\n",
      "epoch 68: loss 0.35729339718818665\n",
      "epoch 69: loss 0.41067278385162354\n",
      "epoch 70: loss 0.3673117458820343\n",
      "epoch 71: loss 0.32705801725387573\n",
      "epoch 72: loss 0.26025521755218506\n",
      "epoch 73: loss 0.3178037405014038\n",
      "epoch 74: loss 0.2609063386917114\n",
      "epoch 75: loss 0.3434678912162781\n",
      "epoch 76: loss 0.3109045922756195\n",
      "epoch 77: loss 0.4003939628601074\n",
      "epoch 78: loss 0.3066815733909607\n",
      "epoch 79: loss 0.33275723457336426\n",
      "epoch 80: loss 0.3906604051589966\n",
      "epoch 81: loss 0.2706438899040222\n",
      "epoch 82: loss 0.32911479473114014\n",
      "epoch 83: loss 0.36932796239852905\n",
      "epoch 84: loss 0.3466230034828186\n",
      "epoch 85: loss 0.29579442739486694\n",
      "epoch 86: loss 0.3403484523296356\n",
      "epoch 87: loss 0.3345140218734741\n",
      "epoch 88: loss 0.2899934649467468\n",
      "epoch 89: loss 0.38755089044570923\n",
      "epoch 90: loss 0.3223694860935211\n",
      "epoch 91: loss 0.45322829484939575\n",
      "epoch 92: loss 0.3885272443294525\n",
      "epoch 93: loss 0.30819201469421387\n",
      "epoch 94: loss 0.31508708000183105\n",
      "epoch 95: loss 0.20871306955814362\n",
      "epoch 96: loss 0.2514740824699402\n",
      "epoch 97: loss 0.32843074202537537\n",
      "epoch 98: loss 0.3491505980491638\n",
      "epoch 99: loss 0.34085991978645325\n",
      "epoch 100: loss 0.46090537309646606\n",
      "epoch 101: loss 0.30865758657455444\n",
      "epoch 102: loss 0.2605695128440857\n",
      "epoch 103: loss 0.30344653129577637\n",
      "epoch 104: loss 0.25440847873687744\n",
      "epoch 105: loss 0.3609073758125305\n",
      "epoch 106: loss 0.3088420629501343\n",
      "epoch 107: loss 0.27208781242370605\n",
      "epoch 108: loss 0.33943283557891846\n",
      "epoch 109: loss 0.3526949882507324\n",
      "epoch 110: loss 0.34427711367607117\n",
      "epoch 111: loss 0.38814449310302734\n",
      "epoch 112: loss 0.3775908946990967\n",
      "epoch 113: loss 0.2739596962928772\n",
      "epoch 114: loss 0.1608981490135193\n",
      "epoch 115: loss 0.3959420919418335\n",
      "epoch 116: loss 0.2911861538887024\n",
      "epoch 117: loss 0.24707426130771637\n",
      "epoch 118: loss 0.32395777106285095\n",
      "epoch 119: loss 0.39535799622535706\n",
      "epoch 120: loss 0.26058056950569153\n",
      "epoch 121: loss 0.36650145053863525\n",
      "epoch 122: loss 0.24652808904647827\n",
      "epoch 123: loss 0.38374489545822144\n",
      "epoch 124: loss 0.31938865780830383\n",
      "epoch 125: loss 0.34386539459228516\n",
      "epoch 126: loss 0.37592238187789917\n",
      "epoch 127: loss 0.42943012714385986\n",
      "epoch 128: loss 0.28025662899017334\n",
      "epoch 129: loss 0.35244983434677124\n",
      "epoch 130: loss 0.3918865919113159\n",
      "epoch 131: loss 0.3207131624221802\n",
      "epoch 132: loss 0.3206633925437927\n",
      "epoch 133: loss 0.3529159426689148\n",
      "epoch 134: loss 0.3163433074951172\n",
      "epoch 135: loss 0.27847570180892944\n",
      "epoch 136: loss 0.25347834825515747\n",
      "epoch 0: loss 0.3449282646179199\n",
      "epoch 1: loss 0.33483976125717163\n",
      "epoch 2: loss 0.3955879211425781\n",
      "epoch 3: loss 0.3642171621322632\n",
      "epoch 4: loss 0.3102794289588928\n",
      "epoch 5: loss 0.2594035267829895\n",
      "epoch 6: loss 0.28038546442985535\n",
      "epoch 7: loss 0.22501879930496216\n",
      "epoch 8: loss 0.4012787342071533\n",
      "epoch 9: loss 0.406301885843277\n",
      "epoch 10: loss 0.31503093242645264\n",
      "epoch 11: loss 0.26113027334213257\n",
      "epoch 12: loss 0.31102705001831055\n",
      "epoch 13: loss 0.4192410707473755\n",
      "epoch 14: loss 0.3634336590766907\n",
      "epoch 15: loss 0.28077834844589233\n",
      "epoch 16: loss 0.35113605856895447\n",
      "epoch 17: loss 0.3317265808582306\n",
      "epoch 18: loss 0.38966846466064453\n",
      "epoch 19: loss 0.2506309449672699\n",
      "epoch 20: loss 0.294145405292511\n",
      "epoch 21: loss 0.3332507610321045\n",
      "epoch 22: loss 0.3136594891548157\n",
      "epoch 23: loss 0.3622943162918091\n",
      "epoch 24: loss 0.33326828479766846\n",
      "epoch 25: loss 0.40168774127960205\n",
      "epoch 26: loss 0.3204168677330017\n",
      "epoch 27: loss 0.3373885750770569\n",
      "epoch 28: loss 0.4042744040489197\n",
      "epoch 29: loss 0.3154241442680359\n",
      "epoch 30: loss 0.28453487157821655\n",
      "epoch 31: loss 0.3324154019355774\n",
      "epoch 32: loss 0.3409900665283203\n",
      "epoch 33: loss 0.33775120973587036\n",
      "epoch 34: loss 0.3842592239379883\n",
      "epoch 35: loss 0.2584368586540222\n",
      "epoch 36: loss 0.32476797699928284\n",
      "epoch 37: loss 0.3221917152404785\n",
      "epoch 38: loss 0.2208418995141983\n",
      "epoch 39: loss 0.3151146173477173\n",
      "epoch 40: loss 0.3077400326728821\n",
      "epoch 41: loss 0.2797095477581024\n",
      "epoch 42: loss 0.247196763753891\n",
      "epoch 43: loss 0.3277119994163513\n",
      "epoch 44: loss 0.33297836780548096\n",
      "epoch 45: loss 0.2779872417449951\n",
      "epoch 46: loss 0.28793734312057495\n",
      "epoch 47: loss 0.2829580307006836\n",
      "epoch 48: loss 0.24612756073474884\n",
      "epoch 49: loss 0.33188894391059875\n",
      "epoch 50: loss 0.28395816683769226\n",
      "epoch 51: loss 0.4169451594352722\n",
      "epoch 52: loss 0.3047901391983032\n",
      "epoch 53: loss 0.35860675573349\n",
      "epoch 54: loss 0.34010520577430725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55: loss 0.304751455783844\n",
      "epoch 56: loss 0.3448045551776886\n",
      "epoch 57: loss 0.24291740357875824\n",
      "epoch 58: loss 0.32177621126174927\n",
      "epoch 59: loss 0.2663559317588806\n",
      "epoch 60: loss 0.32813772559165955\n",
      "epoch 61: loss 0.22250738739967346\n",
      "epoch 62: loss 0.24045942723751068\n",
      "epoch 63: loss 0.3350575566291809\n",
      "epoch 64: loss 0.2819483280181885\n",
      "epoch 65: loss 0.3868187665939331\n",
      "epoch 66: loss 0.3281022906303406\n",
      "epoch 67: loss 0.3780256509780884\n",
      "epoch 68: loss 0.3559485971927643\n",
      "epoch 69: loss 0.4217475652694702\n",
      "epoch 70: loss 0.37403765320777893\n",
      "epoch 71: loss 0.31613457202911377\n",
      "epoch 72: loss 0.26028281450271606\n",
      "epoch 73: loss 0.31818023324012756\n",
      "epoch 74: loss 0.262311190366745\n",
      "epoch 75: loss 0.33738526701927185\n",
      "epoch 76: loss 0.31200718879699707\n",
      "epoch 77: loss 0.3956218361854553\n",
      "epoch 78: loss 0.3121962547302246\n",
      "epoch 79: loss 0.31414681673049927\n",
      "epoch 80: loss 0.380235493183136\n",
      "epoch 81: loss 0.26914143562316895\n",
      "epoch 82: loss 0.3342747092247009\n",
      "epoch 83: loss 0.3770514130592346\n",
      "epoch 84: loss 0.2661622166633606\n",
      "epoch 85: loss 0.3109746277332306\n",
      "epoch 86: loss 0.3316103518009186\n",
      "epoch 87: loss 0.3392302393913269\n",
      "epoch 88: loss 0.2753404378890991\n",
      "epoch 89: loss 0.3080436587333679\n",
      "epoch 90: loss 0.2766648828983307\n",
      "epoch 91: loss 0.3293190598487854\n",
      "epoch 92: loss 0.27868396043777466\n",
      "epoch 93: loss 0.31235480308532715\n",
      "epoch 94: loss 0.3000725507736206\n",
      "epoch 95: loss 0.21653787791728973\n",
      "epoch 96: loss 0.21693222224712372\n",
      "epoch 97: loss 0.2851688265800476\n",
      "epoch 98: loss 0.3169648349285126\n",
      "epoch 99: loss 0.35758668184280396\n",
      "epoch 100: loss 0.4843117892742157\n",
      "epoch 101: loss 0.3048904538154602\n",
      "epoch 102: loss 0.2601919174194336\n",
      "epoch 103: loss 0.31351035833358765\n",
      "epoch 104: loss 0.2505056858062744\n",
      "epoch 105: loss 0.36410582065582275\n",
      "epoch 106: loss 0.3144035339355469\n",
      "epoch 107: loss 0.281390905380249\n",
      "epoch 108: loss 0.3510794937610626\n",
      "epoch 109: loss 0.3495621085166931\n",
      "epoch 110: loss 0.34473341703414917\n",
      "epoch 111: loss 0.3662456274032593\n",
      "epoch 112: loss 0.3617315888404846\n",
      "epoch 113: loss 0.2771637737751007\n",
      "epoch 114: loss 0.1656414270401001\n",
      "epoch 115: loss 0.3919559419155121\n",
      "epoch 116: loss 0.28589901328086853\n",
      "epoch 117: loss 0.25771623849868774\n",
      "epoch 118: loss 0.31570902466773987\n",
      "epoch 119: loss 0.3879668712615967\n",
      "epoch 120: loss 0.2578786611557007\n",
      "epoch 121: loss 0.3691464066505432\n",
      "epoch 122: loss 0.2359338402748108\n",
      "epoch 123: loss 0.38727444410324097\n",
      "epoch 124: loss 0.34697529673576355\n",
      "epoch 125: loss 0.34287601709365845\n",
      "epoch 126: loss 0.3777783513069153\n",
      "epoch 127: loss 0.4326596260070801\n",
      "epoch 128: loss 0.29771801829338074\n",
      "epoch 129: loss 0.342693030834198\n",
      "epoch 130: loss 0.4456945061683655\n",
      "epoch 131: loss 0.3618786633014679\n",
      "epoch 132: loss 0.38584303855895996\n",
      "epoch 133: loss 0.3971015214920044\n",
      "epoch 134: loss 0.32148927450180054\n",
      "epoch 135: loss 0.2884873151779175\n",
      "epoch 136: loss 0.2607264518737793\n",
      "epoch 0: loss 0.3693240284919739\n",
      "epoch 1: loss 0.3401907682418823\n",
      "epoch 2: loss 0.3863087296485901\n",
      "epoch 3: loss 0.38170573115348816\n",
      "epoch 4: loss 0.3693954348564148\n",
      "epoch 5: loss 0.27046942710876465\n",
      "epoch 6: loss 0.2869378328323364\n",
      "epoch 7: loss 0.23097403347492218\n",
      "epoch 8: loss 0.417664110660553\n",
      "epoch 9: loss 0.4576762914657593\n",
      "epoch 10: loss 0.3745807707309723\n",
      "epoch 11: loss 0.2976696491241455\n",
      "epoch 12: loss 0.30309104919433594\n",
      "epoch 13: loss 0.44868534803390503\n",
      "epoch 14: loss 0.34724870324134827\n",
      "epoch 15: loss 0.28725457191467285\n",
      "epoch 16: loss 0.36098164319992065\n",
      "epoch 17: loss 0.33624857664108276\n",
      "epoch 18: loss 0.3635614514350891\n",
      "epoch 19: loss 0.2641313076019287\n",
      "epoch 20: loss 0.29579219222068787\n",
      "epoch 21: loss 0.32835233211517334\n",
      "epoch 22: loss 0.31535768508911133\n",
      "epoch 23: loss 0.37059253454208374\n",
      "epoch 24: loss 0.3504684865474701\n",
      "epoch 25: loss 0.3839722275733948\n",
      "epoch 26: loss 0.3246362805366516\n",
      "epoch 27: loss 0.34104302525520325\n",
      "epoch 28: loss 0.39381277561187744\n",
      "epoch 29: loss 0.3118179440498352\n",
      "epoch 30: loss 0.27745723724365234\n",
      "epoch 31: loss 0.3358830213546753\n",
      "epoch 32: loss 0.33587145805358887\n",
      "epoch 33: loss 0.3408467769622803\n",
      "epoch 34: loss 0.3851754069328308\n",
      "epoch 35: loss 0.25850486755371094\n",
      "epoch 36: loss 0.33000829815864563\n",
      "epoch 37: loss 0.3341299295425415\n",
      "epoch 38: loss 0.2278616577386856\n",
      "epoch 39: loss 0.32419881224632263\n",
      "epoch 40: loss 0.3235167860984802\n",
      "epoch 41: loss 0.2823726534843445\n",
      "epoch 42: loss 0.2453007996082306\n",
      "epoch 43: loss 0.3269084692001343\n",
      "epoch 44: loss 0.3404993712902069\n",
      "epoch 45: loss 0.2819230854511261\n",
      "epoch 46: loss 0.289572536945343\n",
      "epoch 47: loss 0.28094154596328735\n",
      "epoch 48: loss 0.24433445930480957\n",
      "epoch 49: loss 0.3176444172859192\n",
      "epoch 50: loss 0.28137290477752686\n",
      "epoch 51: loss 0.41068583726882935\n",
      "epoch 52: loss 0.31042683124542236\n",
      "epoch 53: loss 0.35953670740127563\n",
      "epoch 54: loss 0.3363233208656311\n",
      "epoch 55: loss 0.3068392276763916\n",
      "epoch 56: loss 0.3442423939704895\n",
      "epoch 57: loss 0.24614232778549194\n",
      "epoch 58: loss 0.3244214951992035\n",
      "epoch 59: loss 0.26216602325439453\n",
      "epoch 60: loss 0.329606831073761\n",
      "epoch 61: loss 0.2201155722141266\n",
      "epoch 62: loss 0.23861533403396606\n",
      "epoch 63: loss 0.33256494998931885\n",
      "epoch 64: loss 0.27885717153549194\n",
      "epoch 65: loss 0.3856385350227356\n",
      "epoch 66: loss 0.3296387791633606\n",
      "epoch 67: loss 0.38208961486816406\n",
      "epoch 68: loss 0.3560296297073364\n",
      "epoch 69: loss 0.41478270292282104\n",
      "epoch 70: loss 0.36710160970687866\n",
      "epoch 71: loss 0.31871354579925537\n",
      "epoch 72: loss 0.2589924931526184\n",
      "epoch 73: loss 0.31941521167755127\n",
      "epoch 74: loss 0.26033031940460205\n",
      "epoch 75: loss 0.34100544452667236\n",
      "epoch 76: loss 0.31198060512542725\n",
      "epoch 77: loss 0.3956127166748047\n",
      "epoch 78: loss 0.3093678951263428\n",
      "epoch 79: loss 0.3148975670337677\n",
      "epoch 80: loss 0.3828023076057434\n",
      "epoch 81: loss 0.26944097876548767\n",
      "epoch 82: loss 0.33316224813461304\n",
      "epoch 83: loss 0.3732907772064209\n",
      "epoch 84: loss 0.28039878606796265\n",
      "epoch 85: loss 0.3078344464302063\n",
      "epoch 86: loss 0.328228235244751\n",
      "epoch 87: loss 0.3396408259868622\n",
      "epoch 88: loss 0.2691064476966858\n",
      "epoch 89: loss 0.31319382786750793\n",
      "epoch 90: loss 0.28482136130332947\n",
      "epoch 91: loss 0.3430309295654297\n",
      "epoch 92: loss 0.30511415004730225\n",
      "epoch 93: loss 0.31151270866394043\n",
      "epoch 94: loss 0.2936376929283142\n",
      "epoch 95: loss 0.20130608975887299\n",
      "epoch 96: loss 0.22612768411636353\n",
      "epoch 97: loss 0.2929113507270813\n",
      "epoch 98: loss 0.31336677074432373\n",
      "epoch 99: loss 0.3357722759246826\n",
      "epoch 100: loss 0.4593678116798401\n",
      "epoch 101: loss 0.3016687035560608\n",
      "epoch 102: loss 0.2538816034793854\n",
      "epoch 103: loss 0.3044435381889343\n",
      "epoch 104: loss 0.2519533038139343\n",
      "epoch 105: loss 0.3651867210865021\n",
      "epoch 106: loss 0.30955201387405396\n",
      "epoch 107: loss 0.26942524313926697\n",
      "epoch 108: loss 0.34321194887161255\n",
      "epoch 109: loss 0.34643176198005676\n",
      "epoch 110: loss 0.3438323736190796\n",
      "epoch 111: loss 0.37068164348602295\n",
      "epoch 112: loss 0.3610627055168152\n",
      "epoch 113: loss 0.2768070101737976\n",
      "epoch 114: loss 0.1578349769115448\n",
      "epoch 115: loss 0.39012449979782104\n",
      "epoch 116: loss 0.28472673892974854\n",
      "epoch 117: loss 0.25089573860168457\n",
      "epoch 118: loss 0.3168909251689911\n",
      "epoch 119: loss 0.3930913805961609\n",
      "epoch 120: loss 0.2600281238555908\n",
      "epoch 121: loss 0.35833847522735596\n",
      "epoch 122: loss 0.2373371124267578\n",
      "epoch 123: loss 0.38582831621170044\n",
      "epoch 124: loss 0.3166261613368988\n",
      "epoch 125: loss 0.336238294839859\n",
      "epoch 126: loss 0.37771695852279663\n",
      "epoch 127: loss 0.4036005139350891\n",
      "epoch 128: loss 0.2685388922691345\n",
      "epoch 129: loss 0.34983086585998535\n",
      "epoch 130: loss 0.36286312341690063\n",
      "epoch 131: loss 0.3186580538749695\n",
      "epoch 132: loss 0.31123489141464233\n",
      "epoch 133: loss 0.34403860569000244\n",
      "epoch 134: loss 0.3150196373462677\n",
      "epoch 135: loss 0.26085352897644043\n",
      "epoch 136: loss 0.2423640340566635\n",
      "epoch 0: loss 0.3412955403327942\n",
      "epoch 1: loss 0.3250463008880615\n",
      "epoch 2: loss 0.4024352431297302\n",
      "epoch 3: loss 0.347787082195282\n",
      "epoch 4: loss 0.2978644371032715\n",
      "epoch 5: loss 0.2583291530609131\n",
      "epoch 6: loss 0.27438393235206604\n",
      "epoch 7: loss 0.2195241004228592\n",
      "epoch 8: loss 0.40103405714035034\n",
      "epoch 9: loss 0.40387409925460815\n",
      "epoch 10: loss 0.323170006275177\n",
      "epoch 11: loss 0.2723429799079895\n",
      "epoch 12: loss 0.30379122495651245\n",
      "epoch 13: loss 0.4171270728111267\n",
      "epoch 14: loss 0.3487705588340759\n",
      "epoch 15: loss 0.27668431401252747\n",
      "epoch 16: loss 0.3438660800457001\n",
      "epoch 17: loss 0.33166006207466125\n",
      "epoch 18: loss 0.38154345750808716\n",
      "epoch 19: loss 0.26579204201698303\n",
      "epoch 20: loss 0.29257696866989136\n",
      "epoch 21: loss 0.33192092180252075\n",
      "epoch 22: loss 0.29959166049957275\n",
      "epoch 23: loss 0.3499714732170105\n",
      "epoch 24: loss 0.32967323064804077\n",
      "epoch 25: loss 0.3790063261985779\n",
      "epoch 26: loss 0.315386027097702\n",
      "epoch 27: loss 0.3373609185218811\n",
      "epoch 28: loss 0.3941582441329956\n",
      "epoch 29: loss 0.3094503581523895\n",
      "epoch 30: loss 0.27661484479904175\n",
      "epoch 31: loss 0.3371657133102417\n",
      "epoch 32: loss 0.3356035351753235\n",
      "epoch 33: loss 0.3466467261314392\n",
      "epoch 34: loss 0.3894369900226593\n",
      "epoch 35: loss 0.25449395179748535\n",
      "epoch 36: loss 0.3254975378513336\n",
      "epoch 37: loss 0.3250366449356079\n",
      "epoch 38: loss 0.2239755243062973\n",
      "epoch 39: loss 0.3210117816925049\n",
      "epoch 40: loss 0.31625860929489136\n",
      "epoch 41: loss 0.28441816568374634\n",
      "epoch 42: loss 0.2439170628786087\n",
      "epoch 43: loss 0.3266106843948364\n",
      "epoch 44: loss 0.3323851227760315\n",
      "epoch 45: loss 0.2770477533340454\n",
      "epoch 46: loss 0.28597402572631836\n",
      "epoch 47: loss 0.2826841473579407\n",
      "epoch 48: loss 0.24202629923820496\n",
      "epoch 49: loss 0.325873464345932\n",
      "epoch 50: loss 0.28097057342529297\n",
      "epoch 51: loss 0.40739595890045166\n",
      "epoch 52: loss 0.2989731431007385\n",
      "epoch 53: loss 0.35836178064346313\n",
      "epoch 54: loss 0.3415357172489166\n",
      "epoch 55: loss 0.3017287850379944\n",
      "epoch 56: loss 0.3418797254562378\n",
      "epoch 57: loss 0.23981666564941406\n",
      "epoch 58: loss 0.3206300735473633\n",
      "epoch 59: loss 0.26440054178237915\n",
      "epoch 60: loss 0.3227590322494507\n",
      "epoch 61: loss 0.21698372066020966\n",
      "epoch 62: loss 0.23764626681804657\n",
      "epoch 63: loss 0.3324827551841736\n",
      "epoch 64: loss 0.275327205657959\n",
      "epoch 65: loss 0.38725656270980835\n",
      "epoch 66: loss 0.3256519138813019\n",
      "epoch 67: loss 0.3786807656288147\n",
      "epoch 68: loss 0.3571644723415375\n",
      "epoch 69: loss 0.4256874918937683\n",
      "epoch 70: loss 0.373495876789093\n",
      "epoch 71: loss 0.31645968556404114\n",
      "epoch 72: loss 0.2626134753227234\n",
      "epoch 73: loss 0.31585532426834106\n",
      "epoch 74: loss 0.26123523712158203\n",
      "epoch 75: loss 0.33778253197669983\n",
      "epoch 76: loss 0.3097054362297058\n",
      "epoch 77: loss 0.39303529262542725\n",
      "epoch 78: loss 0.30612045526504517\n",
      "epoch 79: loss 0.3127434551715851\n",
      "epoch 80: loss 0.3805978000164032\n",
      "epoch 81: loss 0.26964476704597473\n",
      "epoch 82: loss 0.3317223787307739\n",
      "epoch 83: loss 0.3693348169326782\n",
      "epoch 84: loss 0.2834850251674652\n",
      "epoch 85: loss 0.3003852665424347\n",
      "epoch 86: loss 0.3291960656642914\n",
      "epoch 87: loss 0.34562620520591736\n",
      "epoch 88: loss 0.2647620141506195\n",
      "epoch 89: loss 0.3251643180847168\n",
      "epoch 90: loss 0.29140225052833557\n",
      "epoch 91: loss 0.36714476346969604\n",
      "epoch 92: loss 0.3333509564399719\n",
      "epoch 93: loss 0.31581059098243713\n",
      "epoch 94: loss 0.2963944971561432\n",
      "epoch 95: loss 0.19547656178474426\n",
      "epoch 96: loss 0.23086799681186676\n",
      "epoch 97: loss 0.2971632480621338\n",
      "epoch 98: loss 0.314279168844223\n",
      "epoch 99: loss 0.3310055136680603\n",
      "epoch 100: loss 0.45669838786125183\n",
      "epoch 101: loss 0.3010779917240143\n",
      "epoch 102: loss 0.2551853656768799\n",
      "epoch 103: loss 0.3051376938819885\n",
      "epoch 104: loss 0.25102609395980835\n",
      "epoch 105: loss 0.3607490360736847\n",
      "epoch 106: loss 0.3075142502784729\n",
      "epoch 107: loss 0.26962125301361084\n",
      "epoch 108: loss 0.33840757608413696\n",
      "epoch 109: loss 0.34851300716400146\n",
      "epoch 110: loss 0.3378143310546875\n",
      "epoch 111: loss 0.368306964635849\n",
      "epoch 112: loss 0.36072033643722534\n",
      "epoch 113: loss 0.2717967629432678\n",
      "epoch 114: loss 0.15012726187705994\n",
      "epoch 115: loss 0.38952845335006714\n",
      "epoch 116: loss 0.27784156799316406\n",
      "epoch 117: loss 0.2527293562889099\n",
      "epoch 118: loss 0.3186103105545044\n",
      "epoch 119: loss 0.3954131603240967\n",
      "epoch 120: loss 0.2598268985748291\n",
      "epoch 121: loss 0.35834699869155884\n",
      "epoch 122: loss 0.2321430891752243\n",
      "epoch 123: loss 0.3842169940471649\n",
      "epoch 124: loss 0.31916379928588867\n",
      "epoch 125: loss 0.33895817399024963\n",
      "epoch 126: loss 0.3777576684951782\n",
      "epoch 127: loss 0.40530306100845337\n",
      "epoch 128: loss 0.27120816707611084\n",
      "epoch 129: loss 0.34766608476638794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 130: loss 0.3650912046432495\n",
      "epoch 131: loss 0.3179527521133423\n",
      "epoch 132: loss 0.3117029070854187\n",
      "epoch 133: loss 0.3456611633300781\n",
      "epoch 134: loss 0.31709396839141846\n",
      "epoch 135: loss 0.26654496788978577\n",
      "epoch 136: loss 0.2506864666938782\n",
      "epoch 0: loss 0.3420323133468628\n",
      "epoch 1: loss 0.32391050457954407\n",
      "epoch 2: loss 0.40122196078300476\n",
      "epoch 3: loss 0.3464887738227844\n",
      "epoch 4: loss 0.2981299161911011\n",
      "epoch 5: loss 0.262371689081192\n",
      "epoch 6: loss 0.27427899837493896\n",
      "epoch 7: loss 0.21600210666656494\n",
      "epoch 8: loss 0.40162062644958496\n",
      "epoch 9: loss 0.398640513420105\n",
      "epoch 10: loss 0.3289870619773865\n",
      "epoch 11: loss 0.2765974700450897\n",
      "epoch 12: loss 0.29651057720184326\n",
      "epoch 13: loss 0.42668813467025757\n",
      "epoch 14: loss 0.3396565020084381\n",
      "epoch 15: loss 0.27491337060928345\n",
      "epoch 16: loss 0.34163928031921387\n",
      "epoch 17: loss 0.33349186182022095\n",
      "epoch 18: loss 0.3654872179031372\n",
      "epoch 19: loss 0.27515488862991333\n",
      "epoch 20: loss 0.29047027230262756\n",
      "epoch 21: loss 0.33116933703422546\n",
      "epoch 22: loss 0.30953630805015564\n",
      "epoch 23: loss 0.3621780276298523\n",
      "epoch 24: loss 0.33415526151657104\n",
      "epoch 25: loss 0.38564473390579224\n",
      "epoch 26: loss 0.35637807846069336\n",
      "epoch 27: loss 0.3381199240684509\n",
      "epoch 28: loss 0.40706124901771545\n",
      "epoch 29: loss 0.3212670385837555\n",
      "epoch 30: loss 0.3107263445854187\n",
      "epoch 31: loss 0.33842557668685913\n",
      "epoch 32: loss 0.3532848358154297\n",
      "epoch 33: loss 0.3279760479927063\n",
      "epoch 34: loss 0.378339558839798\n",
      "epoch 35: loss 0.2635580897331238\n",
      "epoch 36: loss 0.32227885723114014\n",
      "epoch 37: loss 0.32686319947242737\n",
      "epoch 38: loss 0.22453419864177704\n",
      "epoch 39: loss 0.30695629119873047\n",
      "epoch 40: loss 0.29229819774627686\n",
      "epoch 41: loss 0.280863881111145\n",
      "epoch 42: loss 0.2598758935928345\n",
      "epoch 43: loss 0.3380817770957947\n",
      "epoch 44: loss 0.32998979091644287\n",
      "epoch 45: loss 0.28115394711494446\n",
      "epoch 46: loss 0.2900729179382324\n",
      "epoch 47: loss 0.2869008481502533\n",
      "epoch 48: loss 0.23445744812488556\n",
      "epoch 49: loss 0.3281187415122986\n",
      "epoch 50: loss 0.28270405530929565\n",
      "epoch 51: loss 0.3984993100166321\n",
      "epoch 52: loss 0.2833710312843323\n",
      "epoch 53: loss 0.3605782985687256\n",
      "epoch 54: loss 0.3417387008666992\n",
      "epoch 55: loss 0.29700446128845215\n",
      "epoch 56: loss 0.3430590033531189\n",
      "epoch 57: loss 0.23702670633792877\n",
      "epoch 58: loss 0.32106032967567444\n",
      "epoch 59: loss 0.26437094807624817\n",
      "epoch 60: loss 0.3202749490737915\n",
      "epoch 61: loss 0.21844321489334106\n",
      "epoch 62: loss 0.2384803593158722\n",
      "epoch 63: loss 0.33157163858413696\n",
      "epoch 64: loss 0.2784336805343628\n",
      "epoch 65: loss 0.3889843821525574\n",
      "epoch 66: loss 0.32658451795578003\n",
      "epoch 67: loss 0.38058269023895264\n",
      "epoch 68: loss 0.3560924232006073\n",
      "epoch 69: loss 0.4176747798919678\n",
      "epoch 70: loss 0.36977535486221313\n",
      "epoch 71: loss 0.3261898458003998\n",
      "epoch 72: loss 0.2541343569755554\n",
      "epoch 73: loss 0.3152211904525757\n",
      "epoch 74: loss 0.26268500089645386\n",
      "epoch 75: loss 0.34503260254859924\n",
      "epoch 76: loss 0.30588820576667786\n",
      "epoch 77: loss 0.40214744210243225\n",
      "epoch 78: loss 0.2999911606311798\n",
      "epoch 79: loss 0.3384942412376404\n",
      "epoch 80: loss 0.38632532954216003\n",
      "epoch 81: loss 0.2683246433734894\n",
      "epoch 82: loss 0.31763362884521484\n",
      "epoch 83: loss 0.3835580348968506\n",
      "epoch 84: loss 0.29300856590270996\n",
      "epoch 85: loss 0.2922136187553406\n",
      "epoch 86: loss 0.3394084870815277\n",
      "epoch 87: loss 0.3319542407989502\n",
      "epoch 88: loss 0.28877556324005127\n",
      "epoch 89: loss 0.3802100718021393\n",
      "epoch 90: loss 0.3246971666812897\n",
      "epoch 91: loss 0.45392704010009766\n",
      "epoch 92: loss 0.3939597010612488\n",
      "epoch 93: loss 0.3084600865840912\n",
      "epoch 94: loss 0.31621238589286804\n",
      "epoch 95: loss 0.19541089236736298\n",
      "epoch 96: loss 0.23935675621032715\n",
      "epoch 97: loss 0.3207842707633972\n",
      "epoch 98: loss 0.3542962372303009\n",
      "epoch 99: loss 0.34535378217697144\n",
      "epoch 100: loss 0.4747121334075928\n",
      "epoch 101: loss 0.3064252436161041\n",
      "epoch 102: loss 0.25578439235687256\n",
      "epoch 103: loss 0.3056199550628662\n",
      "epoch 104: loss 0.25901105999946594\n",
      "epoch 105: loss 0.36878731846809387\n",
      "epoch 106: loss 0.31027472019195557\n",
      "epoch 107: loss 0.27114588022232056\n",
      "epoch 108: loss 0.34050917625427246\n",
      "epoch 109: loss 0.3511223793029785\n",
      "epoch 110: loss 0.34094899892807007\n",
      "epoch 111: loss 0.3865385055541992\n",
      "epoch 112: loss 0.3708496689796448\n",
      "epoch 113: loss 0.27457958459854126\n",
      "epoch 114: loss 0.16258706152439117\n",
      "epoch 115: loss 0.3967241942882538\n",
      "epoch 116: loss 0.2859511375427246\n",
      "epoch 117: loss 0.24998798966407776\n",
      "epoch 118: loss 0.3165739178657532\n",
      "epoch 119: loss 0.3900199830532074\n",
      "epoch 120: loss 0.26642167568206787\n",
      "epoch 121: loss 0.36275506019592285\n",
      "epoch 122: loss 0.2428247332572937\n",
      "epoch 123: loss 0.3845887780189514\n",
      "epoch 124: loss 0.2990983724594116\n",
      "epoch 125: loss 0.33775731921195984\n",
      "epoch 126: loss 0.3553820252418518\n",
      "epoch 127: loss 0.376270055770874\n",
      "epoch 128: loss 0.25915902853012085\n",
      "epoch 129: loss 0.3375096917152405\n",
      "epoch 130: loss 0.35963669419288635\n",
      "epoch 131: loss 0.316944420337677\n",
      "epoch 132: loss 0.3141411542892456\n",
      "epoch 133: loss 0.34677600860595703\n",
      "epoch 134: loss 0.30306094884872437\n",
      "epoch 135: loss 0.24522113800048828\n",
      "epoch 136: loss 0.22089701890945435\n",
      "epoch 0: loss 0.33057308197021484\n",
      "epoch 1: loss 0.3325765132904053\n",
      "epoch 2: loss 0.37649357318878174\n",
      "epoch 3: loss 0.3611886501312256\n",
      "epoch 4: loss 0.3167908787727356\n",
      "epoch 5: loss 0.2419189214706421\n",
      "epoch 6: loss 0.27480533719062805\n",
      "epoch 7: loss 0.2202337384223938\n",
      "epoch 8: loss 0.42580240964889526\n",
      "epoch 9: loss 0.4295908510684967\n",
      "epoch 10: loss 0.33552291989326477\n",
      "epoch 11: loss 0.26683396100997925\n",
      "epoch 12: loss 0.31594449281692505\n",
      "epoch 13: loss 0.4063228666782379\n",
      "epoch 14: loss 0.3694866895675659\n",
      "epoch 15: loss 0.26580744981765747\n",
      "epoch 16: loss 0.3464675843715668\n",
      "epoch 17: loss 0.3533831536769867\n",
      "epoch 18: loss 0.4104762077331543\n",
      "epoch 19: loss 0.2731814980506897\n",
      "epoch 20: loss 0.30887117981910706\n",
      "epoch 21: loss 0.3363940417766571\n",
      "epoch 22: loss 0.31523144245147705\n",
      "epoch 23: loss 0.3622031509876251\n",
      "epoch 24: loss 0.37058061361312866\n",
      "epoch 25: loss 0.38524380326271057\n",
      "epoch 26: loss 0.3082411587238312\n",
      "epoch 27: loss 0.34444141387939453\n",
      "epoch 28: loss 0.3982549011707306\n",
      "epoch 29: loss 0.3189428448677063\n",
      "epoch 30: loss 0.2866308093070984\n",
      "epoch 31: loss 0.3295605182647705\n",
      "epoch 32: loss 0.3322869539260864\n",
      "epoch 33: loss 0.34951239824295044\n",
      "epoch 34: loss 0.3901492953300476\n",
      "epoch 35: loss 0.251872718334198\n",
      "epoch 36: loss 0.32721972465515137\n",
      "epoch 37: loss 0.31762564182281494\n",
      "epoch 38: loss 0.2206261307001114\n",
      "epoch 39: loss 0.31516438722610474\n",
      "epoch 40: loss 0.3176039457321167\n",
      "epoch 41: loss 0.279198557138443\n",
      "epoch 42: loss 0.2462075799703598\n",
      "epoch 43: loss 0.31864237785339355\n",
      "epoch 44: loss 0.33186766505241394\n",
      "epoch 45: loss 0.2795552611351013\n",
      "epoch 46: loss 0.2970077395439148\n",
      "epoch 47: loss 0.2827931344509125\n",
      "epoch 48: loss 0.24798917770385742\n",
      "epoch 49: loss 0.321613609790802\n",
      "epoch 50: loss 0.2808202803134918\n",
      "epoch 51: loss 0.41725876927375793\n",
      "epoch 52: loss 0.313784122467041\n",
      "epoch 53: loss 0.36296606063842773\n",
      "epoch 54: loss 0.3340328633785248\n",
      "epoch 55: loss 0.30956101417541504\n",
      "epoch 56: loss 0.34605786204338074\n",
      "epoch 57: loss 0.24311640858650208\n",
      "epoch 58: loss 0.3234853744506836\n",
      "epoch 59: loss 0.2567765414714813\n",
      "epoch 60: loss 0.32968783378601074\n",
      "epoch 61: loss 0.22009873390197754\n",
      "epoch 62: loss 0.23849359154701233\n",
      "epoch 63: loss 0.3321160674095154\n",
      "epoch 64: loss 0.28070101141929626\n",
      "epoch 65: loss 0.3831474781036377\n",
      "epoch 66: loss 0.3291415572166443\n",
      "epoch 67: loss 0.38006994128227234\n",
      "epoch 68: loss 0.355385959148407\n",
      "epoch 69: loss 0.41843557357788086\n",
      "epoch 70: loss 0.37265652418136597\n",
      "epoch 71: loss 0.3075711131095886\n",
      "epoch 72: loss 0.2544862627983093\n",
      "epoch 73: loss 0.3194522559642792\n",
      "epoch 74: loss 0.26184386014938354\n",
      "epoch 75: loss 0.3367319107055664\n",
      "epoch 76: loss 0.3145173192024231\n",
      "epoch 77: loss 0.3947165608406067\n",
      "epoch 78: loss 0.31489625573158264\n",
      "epoch 79: loss 0.3086097538471222\n",
      "epoch 80: loss 0.3779236376285553\n",
      "epoch 81: loss 0.267610639333725\n",
      "epoch 82: loss 0.33247673511505127\n",
      "epoch 83: loss 0.3802551031112671\n",
      "epoch 84: loss 0.25228559970855713\n",
      "epoch 85: loss 0.30763792991638184\n",
      "epoch 86: loss 0.332844078540802\n",
      "epoch 87: loss 0.33368030190467834\n",
      "epoch 88: loss 0.2771646976470947\n",
      "epoch 89: loss 0.3015141487121582\n",
      "epoch 90: loss 0.25530117750167847\n",
      "epoch 91: loss 0.31487399339675903\n",
      "epoch 92: loss 0.2593114972114563\n",
      "epoch 93: loss 0.3175637722015381\n",
      "epoch 94: loss 0.312366783618927\n",
      "epoch 95: loss 0.1875976026058197\n",
      "epoch 96: loss 0.20445209741592407\n",
      "epoch 97: loss 0.286102294921875\n",
      "epoch 98: loss 0.32965099811553955\n",
      "epoch 99: loss 0.37121862173080444\n",
      "epoch 100: loss 0.5000216960906982\n",
      "epoch 101: loss 0.30555224418640137\n",
      "epoch 102: loss 0.2527899146080017\n",
      "epoch 103: loss 0.2985711693763733\n",
      "epoch 104: loss 0.24409998953342438\n",
      "epoch 105: loss 0.3616274297237396\n",
      "epoch 106: loss 0.31367841362953186\n",
      "epoch 107: loss 0.29114991426467896\n",
      "epoch 108: loss 0.3467627167701721\n",
      "epoch 109: loss 0.34606868028640747\n",
      "epoch 110: loss 0.34636390209198\n",
      "epoch 111: loss 0.3758261203765869\n",
      "epoch 112: loss 0.3697229027748108\n",
      "epoch 113: loss 0.29351577162742615\n",
      "epoch 114: loss 0.17676712572574615\n",
      "epoch 115: loss 0.40032172203063965\n",
      "epoch 116: loss 0.2894064784049988\n",
      "epoch 117: loss 0.2789633572101593\n",
      "epoch 118: loss 0.3169414699077606\n",
      "epoch 119: loss 0.3858944773674011\n",
      "epoch 120: loss 0.26296114921569824\n",
      "epoch 121: loss 0.3701438307762146\n",
      "epoch 122: loss 0.24138383567333221\n",
      "epoch 123: loss 0.39816927909851074\n",
      "epoch 124: loss 0.2887780964374542\n",
      "epoch 125: loss 0.3318840563297272\n",
      "epoch 126: loss 0.3335142135620117\n",
      "epoch 127: loss 0.36828356981277466\n",
      "epoch 128: loss 0.2641444802284241\n",
      "epoch 129: loss 0.3369929790496826\n",
      "epoch 130: loss 0.36114683747291565\n",
      "epoch 131: loss 0.322190523147583\n",
      "epoch 132: loss 0.3168182075023651\n",
      "epoch 133: loss 0.3618704080581665\n",
      "epoch 134: loss 0.31999558210372925\n",
      "epoch 135: loss 0.2500371038913727\n",
      "epoch 136: loss 0.23083914816379547\n",
      "epoch 0: loss 0.3394837975502014\n",
      "epoch 1: loss 0.33091825246810913\n",
      "epoch 2: loss 0.3672173023223877\n",
      "epoch 3: loss 0.36377495527267456\n",
      "epoch 4: loss 0.320056676864624\n",
      "epoch 5: loss 0.2472044974565506\n",
      "epoch 6: loss 0.2767885625362396\n",
      "epoch 7: loss 0.22134041786193848\n",
      "epoch 8: loss 0.4091685116291046\n",
      "epoch 9: loss 0.4220699071884155\n",
      "epoch 10: loss 0.34598612785339355\n",
      "epoch 11: loss 0.2806093692779541\n",
      "epoch 12: loss 0.3140135109424591\n",
      "epoch 13: loss 0.41440707445144653\n",
      "epoch 14: loss 0.36781471967697144\n",
      "epoch 15: loss 0.28295958042144775\n",
      "epoch 16: loss 0.3557090163230896\n",
      "epoch 17: loss 0.33266961574554443\n",
      "epoch 18: loss 0.3771774172782898\n",
      "epoch 19: loss 0.26695212721824646\n",
      "epoch 20: loss 0.3032921850681305\n",
      "epoch 21: loss 0.3319210410118103\n",
      "epoch 22: loss 0.3193073868751526\n",
      "epoch 23: loss 0.36604171991348267\n",
      "epoch 24: loss 0.33420780301094055\n",
      "epoch 25: loss 0.4008396565914154\n",
      "epoch 26: loss 0.34042730927467346\n",
      "epoch 27: loss 0.33480265736579895\n",
      "epoch 28: loss 0.3976920247077942\n",
      "epoch 29: loss 0.3158467411994934\n",
      "epoch 30: loss 0.2904568314552307\n",
      "epoch 31: loss 0.3303470015525818\n",
      "epoch 32: loss 0.34088391065597534\n",
      "epoch 33: loss 0.34092992544174194\n",
      "epoch 34: loss 0.38437968492507935\n",
      "epoch 35: loss 0.25508853793144226\n",
      "epoch 36: loss 0.32422304153442383\n",
      "epoch 37: loss 0.32111504673957825\n",
      "epoch 38: loss 0.21813419461250305\n",
      "epoch 39: loss 0.30978673696517944\n",
      "epoch 40: loss 0.2993951439857483\n",
      "epoch 41: loss 0.2759895324707031\n",
      "epoch 42: loss 0.24932768940925598\n",
      "epoch 43: loss 0.33307886123657227\n",
      "epoch 44: loss 0.3323993980884552\n",
      "epoch 45: loss 0.27646130323410034\n",
      "epoch 46: loss 0.2839680314064026\n",
      "epoch 47: loss 0.28088194131851196\n",
      "epoch 48: loss 0.24284429848194122\n",
      "epoch 49: loss 0.3230130076408386\n",
      "epoch 50: loss 0.2807438373565674\n",
      "epoch 51: loss 0.40421736240386963\n",
      "epoch 52: loss 0.2884083092212677\n",
      "epoch 53: loss 0.3568180203437805\n",
      "epoch 54: loss 0.34675925970077515\n",
      "epoch 55: loss 0.29955798387527466\n",
      "epoch 56: loss 0.34666818380355835\n",
      "epoch 57: loss 0.24280992150306702\n",
      "epoch 58: loss 0.3184922933578491\n",
      "epoch 59: loss 0.2641359567642212\n",
      "epoch 60: loss 0.32209450006484985\n",
      "epoch 61: loss 0.21556702256202698\n",
      "epoch 62: loss 0.2373068630695343\n",
      "epoch 63: loss 0.33184102177619934\n",
      "epoch 64: loss 0.27610617876052856\n",
      "epoch 65: loss 0.3853002190589905\n",
      "epoch 66: loss 0.32496777176856995\n",
      "epoch 67: loss 0.37956494092941284\n",
      "epoch 68: loss 0.35231542587280273\n",
      "epoch 69: loss 0.411546915769577\n",
      "epoch 70: loss 0.37077808380126953\n",
      "epoch 71: loss 0.3201253414154053\n",
      "epoch 72: loss 0.24917754530906677\n",
      "epoch 73: loss 0.31729602813720703\n",
      "epoch 74: loss 0.2601624131202698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 75: loss 0.34533581137657166\n",
      "epoch 76: loss 0.30596277117729187\n",
      "epoch 77: loss 0.400729775428772\n",
      "epoch 78: loss 0.3022969961166382\n",
      "epoch 79: loss 0.3293028473854065\n",
      "epoch 80: loss 0.379316508769989\n",
      "epoch 81: loss 0.26617714762687683\n",
      "epoch 82: loss 0.31128832697868347\n",
      "epoch 83: loss 0.3903157114982605\n",
      "epoch 84: loss 0.2559537887573242\n",
      "epoch 85: loss 0.2953408360481262\n",
      "epoch 86: loss 0.33620738983154297\n",
      "epoch 87: loss 0.3382081389427185\n",
      "epoch 88: loss 0.27262818813323975\n",
      "epoch 89: loss 0.35897278785705566\n",
      "epoch 90: loss 0.30450013279914856\n",
      "epoch 91: loss 0.40170401334762573\n",
      "epoch 92: loss 0.36341625452041626\n",
      "epoch 93: loss 0.310494989156723\n",
      "epoch 94: loss 0.2942112386226654\n",
      "epoch 95: loss 0.1962691843509674\n",
      "epoch 96: loss 0.2311319261789322\n",
      "epoch 97: loss 0.31362050771713257\n",
      "epoch 98: loss 0.32568156719207764\n",
      "epoch 99: loss 0.32676464319229126\n",
      "epoch 100: loss 0.45859140157699585\n",
      "epoch 101: loss 0.2994562089443207\n",
      "epoch 102: loss 0.2490890920162201\n",
      "epoch 103: loss 0.2994835674762726\n",
      "epoch 104: loss 0.24374371767044067\n",
      "epoch 105: loss 0.359150767326355\n",
      "epoch 106: loss 0.3046494424343109\n",
      "epoch 107: loss 0.2668766677379608\n",
      "epoch 108: loss 0.3378395438194275\n",
      "epoch 109: loss 0.35137873888015747\n",
      "epoch 110: loss 0.3322965204715729\n",
      "epoch 111: loss 0.3842676877975464\n",
      "epoch 112: loss 0.3676875829696655\n",
      "epoch 113: loss 0.2718551754951477\n",
      "epoch 114: loss 0.14864882826805115\n",
      "epoch 115: loss 0.399352490901947\n",
      "epoch 116: loss 0.2850261330604553\n",
      "epoch 117: loss 0.2526194453239441\n",
      "epoch 118: loss 0.3174739480018616\n",
      "epoch 119: loss 0.398285448551178\n",
      "epoch 120: loss 0.27030080556869507\n",
      "epoch 121: loss 0.35726773738861084\n",
      "epoch 122: loss 0.23183700442314148\n",
      "epoch 123: loss 0.38737863302230835\n",
      "epoch 124: loss 0.2974057197570801\n",
      "epoch 125: loss 0.33701092004776\n",
      "epoch 126: loss 0.3520583510398865\n",
      "epoch 127: loss 0.36920908093452454\n",
      "epoch 128: loss 0.2696373760700226\n",
      "epoch 129: loss 0.3380313217639923\n",
      "epoch 130: loss 0.360127329826355\n",
      "epoch 131: loss 0.31820422410964966\n",
      "epoch 132: loss 0.31125563383102417\n",
      "epoch 133: loss 0.34114015102386475\n",
      "epoch 134: loss 0.3048335015773773\n",
      "epoch 135: loss 0.24461327493190765\n",
      "epoch 136: loss 0.22158819437026978\n",
      "epoch 0: loss 0.3312545716762543\n",
      "epoch 1: loss 0.33089402318000793\n",
      "epoch 2: loss 0.3728151321411133\n",
      "epoch 3: loss 0.35298359394073486\n",
      "epoch 4: loss 0.3188597559928894\n",
      "epoch 5: loss 0.2431623637676239\n",
      "epoch 6: loss 0.27085059881210327\n",
      "epoch 7: loss 0.21818089485168457\n",
      "epoch 8: loss 0.4119974374771118\n",
      "epoch 9: loss 0.4146825969219208\n",
      "epoch 10: loss 0.34410911798477173\n",
      "epoch 11: loss 0.2802197337150574\n",
      "epoch 12: loss 0.30472874641418457\n",
      "epoch 13: loss 0.4113709628582001\n",
      "epoch 14: loss 0.3720012307167053\n",
      "epoch 15: loss 0.2722112536430359\n",
      "epoch 16: loss 0.3489081859588623\n",
      "epoch 17: loss 0.33341771364212036\n",
      "epoch 18: loss 0.3846140205860138\n",
      "epoch 19: loss 0.272569864988327\n",
      "epoch 20: loss 0.31051406264305115\n",
      "epoch 21: loss 0.33325713872909546\n",
      "epoch 22: loss 0.3218435049057007\n",
      "epoch 23: loss 0.3718479871749878\n",
      "epoch 24: loss 0.33939677476882935\n",
      "epoch 25: loss 0.405936598777771\n",
      "epoch 26: loss 0.3319493234157562\n",
      "epoch 27: loss 0.33982667326927185\n",
      "epoch 28: loss 0.38877665996551514\n",
      "epoch 29: loss 0.31720155477523804\n",
      "epoch 30: loss 0.2814485430717468\n",
      "epoch 31: loss 0.32861918210983276\n",
      "epoch 32: loss 0.3375574052333832\n",
      "epoch 33: loss 0.3474394679069519\n",
      "epoch 34: loss 0.39158880710601807\n",
      "epoch 35: loss 0.25581979751586914\n",
      "epoch 36: loss 0.3284919261932373\n",
      "epoch 37: loss 0.32484933733940125\n",
      "epoch 38: loss 0.22410345077514648\n",
      "epoch 39: loss 0.31549182534217834\n",
      "epoch 40: loss 0.30554479360580444\n",
      "epoch 41: loss 0.2761950194835663\n",
      "epoch 42: loss 0.2471419870853424\n",
      "epoch 43: loss 0.332822322845459\n",
      "epoch 44: loss 0.33407115936279297\n",
      "epoch 45: loss 0.27837151288986206\n",
      "epoch 46: loss 0.28374600410461426\n",
      "epoch 47: loss 0.2819233536720276\n",
      "epoch 48: loss 0.24318820238113403\n",
      "epoch 49: loss 0.32051461935043335\n",
      "epoch 50: loss 0.2784225344657898\n",
      "epoch 51: loss 0.4063156843185425\n",
      "epoch 52: loss 0.2948801517486572\n",
      "epoch 53: loss 0.3590793013572693\n",
      "epoch 54: loss 0.3441629409790039\n",
      "epoch 55: loss 0.2984933853149414\n",
      "epoch 56: loss 0.34458962082862854\n",
      "epoch 57: loss 0.2390894889831543\n",
      "epoch 58: loss 0.320106565952301\n",
      "epoch 59: loss 0.26510295271873474\n",
      "epoch 60: loss 0.3210800290107727\n",
      "epoch 61: loss 0.21604293584823608\n",
      "epoch 62: loss 0.23747576773166656\n",
      "epoch 63: loss 0.33023276925086975\n",
      "epoch 64: loss 0.2759399712085724\n",
      "epoch 65: loss 0.3875748813152313\n",
      "epoch 66: loss 0.3245924115180969\n",
      "epoch 67: loss 0.3784240484237671\n",
      "epoch 68: loss 0.3541390597820282\n",
      "epoch 69: loss 0.4132002890110016\n",
      "epoch 70: loss 0.3658614754676819\n",
      "epoch 71: loss 0.3239160180091858\n",
      "epoch 72: loss 0.24860182404518127\n",
      "epoch 73: loss 0.314199298620224\n",
      "epoch 74: loss 0.2575167417526245\n",
      "epoch 75: loss 0.34762951731681824\n",
      "epoch 76: loss 0.30868086218833923\n",
      "epoch 77: loss 0.39983463287353516\n",
      "epoch 78: loss 0.3013343811035156\n",
      "epoch 79: loss 0.33617252111434937\n",
      "epoch 80: loss 0.3818708062171936\n",
      "epoch 81: loss 0.26602083444595337\n",
      "epoch 82: loss 0.31224602460861206\n",
      "epoch 83: loss 0.3965792655944824\n",
      "epoch 84: loss 0.2578768730163574\n",
      "epoch 85: loss 0.2922624945640564\n",
      "epoch 86: loss 0.33736568689346313\n",
      "epoch 87: loss 0.3373098075389862\n",
      "epoch 88: loss 0.2740263342857361\n",
      "epoch 89: loss 0.3583426773548126\n",
      "epoch 90: loss 0.3041711449623108\n",
      "epoch 91: loss 0.4025760293006897\n",
      "epoch 92: loss 0.36701199412345886\n",
      "epoch 93: loss 0.30975085496902466\n",
      "epoch 94: loss 0.2977237105369568\n",
      "epoch 95: loss 0.1841053068637848\n",
      "epoch 96: loss 0.22727736830711365\n",
      "epoch 97: loss 0.31209713220596313\n",
      "epoch 98: loss 0.33380115032196045\n",
      "epoch 99: loss 0.3251674175262451\n",
      "epoch 100: loss 0.45388323068618774\n",
      "epoch 101: loss 0.30079060792922974\n",
      "epoch 102: loss 0.25096142292022705\n",
      "epoch 103: loss 0.30621451139450073\n",
      "epoch 104: loss 0.2511080801486969\n",
      "epoch 105: loss 0.3560858368873596\n",
      "epoch 106: loss 0.30540162324905396\n",
      "epoch 107: loss 0.2672348618507385\n",
      "epoch 108: loss 0.3334329128265381\n",
      "epoch 109: loss 0.34900128841400146\n",
      "epoch 110: loss 0.3327184021472931\n",
      "epoch 111: loss 0.38039055466651917\n",
      "epoch 112: loss 0.3682737946510315\n",
      "epoch 113: loss 0.268352210521698\n",
      "epoch 114: loss 0.14630913734436035\n",
      "epoch 115: loss 0.3950251340866089\n",
      "epoch 116: loss 0.2817886471748352\n",
      "epoch 117: loss 0.24727216362953186\n",
      "epoch 118: loss 0.3180351257324219\n",
      "epoch 119: loss 0.40085965394973755\n",
      "epoch 120: loss 0.27231845259666443\n",
      "epoch 121: loss 0.35430431365966797\n",
      "epoch 122: loss 0.23325321078300476\n",
      "epoch 123: loss 0.3863641023635864\n",
      "epoch 124: loss 0.2975042462348938\n",
      "epoch 125: loss 0.3374761939048767\n",
      "epoch 126: loss 0.3511725664138794\n",
      "epoch 127: loss 0.3633301258087158\n",
      "epoch 128: loss 0.27418631315231323\n",
      "epoch 129: loss 0.3356095254421234\n",
      "epoch 130: loss 0.36095696687698364\n",
      "epoch 131: loss 0.3162665367126465\n",
      "epoch 132: loss 0.3141530752182007\n",
      "epoch 133: loss 0.3439224362373352\n",
      "epoch 134: loss 0.3041919469833374\n",
      "epoch 135: loss 0.24457457661628723\n",
      "epoch 136: loss 0.22078129649162292\n",
      "epoch 0: loss 0.3273541331291199\n",
      "epoch 1: loss 0.33082425594329834\n",
      "epoch 2: loss 0.374423086643219\n",
      "epoch 3: loss 0.34955114126205444\n",
      "epoch 4: loss 0.3198520541191101\n",
      "epoch 5: loss 0.2432614117860794\n",
      "epoch 6: loss 0.2691880166530609\n",
      "epoch 7: loss 0.21962302923202515\n",
      "epoch 8: loss 0.41611766815185547\n",
      "epoch 9: loss 0.42156970500946045\n",
      "epoch 10: loss 0.3461988568305969\n",
      "epoch 11: loss 0.2796696126461029\n",
      "epoch 12: loss 0.30202344059944153\n",
      "epoch 13: loss 0.41056501865386963\n",
      "epoch 14: loss 0.36816662549972534\n",
      "epoch 15: loss 0.27523553371429443\n",
      "epoch 16: loss 0.3462621569633484\n",
      "epoch 17: loss 0.3333199620246887\n",
      "epoch 18: loss 0.38253018260002136\n",
      "epoch 19: loss 0.27706795930862427\n",
      "epoch 20: loss 0.3132284879684448\n",
      "epoch 21: loss 0.33438724279403687\n",
      "epoch 22: loss 0.3228575885295868\n",
      "epoch 23: loss 0.3666389286518097\n",
      "epoch 24: loss 0.3348761796951294\n",
      "epoch 25: loss 0.406508207321167\n",
      "epoch 26: loss 0.34084251523017883\n",
      "epoch 27: loss 0.34084832668304443\n",
      "epoch 28: loss 0.3907665014266968\n",
      "epoch 29: loss 0.3202723264694214\n",
      "epoch 30: loss 0.2885981798171997\n",
      "epoch 31: loss 0.3274924159049988\n",
      "epoch 32: loss 0.34223300218582153\n",
      "epoch 33: loss 0.34170660376548767\n",
      "epoch 34: loss 0.3854343891143799\n",
      "epoch 35: loss 0.2550731897354126\n",
      "epoch 36: loss 0.3235090374946594\n",
      "epoch 37: loss 0.3246222734451294\n",
      "epoch 38: loss 0.22425620257854462\n",
      "epoch 39: loss 0.3128001391887665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 40: loss 0.30515965819358826\n",
      "epoch 41: loss 0.2799372971057892\n",
      "epoch 42: loss 0.24277307093143463\n",
      "epoch 43: loss 0.331327348947525\n",
      "epoch 44: loss 0.3322764039039612\n",
      "epoch 45: loss 0.2773764133453369\n",
      "epoch 46: loss 0.28191858530044556\n",
      "epoch 47: loss 0.2810555100440979\n",
      "epoch 48: loss 0.2416004091501236\n",
      "epoch 49: loss 0.32432299852371216\n",
      "epoch 50: loss 0.2804466485977173\n",
      "epoch 51: loss 0.4075452983379364\n",
      "epoch 52: loss 0.2946244776248932\n",
      "epoch 53: loss 0.35902947187423706\n",
      "epoch 54: loss 0.3434126377105713\n",
      "epoch 55: loss 0.30010849237442017\n",
      "epoch 56: loss 0.3428102433681488\n",
      "epoch 57: loss 0.23840215802192688\n",
      "epoch 58: loss 0.31887340545654297\n",
      "epoch 59: loss 0.2661454677581787\n",
      "epoch 60: loss 0.32099443674087524\n",
      "epoch 61: loss 0.21475011110305786\n",
      "epoch 62: loss 0.23711323738098145\n",
      "epoch 63: loss 0.33044010400772095\n",
      "epoch 64: loss 0.2741336226463318\n",
      "epoch 65: loss 0.38635414838790894\n",
      "epoch 66: loss 0.32406121492385864\n",
      "epoch 67: loss 0.38033145666122437\n",
      "epoch 68: loss 0.3555970788002014\n",
      "epoch 69: loss 0.4232456684112549\n",
      "epoch 70: loss 0.3629559874534607\n",
      "epoch 71: loss 0.32685208320617676\n",
      "epoch 72: loss 0.2566124498844147\n",
      "epoch 73: loss 0.31083914637565613\n",
      "epoch 74: loss 0.25637006759643555\n",
      "epoch 75: loss 0.3457789719104767\n",
      "epoch 76: loss 0.3085993230342865\n",
      "epoch 77: loss 0.3968884348869324\n",
      "epoch 78: loss 0.3022284507751465\n",
      "epoch 79: loss 0.3429735600948334\n",
      "epoch 80: loss 0.3854793310165405\n",
      "epoch 81: loss 0.2707756757736206\n",
      "epoch 82: loss 0.3149282932281494\n",
      "epoch 83: loss 0.3971152901649475\n",
      "epoch 84: loss 0.27789589762687683\n",
      "epoch 85: loss 0.2930249869823456\n",
      "epoch 86: loss 0.33949053287506104\n",
      "epoch 87: loss 0.3318592309951782\n",
      "epoch 88: loss 0.2806262969970703\n",
      "epoch 89: loss 0.36985158920288086\n",
      "epoch 90: loss 0.32055696845054626\n",
      "epoch 91: loss 0.4400845170021057\n",
      "epoch 92: loss 0.39072704315185547\n",
      "epoch 93: loss 0.30617713928222656\n",
      "epoch 94: loss 0.3149857819080353\n",
      "epoch 95: loss 0.18584869801998138\n",
      "epoch 96: loss 0.2350519597530365\n",
      "epoch 97: loss 0.3223530054092407\n",
      "epoch 98: loss 0.3578934073448181\n",
      "epoch 99: loss 0.3403264284133911\n",
      "epoch 100: loss 0.467701256275177\n",
      "epoch 101: loss 0.3035472333431244\n",
      "epoch 102: loss 0.25528445839881897\n",
      "epoch 103: loss 0.31090182065963745\n",
      "epoch 104: loss 0.25654441118240356\n",
      "epoch 105: loss 0.3618590831756592\n",
      "epoch 106: loss 0.3111913800239563\n",
      "epoch 107: loss 0.2719719111919403\n",
      "epoch 108: loss 0.3380904793739319\n",
      "epoch 109: loss 0.3475830554962158\n",
      "epoch 110: loss 0.3391071557998657\n",
      "epoch 111: loss 0.3813771605491638\n",
      "epoch 112: loss 0.36690962314605713\n",
      "epoch 113: loss 0.27061405777931213\n",
      "epoch 114: loss 0.15822583436965942\n",
      "epoch 115: loss 0.389928936958313\n",
      "epoch 116: loss 0.280649870634079\n",
      "epoch 117: loss 0.24974969029426575\n",
      "epoch 118: loss 0.3173671364784241\n",
      "epoch 119: loss 0.39522698521614075\n",
      "epoch 120: loss 0.26652705669403076\n",
      "epoch 121: loss 0.3648775815963745\n",
      "epoch 122: loss 0.24061241745948792\n",
      "epoch 123: loss 0.38168537616729736\n",
      "epoch 124: loss 0.3047848343849182\n",
      "epoch 125: loss 0.3387528359889984\n",
      "epoch 126: loss 0.3616800308227539\n",
      "epoch 127: loss 0.382522314786911\n",
      "epoch 128: loss 0.25391945242881775\n",
      "epoch 129: loss 0.3357539474964142\n",
      "epoch 130: loss 0.36428302526474\n",
      "epoch 131: loss 0.3154931664466858\n",
      "epoch 132: loss 0.3152945935726166\n",
      "epoch 133: loss 0.3459489345550537\n",
      "epoch 134: loss 0.303201824426651\n",
      "epoch 135: loss 0.2439093291759491\n",
      "epoch 136: loss 0.22020956873893738\n",
      "epoch 0: loss 0.32693955302238464\n",
      "epoch 1: loss 0.3355313837528229\n",
      "epoch 2: loss 0.368949294090271\n",
      "epoch 3: loss 0.3558213412761688\n",
      "epoch 4: loss 0.3009134531021118\n",
      "epoch 5: loss 0.24206025898456573\n",
      "epoch 6: loss 0.27033674716949463\n",
      "epoch 7: loss 0.21670761704444885\n",
      "epoch 8: loss 0.4200339913368225\n",
      "epoch 9: loss 0.4214624762535095\n",
      "epoch 10: loss 0.3187549114227295\n",
      "epoch 11: loss 0.26114872097969055\n",
      "epoch 12: loss 0.3194761276245117\n",
      "epoch 13: loss 0.41800177097320557\n",
      "epoch 14: loss 0.3405155539512634\n",
      "epoch 15: loss 0.26398545503616333\n",
      "epoch 16: loss 0.34303534030914307\n",
      "epoch 17: loss 0.32801252603530884\n",
      "epoch 18: loss 0.3726591169834137\n",
      "epoch 19: loss 0.25884878635406494\n",
      "epoch 20: loss 0.2897334098815918\n",
      "epoch 21: loss 0.32680559158325195\n",
      "epoch 22: loss 0.296928733587265\n",
      "epoch 23: loss 0.35439184308052063\n",
      "epoch 24: loss 0.3280295133590698\n",
      "epoch 25: loss 0.3829672336578369\n",
      "epoch 26: loss 0.32415705919265747\n",
      "epoch 27: loss 0.33252233266830444\n",
      "epoch 28: loss 0.40137624740600586\n",
      "epoch 29: loss 0.3085141181945801\n",
      "epoch 30: loss 0.2826687693595886\n",
      "epoch 31: loss 0.3313465714454651\n",
      "epoch 32: loss 0.33406245708465576\n",
      "epoch 33: loss 0.34134966135025024\n",
      "epoch 34: loss 0.37999245524406433\n",
      "epoch 35: loss 0.2572198510169983\n",
      "epoch 36: loss 0.32743310928344727\n",
      "epoch 37: loss 0.313888281583786\n",
      "epoch 38: loss 0.21784749627113342\n",
      "epoch 39: loss 0.2943917512893677\n",
      "epoch 40: loss 0.2901282012462616\n",
      "epoch 41: loss 0.27616339921951294\n",
      "epoch 42: loss 0.2561741769313812\n",
      "epoch 43: loss 0.3258773684501648\n",
      "epoch 44: loss 0.32764148712158203\n",
      "epoch 45: loss 0.281906396150589\n",
      "epoch 46: loss 0.28492429852485657\n",
      "epoch 47: loss 0.2826821804046631\n",
      "epoch 48: loss 0.23835057020187378\n",
      "epoch 49: loss 0.3278048634529114\n",
      "epoch 50: loss 0.27975836396217346\n",
      "epoch 51: loss 0.4000762104988098\n",
      "epoch 52: loss 0.2797030806541443\n",
      "epoch 53: loss 0.3534815311431885\n",
      "epoch 54: loss 0.34047287702560425\n",
      "epoch 55: loss 0.29124215245246887\n",
      "epoch 56: loss 0.34999534487724304\n",
      "epoch 57: loss 0.24901407957077026\n",
      "epoch 58: loss 0.31627118587493896\n",
      "epoch 59: loss 0.2631935775279999\n",
      "epoch 60: loss 0.3234504759311676\n",
      "epoch 61: loss 0.22620876133441925\n",
      "epoch 62: loss 0.2450612187385559\n",
      "epoch 63: loss 0.3542081117630005\n",
      "epoch 64: loss 0.27886953949928284\n",
      "epoch 65: loss 0.39426177740097046\n",
      "epoch 66: loss 0.3212777078151703\n",
      "epoch 67: loss 0.3797725439071655\n",
      "epoch 68: loss 0.3541748821735382\n",
      "epoch 69: loss 0.3996623754501343\n",
      "epoch 70: loss 0.3868182897567749\n",
      "epoch 71: loss 0.31292930245399475\n",
      "epoch 72: loss 0.24343618750572205\n",
      "epoch 73: loss 0.33036869764328003\n",
      "epoch 74: loss 0.27234917879104614\n",
      "epoch 75: loss 0.3464108109474182\n",
      "epoch 76: loss 0.30800551176071167\n",
      "epoch 77: loss 0.39989715814590454\n",
      "epoch 78: loss 0.3040148615837097\n",
      "epoch 79: loss 0.3201160430908203\n",
      "epoch 80: loss 0.37020593881607056\n",
      "epoch 81: loss 0.26372626423835754\n",
      "epoch 82: loss 0.31688782572746277\n",
      "epoch 83: loss 0.3897874355316162\n",
      "epoch 84: loss 0.251659095287323\n",
      "epoch 85: loss 0.29344481229782104\n",
      "epoch 86: loss 0.33607017993927\n",
      "epoch 87: loss 0.3391166925430298\n",
      "epoch 88: loss 0.2686229646205902\n",
      "epoch 89: loss 0.3440430164337158\n",
      "epoch 90: loss 0.29805636405944824\n",
      "epoch 91: loss 0.38158559799194336\n",
      "epoch 92: loss 0.34354180097579956\n",
      "epoch 93: loss 0.3109146058559418\n",
      "epoch 94: loss 0.295866459608078\n",
      "epoch 95: loss 0.20024222135543823\n",
      "epoch 96: loss 0.23796802759170532\n",
      "epoch 97: loss 0.30445432662963867\n",
      "epoch 98: loss 0.32253193855285645\n",
      "epoch 99: loss 0.33049044013023376\n",
      "epoch 100: loss 0.4566928744316101\n",
      "epoch 101: loss 0.3016929030418396\n",
      "epoch 102: loss 0.25685054063796997\n",
      "epoch 103: loss 0.31233152747154236\n",
      "epoch 104: loss 0.25622284412384033\n",
      "epoch 105: loss 0.36637866497039795\n",
      "epoch 106: loss 0.31235259771347046\n",
      "epoch 107: loss 0.26531654596328735\n",
      "epoch 108: loss 0.3410787284374237\n",
      "epoch 109: loss 0.3432425558567047\n",
      "epoch 110: loss 0.34135305881500244\n",
      "epoch 111: loss 0.3728376626968384\n",
      "epoch 112: loss 0.36035725474357605\n",
      "epoch 113: loss 0.2724699378013611\n",
      "epoch 114: loss 0.15191751718521118\n",
      "epoch 115: loss 0.38920801877975464\n",
      "epoch 116: loss 0.2796053886413574\n",
      "epoch 117: loss 0.2510814964771271\n",
      "epoch 118: loss 0.31727802753448486\n",
      "epoch 119: loss 0.3963562846183777\n",
      "epoch 120: loss 0.26160427927970886\n",
      "epoch 121: loss 0.35771965980529785\n",
      "epoch 122: loss 0.23153036832809448\n",
      "epoch 123: loss 0.37851089239120483\n",
      "epoch 124: loss 0.313052773475647\n",
      "epoch 125: loss 0.3422399163246155\n",
      "epoch 126: loss 0.3812773823738098\n",
      "epoch 127: loss 0.4129955470561981\n",
      "epoch 128: loss 0.2757911682128906\n",
      "epoch 129: loss 0.34949827194213867\n",
      "epoch 130: loss 0.36813563108444214\n",
      "epoch 131: loss 0.319831520318985\n",
      "epoch 132: loss 0.3121844530105591\n",
      "epoch 133: loss 0.3405369818210602\n",
      "epoch 134: loss 0.3090673089027405\n",
      "epoch 135: loss 0.2675864100456238\n",
      "epoch 136: loss 0.23798152804374695\n",
      "epoch 0: loss 0.34481218457221985\n",
      "epoch 1: loss 0.32332366704940796\n",
      "epoch 2: loss 0.4154474437236786\n",
      "epoch 3: loss 0.3493614196777344\n",
      "epoch 4: loss 0.2963981032371521\n",
      "epoch 5: loss 0.259930282831192\n",
      "epoch 6: loss 0.27371740341186523\n",
      "epoch 7: loss 0.2285732924938202\n",
      "epoch 8: loss 0.39754176139831543\n",
      "epoch 9: loss 0.40023869276046753\n",
      "epoch 10: loss 0.3207063674926758\n",
      "epoch 11: loss 0.27660080790519714\n",
      "epoch 12: loss 0.2960662841796875\n",
      "epoch 13: loss 0.4311874508857727\n",
      "epoch 14: loss 0.3327188491821289\n",
      "epoch 15: loss 0.26645171642303467\n",
      "epoch 16: loss 0.3451891839504242\n",
      "epoch 17: loss 0.3275674283504486\n",
      "epoch 18: loss 0.36223405599594116\n",
      "epoch 19: loss 0.27597442269325256\n",
      "epoch 20: loss 0.2904658913612366\n",
      "epoch 21: loss 0.32059454917907715\n",
      "epoch 22: loss 0.31617701053619385\n",
      "epoch 23: loss 0.37234607338905334\n",
      "epoch 24: loss 0.3569449782371521\n",
      "epoch 25: loss 0.380367249250412\n",
      "epoch 26: loss 0.31360214948654175\n",
      "epoch 27: loss 0.33215808868408203\n",
      "epoch 28: loss 0.39068400859832764\n",
      "epoch 29: loss 0.3136991560459137\n",
      "epoch 30: loss 0.2809755504131317\n",
      "epoch 31: loss 0.33372533321380615\n",
      "epoch 32: loss 0.3326650857925415\n",
      "epoch 33: loss 0.33598852157592773\n",
      "epoch 34: loss 0.37520453333854675\n",
      "epoch 35: loss 0.2699488699436188\n",
      "epoch 36: loss 0.33087584376335144\n",
      "epoch 37: loss 0.34744441509246826\n",
      "epoch 38: loss 0.23922768235206604\n",
      "epoch 39: loss 0.32949298620224\n",
      "epoch 40: loss 0.3270092010498047\n",
      "epoch 41: loss 0.2816471457481384\n",
      "epoch 42: loss 0.244665265083313\n",
      "epoch 43: loss 0.33487623929977417\n",
      "epoch 44: loss 0.34346798062324524\n",
      "epoch 45: loss 0.2762387692928314\n",
      "epoch 46: loss 0.29053834080696106\n",
      "epoch 47: loss 0.2848513722419739\n",
      "epoch 48: loss 0.24175482988357544\n",
      "epoch 49: loss 0.31454795598983765\n",
      "epoch 50: loss 0.2820042371749878\n",
      "epoch 51: loss 0.41602855920791626\n",
      "epoch 52: loss 0.31800657510757446\n",
      "epoch 53: loss 0.35857221484184265\n",
      "epoch 54: loss 0.3371739983558655\n",
      "epoch 55: loss 0.30532485246658325\n",
      "epoch 56: loss 0.343487024307251\n",
      "epoch 57: loss 0.24252691864967346\n",
      "epoch 58: loss 0.31926798820495605\n",
      "epoch 59: loss 0.25983577966690063\n",
      "epoch 60: loss 0.32341650128364563\n",
      "epoch 61: loss 0.21936844289302826\n",
      "epoch 62: loss 0.23909078538417816\n",
      "epoch 63: loss 0.3313877284526825\n",
      "epoch 64: loss 0.2775229811668396\n",
      "epoch 65: loss 0.3820369839668274\n",
      "epoch 66: loss 0.3248264193534851\n",
      "epoch 67: loss 0.3757677376270294\n",
      "epoch 68: loss 0.3541843295097351\n",
      "epoch 69: loss 0.4191513657569885\n",
      "epoch 70: loss 0.3678601384162903\n",
      "epoch 71: loss 0.31421154737472534\n",
      "epoch 72: loss 0.25722411274909973\n",
      "epoch 73: loss 0.31522613763809204\n",
      "epoch 74: loss 0.25359055399894714\n",
      "epoch 75: loss 0.34341493248939514\n",
      "epoch 76: loss 0.3146457076072693\n",
      "epoch 77: loss 0.39469820261001587\n",
      "epoch 78: loss 0.3062581419944763\n",
      "epoch 79: loss 0.31430941820144653\n",
      "epoch 80: loss 0.38047266006469727\n",
      "epoch 81: loss 0.2683025002479553\n",
      "epoch 82: loss 0.32417550683021545\n",
      "epoch 83: loss 0.3676447868347168\n",
      "epoch 84: loss 0.3193485140800476\n",
      "epoch 85: loss 0.29430246353149414\n",
      "epoch 86: loss 0.3372952342033386\n",
      "epoch 87: loss 0.33207136392593384\n",
      "epoch 88: loss 0.2867242991924286\n",
      "epoch 89: loss 0.3790437579154968\n",
      "epoch 90: loss 0.3251282870769501\n",
      "epoch 91: loss 0.448035329580307\n",
      "epoch 92: loss 0.39319300651550293\n",
      "epoch 93: loss 0.304021418094635\n",
      "epoch 94: loss 0.3119578957557678\n",
      "epoch 95: loss 0.18831537663936615\n",
      "epoch 96: loss 0.23817841708660126\n",
      "epoch 97: loss 0.31989458203315735\n",
      "epoch 98: loss 0.3564431965351105\n",
      "epoch 99: loss 0.3411164879798889\n",
      "epoch 100: loss 0.4677773118019104\n",
      "epoch 101: loss 0.30627208948135376\n",
      "epoch 102: loss 0.25874561071395874\n",
      "epoch 103: loss 0.30917274951934814\n",
      "epoch 104: loss 0.2612641453742981\n",
      "epoch 105: loss 0.3639521598815918\n",
      "epoch 106: loss 0.3117707371711731\n",
      "epoch 107: loss 0.27014100551605225\n",
      "epoch 108: loss 0.33879223465919495\n",
      "epoch 109: loss 0.34375855326652527\n",
      "epoch 110: loss 0.3418329060077667\n",
      "epoch 111: loss 0.3796465992927551\n",
      "epoch 112: loss 0.3666590452194214\n",
      "epoch 113: loss 0.2696000933647156\n",
      "epoch 114: loss 0.15371006727218628\n",
      "epoch 115: loss 0.3893586993217468\n",
      "epoch 116: loss 0.27852487564086914\n",
      "epoch 117: loss 0.251220166683197\n",
      "epoch 118: loss 0.31790897250175476\n",
      "epoch 119: loss 0.396517276763916\n",
      "epoch 120: loss 0.262739896774292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 121: loss 0.36712971329689026\n",
      "epoch 122: loss 0.23434197902679443\n",
      "epoch 123: loss 0.3767397999763489\n",
      "epoch 124: loss 0.3201954960823059\n",
      "epoch 125: loss 0.34190595149993896\n",
      "epoch 126: loss 0.377008855342865\n",
      "epoch 127: loss 0.4219154119491577\n",
      "epoch 128: loss 0.2759777307510376\n",
      "epoch 129: loss 0.35271650552749634\n",
      "epoch 130: loss 0.36884504556655884\n",
      "epoch 131: loss 0.3171008825302124\n",
      "epoch 132: loss 0.31421971321105957\n",
      "epoch 133: loss 0.3483296036720276\n",
      "epoch 134: loss 0.31060582399368286\n",
      "epoch 135: loss 0.2541762590408325\n",
      "epoch 136: loss 0.23035721480846405\n",
      "epoch 0: loss 0.3476243019104004\n",
      "epoch 1: loss 0.32462209463119507\n",
      "epoch 2: loss 0.4167634844779968\n",
      "epoch 3: loss 0.3492060601711273\n",
      "epoch 4: loss 0.2952440679073334\n",
      "epoch 5: loss 0.25686582922935486\n",
      "epoch 6: loss 0.2772396504878998\n",
      "epoch 7: loss 0.23018822073936462\n",
      "epoch 8: loss 0.39722317457199097\n",
      "epoch 9: loss 0.3984944224357605\n",
      "epoch 10: loss 0.33266130089759827\n",
      "epoch 11: loss 0.2830193042755127\n",
      "epoch 12: loss 0.2944920063018799\n",
      "epoch 13: loss 0.43562817573547363\n",
      "epoch 14: loss 0.33032551407814026\n",
      "epoch 15: loss 0.267557293176651\n",
      "epoch 16: loss 0.34526878595352173\n",
      "epoch 17: loss 0.32908034324645996\n",
      "epoch 18: loss 0.3624013364315033\n",
      "epoch 19: loss 0.27310651540756226\n",
      "epoch 20: loss 0.2930593192577362\n",
      "epoch 21: loss 0.32214003801345825\n",
      "epoch 22: loss 0.3163778781890869\n",
      "epoch 23: loss 0.37143996357917786\n",
      "epoch 24: loss 0.35732564330101013\n",
      "epoch 25: loss 0.382294237613678\n",
      "epoch 26: loss 0.30481570959091187\n",
      "epoch 27: loss 0.3357240557670593\n",
      "epoch 28: loss 0.3830704092979431\n",
      "epoch 29: loss 0.3136579394340515\n",
      "epoch 30: loss 0.2799152135848999\n",
      "epoch 31: loss 0.33835700154304504\n",
      "epoch 32: loss 0.32630428671836853\n",
      "epoch 33: loss 0.34264519810676575\n",
      "epoch 34: loss 0.3774939775466919\n",
      "epoch 35: loss 0.2581046223640442\n",
      "epoch 36: loss 0.3251084089279175\n",
      "epoch 37: loss 0.32578104734420776\n",
      "epoch 38: loss 0.22389166057109833\n",
      "epoch 39: loss 0.3174908757209778\n",
      "epoch 40: loss 0.31502577662467957\n",
      "epoch 41: loss 0.27888667583465576\n",
      "epoch 42: loss 0.24203872680664062\n",
      "epoch 43: loss 0.3308221697807312\n",
      "epoch 44: loss 0.33474093675613403\n",
      "epoch 45: loss 0.28034836053848267\n",
      "epoch 46: loss 0.2836630046367645\n",
      "epoch 47: loss 0.2809050977230072\n",
      "epoch 48: loss 0.2413206696510315\n",
      "epoch 49: loss 0.3171958327293396\n",
      "epoch 50: loss 0.2806657552719116\n",
      "epoch 51: loss 0.41359052062034607\n",
      "epoch 52: loss 0.3069704473018646\n",
      "epoch 53: loss 0.35409194231033325\n",
      "epoch 54: loss 0.34263378381729126\n",
      "epoch 55: loss 0.29797446727752686\n",
      "epoch 56: loss 0.3442138135433197\n",
      "epoch 57: loss 0.24504515528678894\n",
      "epoch 58: loss 0.3185167610645294\n",
      "epoch 59: loss 0.2649833858013153\n",
      "epoch 60: loss 0.3164404630661011\n",
      "epoch 61: loss 0.217647984623909\n",
      "epoch 62: loss 0.23919105529785156\n",
      "epoch 63: loss 0.3360013961791992\n",
      "epoch 64: loss 0.2731037735939026\n",
      "epoch 65: loss 0.3870413303375244\n",
      "epoch 66: loss 0.3218378722667694\n",
      "epoch 67: loss 0.37447285652160645\n",
      "epoch 68: loss 0.35203054547309875\n",
      "epoch 69: loss 0.40088585019111633\n",
      "epoch 70: loss 0.3735654354095459\n",
      "epoch 71: loss 0.3165464401245117\n",
      "epoch 72: loss 0.24159030616283417\n",
      "epoch 73: loss 0.3232114315032959\n",
      "epoch 74: loss 0.26830998063087463\n",
      "epoch 75: loss 0.3478093147277832\n",
      "epoch 76: loss 0.30873364210128784\n",
      "epoch 77: loss 0.40219026803970337\n",
      "epoch 78: loss 0.30636465549468994\n",
      "epoch 79: loss 0.3201901316642761\n",
      "epoch 80: loss 0.37151679396629333\n",
      "epoch 81: loss 0.26383379101753235\n",
      "epoch 82: loss 0.31776297092437744\n",
      "epoch 83: loss 0.3920336663722992\n",
      "epoch 84: loss 0.24883787333965302\n",
      "epoch 85: loss 0.2932795584201813\n",
      "epoch 86: loss 0.3392982482910156\n",
      "epoch 87: loss 0.336999773979187\n",
      "epoch 88: loss 0.2723642885684967\n",
      "epoch 89: loss 0.34665238857269287\n",
      "epoch 90: loss 0.29943960905075073\n",
      "epoch 91: loss 0.3871793746948242\n",
      "epoch 92: loss 0.35095900297164917\n",
      "epoch 93: loss 0.3095300793647766\n",
      "epoch 94: loss 0.29562175273895264\n",
      "epoch 95: loss 0.2003842145204544\n",
      "epoch 96: loss 0.23779527842998505\n",
      "epoch 97: loss 0.2997888922691345\n",
      "epoch 98: loss 0.3170331120491028\n",
      "epoch 99: loss 0.3294851779937744\n",
      "epoch 100: loss 0.4551801085472107\n",
      "epoch 101: loss 0.30130735039711\n",
      "epoch 102: loss 0.25794705748558044\n",
      "epoch 103: loss 0.31497007608413696\n",
      "epoch 104: loss 0.25661560893058777\n",
      "epoch 105: loss 0.3676877021789551\n",
      "epoch 106: loss 0.31014299392700195\n",
      "epoch 107: loss 0.263359934091568\n",
      "epoch 108: loss 0.3447168469429016\n",
      "epoch 109: loss 0.3458545506000519\n",
      "epoch 110: loss 0.3411853313446045\n",
      "epoch 111: loss 0.3673274517059326\n",
      "epoch 112: loss 0.35843008756637573\n",
      "epoch 113: loss 0.2708187997341156\n",
      "epoch 114: loss 0.15309742093086243\n",
      "epoch 115: loss 0.3864218294620514\n",
      "epoch 116: loss 0.27791059017181396\n",
      "epoch 117: loss 0.25533950328826904\n",
      "epoch 118: loss 0.31674960255622864\n",
      "epoch 119: loss 0.3963218927383423\n",
      "epoch 120: loss 0.2575061321258545\n",
      "epoch 121: loss 0.36456993222236633\n",
      "epoch 122: loss 0.22864747047424316\n",
      "epoch 123: loss 0.3768587112426758\n",
      "epoch 124: loss 0.33959460258483887\n",
      "epoch 125: loss 0.3441891074180603\n",
      "epoch 126: loss 0.3821401596069336\n",
      "epoch 127: loss 0.434263676404953\n",
      "epoch 128: loss 0.31989872455596924\n",
      "epoch 129: loss 0.36037322878837585\n",
      "epoch 130: loss 0.45437395572662354\n",
      "epoch 131: loss 0.36133652925491333\n",
      "epoch 132: loss 0.3788530230522156\n",
      "epoch 133: loss 0.40003347396850586\n",
      "epoch 134: loss 0.33210402727127075\n",
      "epoch 135: loss 0.3100169599056244\n",
      "epoch 136: loss 0.290047824382782\n",
      "epoch 0: loss 0.3696932792663574\n",
      "epoch 1: loss 0.3486705422401428\n",
      "epoch 2: loss 0.38981425762176514\n",
      "epoch 3: loss 0.3755987882614136\n",
      "epoch 4: loss 0.3331567645072937\n",
      "epoch 5: loss 0.26289623975753784\n",
      "epoch 6: loss 0.27602577209472656\n",
      "epoch 7: loss 0.2219322919845581\n",
      "epoch 8: loss 0.4051780104637146\n",
      "epoch 9: loss 0.4159916639328003\n",
      "epoch 10: loss 0.33323991298675537\n",
      "epoch 11: loss 0.2823636829853058\n",
      "epoch 12: loss 0.3278222680091858\n",
      "epoch 13: loss 0.42320191860198975\n",
      "epoch 14: loss 0.3713151216506958\n",
      "epoch 15: loss 0.2822248935699463\n",
      "epoch 16: loss 0.35584402084350586\n",
      "epoch 17: loss 0.3389762043952942\n",
      "epoch 18: loss 0.38801994919776917\n",
      "epoch 19: loss 0.25111135840415955\n",
      "epoch 20: loss 0.2911779582500458\n",
      "epoch 21: loss 0.3315659463405609\n",
      "epoch 22: loss 0.30808597803115845\n",
      "epoch 23: loss 0.35342156887054443\n",
      "epoch 24: loss 0.3549751043319702\n",
      "epoch 25: loss 0.38427674770355225\n",
      "epoch 26: loss 0.308206170797348\n",
      "epoch 27: loss 0.3424330949783325\n",
      "epoch 28: loss 0.4127386212348938\n",
      "epoch 29: loss 0.3100759983062744\n",
      "epoch 30: loss 0.2812389135360718\n",
      "epoch 31: loss 0.3402954339981079\n",
      "epoch 32: loss 0.3332597017288208\n",
      "epoch 33: loss 0.3426329791545868\n",
      "epoch 34: loss 0.37807005643844604\n",
      "epoch 35: loss 0.26300299167633057\n",
      "epoch 36: loss 0.3375682830810547\n",
      "epoch 37: loss 0.3052572011947632\n",
      "epoch 38: loss 0.2156635820865631\n",
      "epoch 39: loss 0.2914620637893677\n",
      "epoch 40: loss 0.29595091938972473\n",
      "epoch 41: loss 0.2791672348976135\n",
      "epoch 42: loss 0.25046008825302124\n",
      "epoch 43: loss 0.3099428713321686\n",
      "epoch 44: loss 0.3390950858592987\n",
      "epoch 45: loss 0.2782956659793854\n",
      "epoch 46: loss 0.27700066566467285\n",
      "epoch 47: loss 0.2810107469558716\n",
      "epoch 48: loss 0.26681214570999146\n",
      "epoch 49: loss 0.340126097202301\n",
      "epoch 50: loss 0.28263986110687256\n",
      "epoch 51: loss 0.40972307324409485\n",
      "epoch 52: loss 0.3092375695705414\n",
      "epoch 53: loss 0.3580579459667206\n",
      "epoch 54: loss 0.3428979814052582\n",
      "epoch 55: loss 0.302410364151001\n",
      "epoch 56: loss 0.34994491934776306\n",
      "epoch 57: loss 0.24525538086891174\n",
      "epoch 58: loss 0.32489126920700073\n",
      "epoch 59: loss 0.26241737604141235\n",
      "epoch 60: loss 0.3283507823944092\n",
      "epoch 61: loss 0.21404694020748138\n",
      "epoch 62: loss 0.238567054271698\n",
      "epoch 63: loss 0.3313872814178467\n",
      "epoch 64: loss 0.2804928719997406\n",
      "epoch 65: loss 0.3908213973045349\n",
      "epoch 66: loss 0.3269387483596802\n",
      "epoch 67: loss 0.36680757999420166\n",
      "epoch 68: loss 0.35606464743614197\n",
      "epoch 69: loss 0.42499232292175293\n",
      "epoch 70: loss 0.36954009532928467\n",
      "epoch 71: loss 0.3171788454055786\n",
      "epoch 72: loss 0.2650354206562042\n",
      "epoch 73: loss 0.32033777236938477\n",
      "epoch 74: loss 0.2643737196922302\n",
      "epoch 75: loss 0.33571234345436096\n",
      "epoch 76: loss 0.3065990209579468\n",
      "epoch 77: loss 0.3950134217739105\n",
      "epoch 78: loss 0.30615925788879395\n",
      "epoch 79: loss 0.33013880252838135\n",
      "epoch 80: loss 0.379075825214386\n",
      "epoch 81: loss 0.26834872364997864\n",
      "epoch 82: loss 0.31609877943992615\n",
      "epoch 83: loss 0.3752140402793884\n",
      "epoch 84: loss 0.2940116822719574\n",
      "epoch 85: loss 0.29654568433761597\n",
      "epoch 86: loss 0.34105050563812256\n",
      "epoch 87: loss 0.33506515622138977\n",
      "epoch 88: loss 0.2888069152832031\n",
      "epoch 89: loss 0.38382667303085327\n",
      "epoch 90: loss 0.3240603804588318\n",
      "epoch 91: loss 0.44914308190345764\n",
      "epoch 92: loss 0.3847070634365082\n",
      "epoch 93: loss 0.3060781955718994\n",
      "epoch 94: loss 0.298709511756897\n",
      "epoch 95: loss 0.2033591866493225\n",
      "epoch 96: loss 0.2497289478778839\n",
      "epoch 97: loss 0.3181300759315491\n",
      "epoch 98: loss 0.32842129468917847\n",
      "epoch 99: loss 0.33093351125717163\n",
      "epoch 100: loss 0.45046719908714294\n",
      "epoch 101: loss 0.30431750416755676\n",
      "epoch 102: loss 0.261618971824646\n",
      "epoch 103: loss 0.3091199994087219\n",
      "epoch 104: loss 0.26092368364334106\n",
      "epoch 105: loss 0.3675745129585266\n",
      "epoch 106: loss 0.3141314387321472\n",
      "epoch 107: loss 0.27192121744155884\n",
      "epoch 108: loss 0.342210978269577\n",
      "epoch 109: loss 0.34123876690864563\n",
      "epoch 110: loss 0.344130277633667\n",
      "epoch 111: loss 0.3743455111980438\n",
      "epoch 112: loss 0.36392468214035034\n",
      "epoch 113: loss 0.27493107318878174\n",
      "epoch 114: loss 0.15654060244560242\n",
      "epoch 115: loss 0.3930290639400482\n",
      "epoch 116: loss 0.28276604413986206\n",
      "epoch 117: loss 0.24654057621955872\n",
      "epoch 118: loss 0.3168138563632965\n",
      "epoch 119: loss 0.3924584686756134\n",
      "epoch 120: loss 0.26217061281204224\n",
      "epoch 121: loss 0.35338157415390015\n",
      "epoch 122: loss 0.2406526803970337\n",
      "epoch 123: loss 0.38081616163253784\n",
      "epoch 124: loss 0.30312442779541016\n",
      "epoch 125: loss 0.3359314203262329\n",
      "epoch 126: loss 0.36765778064727783\n",
      "epoch 127: loss 0.3918708562850952\n",
      "epoch 128: loss 0.25525808334350586\n",
      "epoch 129: loss 0.33641496300697327\n",
      "epoch 130: loss 0.36050477623939514\n",
      "epoch 131: loss 0.33046281337738037\n",
      "epoch 132: loss 0.3164847493171692\n",
      "epoch 133: loss 0.33963876962661743\n",
      "epoch 134: loss 0.31308889389038086\n",
      "epoch 135: loss 0.2501114010810852\n",
      "epoch 136: loss 0.2277606576681137\n",
      "epoch 0: loss 0.343853622674942\n",
      "epoch 1: loss 0.3245013952255249\n",
      "epoch 2: loss 0.39287269115448\n",
      "epoch 3: loss 0.3497616946697235\n",
      "epoch 4: loss 0.3559165596961975\n",
      "epoch 5: loss 0.2505691647529602\n",
      "epoch 6: loss 0.27843254804611206\n",
      "epoch 7: loss 0.23069743812084198\n",
      "epoch 8: loss 0.4321361482143402\n",
      "epoch 9: loss 0.4495953917503357\n",
      "epoch 10: loss 0.3581780195236206\n",
      "epoch 11: loss 0.2816915512084961\n",
      "epoch 12: loss 0.31019455194473267\n",
      "epoch 13: loss 0.405878484249115\n",
      "epoch 14: loss 0.4199576675891876\n",
      "epoch 15: loss 0.27224427461624146\n",
      "epoch 16: loss 0.34546318650245667\n",
      "epoch 17: loss 0.36358362436294556\n",
      "epoch 18: loss 0.4185760021209717\n",
      "epoch 19: loss 0.29084348678588867\n",
      "epoch 20: loss 0.3280682861804962\n",
      "epoch 21: loss 0.3496207594871521\n",
      "epoch 22: loss 0.32600075006484985\n",
      "epoch 23: loss 0.3858060836791992\n",
      "epoch 24: loss 0.349891722202301\n",
      "epoch 25: loss 0.4205913245677948\n",
      "epoch 26: loss 0.3366677463054657\n",
      "epoch 27: loss 0.3426610827445984\n",
      "epoch 28: loss 0.4010269045829773\n",
      "epoch 29: loss 0.31637346744537354\n",
      "epoch 30: loss 0.2820854187011719\n",
      "epoch 31: loss 0.3289709687232971\n",
      "epoch 32: loss 0.34516632556915283\n",
      "epoch 33: loss 0.3461745083332062\n",
      "epoch 34: loss 0.394464910030365\n",
      "epoch 35: loss 0.25266343355178833\n",
      "epoch 36: loss 0.33096379041671753\n",
      "epoch 37: loss 0.3163144588470459\n",
      "epoch 38: loss 0.21979942917823792\n",
      "epoch 39: loss 0.30510562658309937\n",
      "epoch 40: loss 0.29431217908859253\n",
      "epoch 41: loss 0.27544325590133667\n",
      "epoch 42: loss 0.25882411003112793\n",
      "epoch 43: loss 0.3335843086242676\n",
      "epoch 44: loss 0.3316330313682556\n",
      "epoch 45: loss 0.2793745696544647\n",
      "epoch 46: loss 0.2846047878265381\n",
      "epoch 47: loss 0.28276360034942627\n",
      "epoch 48: loss 0.24732382595539093\n",
      "epoch 49: loss 0.3241199254989624\n",
      "epoch 50: loss 0.27853864431381226\n",
      "epoch 51: loss 0.40099379420280457\n",
      "epoch 52: loss 0.292145311832428\n",
      "epoch 53: loss 0.35831165313720703\n",
      "epoch 54: loss 0.34494102001190186\n",
      "epoch 55: loss 0.29478543996810913\n",
      "epoch 56: loss 0.3467106223106384\n",
      "epoch 57: loss 0.24035759270191193\n",
      "epoch 58: loss 0.32157382369041443\n",
      "epoch 59: loss 0.2636585235595703\n",
      "epoch 60: loss 0.3203057646751404\n",
      "epoch 61: loss 0.21700680255889893\n",
      "epoch 62: loss 0.23715241253376007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 63: loss 0.3322314918041229\n",
      "epoch 64: loss 0.27831876277923584\n",
      "epoch 65: loss 0.38879963755607605\n",
      "epoch 66: loss 0.32472336292266846\n",
      "epoch 67: loss 0.3759390115737915\n",
      "epoch 68: loss 0.35459205508232117\n",
      "epoch 69: loss 0.4126867651939392\n",
      "epoch 70: loss 0.368717759847641\n",
      "epoch 71: loss 0.3238111138343811\n",
      "epoch 72: loss 0.24880225956439972\n",
      "epoch 73: loss 0.31679385900497437\n",
      "epoch 74: loss 0.26321834325790405\n",
      "epoch 75: loss 0.34694617986679077\n",
      "epoch 76: loss 0.30712175369262695\n",
      "epoch 77: loss 0.4050050973892212\n",
      "epoch 78: loss 0.304379403591156\n",
      "epoch 79: loss 0.33133065700531006\n",
      "epoch 80: loss 0.3764028549194336\n",
      "epoch 81: loss 0.2668599784374237\n",
      "epoch 82: loss 0.3142870366573334\n",
      "epoch 83: loss 0.3921254873275757\n",
      "epoch 84: loss 0.26220476627349854\n",
      "epoch 85: loss 0.2935798168182373\n",
      "epoch 86: loss 0.3379501700401306\n",
      "epoch 87: loss 0.3354632258415222\n",
      "epoch 88: loss 0.2794652581214905\n",
      "epoch 89: loss 0.36500728130340576\n",
      "epoch 90: loss 0.31156259775161743\n",
      "epoch 91: loss 0.4190683662891388\n",
      "epoch 92: loss 0.3737359046936035\n",
      "epoch 93: loss 0.3084835112094879\n",
      "epoch 94: loss 0.29847264289855957\n",
      "epoch 95: loss 0.1948593705892563\n",
      "epoch 96: loss 0.23881706595420837\n",
      "epoch 97: loss 0.3124961256980896\n",
      "epoch 98: loss 0.32919037342071533\n",
      "epoch 99: loss 0.3278663158416748\n",
      "epoch 100: loss 0.45398491621017456\n",
      "epoch 101: loss 0.29846349358558655\n",
      "epoch 102: loss 0.2549075782299042\n",
      "epoch 103: loss 0.30405551195144653\n",
      "epoch 104: loss 0.25116702914237976\n",
      "epoch 105: loss 0.3594561219215393\n",
      "epoch 106: loss 0.30759990215301514\n",
      "epoch 107: loss 0.267722487449646\n",
      "epoch 108: loss 0.3380921185016632\n",
      "epoch 109: loss 0.3450612425804138\n",
      "epoch 110: loss 0.3337969183921814\n",
      "epoch 111: loss 0.3829559087753296\n",
      "epoch 112: loss 0.36308103799819946\n",
      "epoch 113: loss 0.26856037974357605\n",
      "epoch 114: loss 0.14977094531059265\n",
      "epoch 115: loss 0.3857203722000122\n",
      "epoch 116: loss 0.2781980037689209\n",
      "epoch 117: loss 0.2509602904319763\n",
      "epoch 118: loss 0.3186630606651306\n",
      "epoch 119: loss 0.4005889892578125\n",
      "epoch 120: loss 0.26210352778434753\n",
      "epoch 121: loss 0.35537415742874146\n",
      "epoch 122: loss 0.2329850196838379\n",
      "epoch 123: loss 0.38163769245147705\n",
      "epoch 124: loss 0.3177098035812378\n",
      "epoch 125: loss 0.34333157539367676\n",
      "epoch 126: loss 0.38057610392570496\n",
      "epoch 127: loss 0.4221131205558777\n",
      "epoch 128: loss 0.2898029685020447\n",
      "epoch 129: loss 0.3511925935745239\n",
      "epoch 130: loss 0.4054175615310669\n",
      "epoch 131: loss 0.3332492709159851\n",
      "epoch 132: loss 0.3339490294456482\n",
      "epoch 133: loss 0.3736339807510376\n",
      "epoch 134: loss 0.32073745131492615\n",
      "epoch 135: loss 0.27188625931739807\n",
      "epoch 136: loss 0.2608855068683624\n",
      "epoch 0: loss 0.3477635085582733\n",
      "epoch 1: loss 0.3449484407901764\n",
      "epoch 2: loss 0.3688240945339203\n",
      "epoch 3: loss 0.35400301218032837\n",
      "epoch 4: loss 0.3060203790664673\n",
      "epoch 5: loss 0.25866609811782837\n",
      "epoch 6: loss 0.2735911011695862\n",
      "epoch 7: loss 0.2145383507013321\n",
      "epoch 8: loss 0.40560483932495117\n",
      "epoch 9: loss 0.40430864691734314\n",
      "epoch 10: loss 0.3206605911254883\n",
      "epoch 11: loss 0.2686666250228882\n",
      "epoch 12: loss 0.31700485944747925\n",
      "epoch 13: loss 0.41529589891433716\n",
      "epoch 14: loss 0.3670480251312256\n",
      "epoch 15: loss 0.27198368310928345\n",
      "epoch 16: loss 0.35096633434295654\n",
      "epoch 17: loss 0.32953938841819763\n",
      "epoch 18: loss 0.38641273975372314\n",
      "epoch 19: loss 0.25943899154663086\n",
      "epoch 20: loss 0.29688480496406555\n",
      "epoch 21: loss 0.3280870318412781\n",
      "epoch 22: loss 0.3129342198371887\n",
      "epoch 23: loss 0.35851842164993286\n",
      "epoch 24: loss 0.3484992980957031\n",
      "epoch 25: loss 0.38391825556755066\n",
      "epoch 26: loss 0.3099132478237152\n",
      "epoch 27: loss 0.3359628915786743\n",
      "epoch 28: loss 0.39759910106658936\n",
      "epoch 29: loss 0.3088434338569641\n",
      "epoch 30: loss 0.2755829989910126\n",
      "epoch 31: loss 0.3360016942024231\n",
      "epoch 32: loss 0.33635208010673523\n",
      "epoch 33: loss 0.34673887491226196\n",
      "epoch 34: loss 0.39087218046188354\n",
      "epoch 35: loss 0.2554832398891449\n",
      "epoch 36: loss 0.32915812730789185\n",
      "epoch 37: loss 0.31799906492233276\n",
      "epoch 38: loss 0.21791450679302216\n",
      "epoch 39: loss 0.30952417850494385\n",
      "epoch 40: loss 0.2967245578765869\n",
      "epoch 41: loss 0.2772001624107361\n",
      "epoch 42: loss 0.2510586380958557\n",
      "epoch 43: loss 0.33007198572158813\n",
      "epoch 44: loss 0.32899802923202515\n",
      "epoch 45: loss 0.2789899706840515\n",
      "epoch 46: loss 0.28456997871398926\n",
      "epoch 47: loss 0.28211653232574463\n",
      "epoch 48: loss 0.24325048923492432\n",
      "epoch 49: loss 0.32160520553588867\n",
      "epoch 50: loss 0.28050634264945984\n",
      "epoch 51: loss 0.39932650327682495\n",
      "epoch 52: loss 0.28548967838287354\n",
      "epoch 53: loss 0.3558914065361023\n",
      "epoch 54: loss 0.3483958840370178\n",
      "epoch 55: loss 0.2910382151603699\n",
      "epoch 56: loss 0.3549199104309082\n",
      "epoch 57: loss 0.24851667881011963\n",
      "epoch 58: loss 0.31955865025520325\n",
      "epoch 59: loss 0.2617127299308777\n",
      "epoch 60: loss 0.3256882131099701\n",
      "epoch 61: loss 0.22334207594394684\n",
      "epoch 62: loss 0.2440555989742279\n",
      "epoch 63: loss 0.3528779149055481\n",
      "epoch 64: loss 0.2814531922340393\n",
      "epoch 65: loss 0.39782819151878357\n",
      "epoch 66: loss 0.32562142610549927\n",
      "epoch 67: loss 0.36958855390548706\n",
      "epoch 68: loss 0.3545153737068176\n",
      "epoch 69: loss 0.4021153450012207\n",
      "epoch 70: loss 0.4048120379447937\n",
      "epoch 71: loss 0.3047570586204529\n",
      "epoch 72: loss 0.24355775117874146\n",
      "epoch 73: loss 0.343472421169281\n",
      "epoch 74: loss 0.29428553581237793\n",
      "epoch 75: loss 0.34688326716423035\n",
      "epoch 76: loss 0.3017547130584717\n",
      "epoch 77: loss 0.4125675857067108\n",
      "epoch 78: loss 0.3028820753097534\n",
      "epoch 79: loss 0.34429293870925903\n",
      "epoch 80: loss 0.37197554111480713\n",
      "epoch 81: loss 0.26871511340141296\n",
      "epoch 82: loss 0.32568973302841187\n",
      "epoch 83: loss 0.3873257637023926\n",
      "epoch 84: loss 0.2461925745010376\n",
      "epoch 85: loss 0.2930827736854553\n",
      "epoch 86: loss 0.3377208709716797\n",
      "epoch 87: loss 0.33991509675979614\n",
      "epoch 88: loss 0.26943743228912354\n",
      "epoch 89: loss 0.342257022857666\n",
      "epoch 90: loss 0.301249623298645\n",
      "epoch 91: loss 0.38317257165908813\n",
      "epoch 92: loss 0.34193652868270874\n",
      "epoch 93: loss 0.30987897515296936\n",
      "epoch 94: loss 0.28566688299179077\n",
      "epoch 95: loss 0.19978269934654236\n",
      "epoch 96: loss 0.23651887476444244\n",
      "epoch 97: loss 0.30476629734039307\n",
      "epoch 98: loss 0.3160485029220581\n",
      "epoch 99: loss 0.33163970708847046\n",
      "epoch 100: loss 0.4600289762020111\n",
      "epoch 101: loss 0.3005438446998596\n",
      "epoch 102: loss 0.2538149058818817\n",
      "epoch 103: loss 0.30474793910980225\n",
      "epoch 104: loss 0.24931854009628296\n",
      "epoch 105: loss 0.3670150935649872\n",
      "epoch 106: loss 0.3120309114456177\n",
      "epoch 107: loss 0.2705988883972168\n",
      "epoch 108: loss 0.3435317575931549\n",
      "epoch 109: loss 0.3450055718421936\n",
      "epoch 110: loss 0.33822259306907654\n",
      "epoch 111: loss 0.3704370856285095\n",
      "epoch 112: loss 0.36252105236053467\n",
      "epoch 113: loss 0.2755405604839325\n",
      "epoch 114: loss 0.1570068895816803\n",
      "epoch 115: loss 0.39302003383636475\n",
      "epoch 116: loss 0.2848355174064636\n",
      "epoch 117: loss 0.24961858987808228\n",
      "epoch 118: loss 0.3142211437225342\n",
      "epoch 119: loss 0.3941141366958618\n",
      "epoch 120: loss 0.25814202427864075\n",
      "epoch 121: loss 0.35456252098083496\n",
      "epoch 122: loss 0.23486849665641785\n",
      "epoch 123: loss 0.38160189986228943\n",
      "epoch 124: loss 0.32113170623779297\n",
      "epoch 125: loss 0.3426662087440491\n",
      "epoch 126: loss 0.38018375635147095\n",
      "epoch 127: loss 0.4217907190322876\n",
      "epoch 128: loss 0.2904258370399475\n",
      "epoch 129: loss 0.34216034412384033\n",
      "epoch 130: loss 0.42571520805358887\n",
      "epoch 131: loss 0.3520120680332184\n",
      "epoch 132: loss 0.36584311723709106\n",
      "epoch 133: loss 0.38647034764289856\n",
      "epoch 134: loss 0.31738072633743286\n",
      "epoch 135: loss 0.2756905257701874\n",
      "epoch 136: loss 0.2544829547405243\n",
      "epoch 0: loss 0.3534359335899353\n",
      "epoch 1: loss 0.3340679109096527\n",
      "epoch 2: loss 0.3700847029685974\n",
      "epoch 3: loss 0.3740450143814087\n",
      "epoch 4: loss 0.3255956768989563\n",
      "epoch 5: loss 0.259793758392334\n",
      "epoch 6: loss 0.2784002125263214\n",
      "epoch 7: loss 0.22237339615821838\n",
      "epoch 8: loss 0.4018377661705017\n",
      "epoch 9: loss 0.4204044044017792\n",
      "epoch 10: loss 0.3443012237548828\n",
      "epoch 11: loss 0.2792685329914093\n",
      "epoch 12: loss 0.3109404146671295\n",
      "epoch 13: loss 0.4319639503955841\n",
      "epoch 14: loss 0.3618854284286499\n",
      "epoch 15: loss 0.2793887257575989\n",
      "epoch 16: loss 0.3603822886943817\n",
      "epoch 17: loss 0.33247750997543335\n",
      "epoch 18: loss 0.37005892395973206\n",
      "epoch 19: loss 0.2552869915962219\n",
      "epoch 20: loss 0.28982096910476685\n",
      "epoch 21: loss 0.32100361585617065\n",
      "epoch 22: loss 0.31332242488861084\n",
      "epoch 23: loss 0.3665105700492859\n",
      "epoch 24: loss 0.33485648036003113\n",
      "epoch 25: loss 0.3979535400867462\n",
      "epoch 26: loss 0.34571418166160583\n",
      "epoch 27: loss 0.3315349817276001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28: loss 0.4029466509819031\n",
      "epoch 29: loss 0.3149615526199341\n",
      "epoch 30: loss 0.2933858036994934\n",
      "epoch 31: loss 0.33078351616859436\n",
      "epoch 32: loss 0.3409712612628937\n",
      "epoch 33: loss 0.3365594148635864\n",
      "epoch 34: loss 0.3817010223865509\n",
      "epoch 35: loss 0.25845277309417725\n",
      "epoch 36: loss 0.32574462890625\n",
      "epoch 37: loss 0.3195406198501587\n",
      "epoch 38: loss 0.21766939759254456\n",
      "epoch 39: loss 0.3052900433540344\n",
      "epoch 40: loss 0.29915565252304077\n",
      "epoch 41: loss 0.2805187404155731\n",
      "epoch 42: loss 0.250845730304718\n",
      "epoch 43: loss 0.33302509784698486\n",
      "epoch 44: loss 0.32685333490371704\n",
      "epoch 45: loss 0.2781293988227844\n",
      "epoch 46: loss 0.2861255407333374\n",
      "epoch 47: loss 0.2815350890159607\n",
      "epoch 48: loss 0.24140620231628418\n",
      "epoch 49: loss 0.32258355617523193\n",
      "epoch 50: loss 0.2829509377479553\n",
      "epoch 51: loss 0.4006611704826355\n",
      "epoch 52: loss 0.2936861515045166\n",
      "epoch 53: loss 0.35659629106521606\n",
      "epoch 54: loss 0.3478250503540039\n",
      "epoch 55: loss 0.3021070957183838\n",
      "epoch 56: loss 0.34423360228538513\n",
      "epoch 57: loss 0.24042165279388428\n",
      "epoch 58: loss 0.31921690702438354\n",
      "epoch 59: loss 0.2654150724411011\n",
      "epoch 60: loss 0.32025521993637085\n",
      "epoch 61: loss 0.2150004357099533\n",
      "epoch 62: loss 0.23720085620880127\n",
      "epoch 63: loss 0.3300306797027588\n",
      "epoch 64: loss 0.2754954695701599\n",
      "epoch 65: loss 0.38725966215133667\n",
      "epoch 66: loss 0.3247489929199219\n",
      "epoch 67: loss 0.3765299320220947\n",
      "epoch 68: loss 0.35577282309532166\n",
      "epoch 69: loss 0.4279766082763672\n",
      "epoch 70: loss 0.3797280192375183\n",
      "epoch 71: loss 0.30791348218917847\n",
      "epoch 72: loss 0.253896564245224\n",
      "epoch 73: loss 0.31246766448020935\n",
      "epoch 74: loss 0.2654363512992859\n",
      "epoch 75: loss 0.33500057458877563\n",
      "epoch 76: loss 0.30937397480010986\n",
      "epoch 77: loss 0.38937586545944214\n",
      "epoch 78: loss 0.32164010405540466\n",
      "epoch 79: loss 0.3192017078399658\n",
      "epoch 80: loss 0.35795140266418457\n",
      "epoch 81: loss 0.26153692603111267\n",
      "epoch 82: loss 0.3181873559951782\n",
      "epoch 83: loss 0.3744388818740845\n",
      "epoch 84: loss 0.26488015055656433\n",
      "epoch 85: loss 0.2984815239906311\n",
      "epoch 86: loss 0.33475637435913086\n",
      "epoch 87: loss 0.35000693798065186\n",
      "epoch 88: loss 0.2695489525794983\n",
      "epoch 89: loss 0.3285238742828369\n",
      "epoch 90: loss 0.29437029361724854\n",
      "epoch 91: loss 0.3647305965423584\n",
      "epoch 92: loss 0.32563984394073486\n",
      "epoch 93: loss 0.31046921014785767\n",
      "epoch 94: loss 0.29070594906806946\n",
      "epoch 95: loss 0.2129688858985901\n",
      "epoch 96: loss 0.21881458163261414\n",
      "epoch 97: loss 0.2918601632118225\n",
      "epoch 98: loss 0.3145132064819336\n",
      "epoch 99: loss 0.33911579847335815\n",
      "epoch 100: loss 0.46422114968299866\n",
      "epoch 101: loss 0.3037841320037842\n",
      "epoch 102: loss 0.2580299973487854\n",
      "epoch 103: loss 0.31259679794311523\n",
      "epoch 104: loss 0.25003254413604736\n",
      "epoch 105: loss 0.363172322511673\n",
      "epoch 106: loss 0.31200137734413147\n",
      "epoch 107: loss 0.2702922523021698\n",
      "epoch 108: loss 0.3473072648048401\n",
      "epoch 109: loss 0.3469454050064087\n",
      "epoch 110: loss 0.3366018235683441\n",
      "epoch 111: loss 0.3685268461704254\n",
      "epoch 112: loss 0.35723644495010376\n",
      "epoch 113: loss 0.26869726181030273\n",
      "epoch 114: loss 0.1536203771829605\n",
      "epoch 115: loss 0.3877187967300415\n",
      "epoch 116: loss 0.27722999453544617\n",
      "epoch 117: loss 0.2556420564651489\n",
      "epoch 118: loss 0.3163021504878998\n",
      "epoch 119: loss 0.39532342553138733\n",
      "epoch 120: loss 0.25832366943359375\n",
      "epoch 121: loss 0.3675413727760315\n",
      "epoch 122: loss 0.23137357831001282\n",
      "epoch 123: loss 0.38604456186294556\n",
      "epoch 124: loss 0.3287305235862732\n",
      "epoch 125: loss 0.33970212936401367\n",
      "epoch 126: loss 0.3760734796524048\n",
      "epoch 127: loss 0.40617549419403076\n",
      "epoch 128: loss 0.2724289298057556\n",
      "epoch 129: loss 0.34960395097732544\n",
      "epoch 130: loss 0.36337172985076904\n",
      "epoch 131: loss 0.31828784942626953\n",
      "epoch 132: loss 0.3084522485733032\n",
      "epoch 133: loss 0.34116142988204956\n",
      "epoch 134: loss 0.31253933906555176\n",
      "epoch 135: loss 0.2625897228717804\n",
      "epoch 136: loss 0.24275615811347961\n",
      "epoch 0: loss 0.3367561995983124\n",
      "epoch 1: loss 0.3270382285118103\n",
      "epoch 2: loss 0.3837393820285797\n",
      "epoch 3: loss 0.34718620777130127\n",
      "epoch 4: loss 0.30427712202072144\n",
      "epoch 5: loss 0.2589368224143982\n",
      "epoch 6: loss 0.2691152095794678\n",
      "epoch 7: loss 0.21472185850143433\n",
      "epoch 8: loss 0.3983421325683594\n",
      "epoch 9: loss 0.3932031989097595\n",
      "epoch 10: loss 0.33123505115509033\n",
      "epoch 11: loss 0.276521772146225\n",
      "epoch 12: loss 0.2985003888607025\n",
      "epoch 13: loss 0.42044776678085327\n",
      "epoch 14: loss 0.34156444668769836\n",
      "epoch 15: loss 0.27447083592414856\n",
      "epoch 16: loss 0.3404983878135681\n",
      "epoch 17: loss 0.32832905650138855\n",
      "epoch 18: loss 0.3670920431613922\n",
      "epoch 19: loss 0.26399171352386475\n",
      "epoch 20: loss 0.29249516129493713\n",
      "epoch 21: loss 0.32812026143074036\n",
      "epoch 22: loss 0.30298784375190735\n",
      "epoch 23: loss 0.36016109585762024\n",
      "epoch 24: loss 0.320110023021698\n",
      "epoch 25: loss 0.3891192376613617\n",
      "epoch 26: loss 0.3237660229206085\n",
      "epoch 27: loss 0.33787935972213745\n",
      "epoch 28: loss 0.39330217242240906\n",
      "epoch 29: loss 0.3128806948661804\n",
      "epoch 30: loss 0.2863895893096924\n",
      "epoch 31: loss 0.33158302307128906\n",
      "epoch 32: loss 0.3347035348415375\n",
      "epoch 33: loss 0.3395138382911682\n",
      "epoch 34: loss 0.379752516746521\n",
      "epoch 35: loss 0.25790050625801086\n",
      "epoch 36: loss 0.32614684104919434\n",
      "epoch 37: loss 0.329393208026886\n",
      "epoch 38: loss 0.22576484084129333\n",
      "epoch 39: loss 0.313640296459198\n",
      "epoch 40: loss 0.30480778217315674\n",
      "epoch 41: loss 0.279511034488678\n",
      "epoch 42: loss 0.24731409549713135\n",
      "epoch 43: loss 0.3387063145637512\n",
      "epoch 44: loss 0.3245587646961212\n",
      "epoch 45: loss 0.27959996461868286\n",
      "epoch 46: loss 0.2853204607963562\n",
      "epoch 47: loss 0.283612996339798\n",
      "epoch 48: loss 0.23608016967773438\n",
      "epoch 49: loss 0.31897979974746704\n",
      "epoch 50: loss 0.2805776000022888\n",
      "epoch 51: loss 0.39494770765304565\n",
      "epoch 52: loss 0.2846519947052002\n",
      "epoch 53: loss 0.35438138246536255\n",
      "epoch 54: loss 0.3460257053375244\n",
      "epoch 55: loss 0.2935735881328583\n",
      "epoch 56: loss 0.3480788469314575\n",
      "epoch 57: loss 0.24016837775707245\n",
      "epoch 58: loss 0.31757402420043945\n",
      "epoch 59: loss 0.26085853576660156\n",
      "epoch 60: loss 0.31637877225875854\n",
      "epoch 61: loss 0.21955600380897522\n",
      "epoch 62: loss 0.2394716739654541\n",
      "epoch 63: loss 0.33102941513061523\n",
      "epoch 64: loss 0.2755677402019501\n",
      "epoch 65: loss 0.3849326968193054\n",
      "epoch 66: loss 0.3248946964740753\n",
      "epoch 67: loss 0.3848940134048462\n",
      "epoch 68: loss 0.3547297716140747\n",
      "epoch 69: loss 0.4091131091117859\n",
      "epoch 70: loss 0.37473562359809875\n",
      "epoch 71: loss 0.31315159797668457\n",
      "epoch 72: loss 0.24822141230106354\n",
      "epoch 73: loss 0.31397536396980286\n",
      "epoch 74: loss 0.2546749711036682\n",
      "epoch 75: loss 0.34839582443237305\n",
      "epoch 76: loss 0.3122434616088867\n",
      "epoch 77: loss 0.3993059992790222\n",
      "epoch 78: loss 0.30420196056365967\n",
      "epoch 79: loss 0.3257017433643341\n",
      "epoch 80: loss 0.37996381521224976\n",
      "epoch 81: loss 0.26822546124458313\n",
      "epoch 82: loss 0.31606289744377136\n",
      "epoch 83: loss 0.3835791349411011\n",
      "epoch 84: loss 0.2679097652435303\n",
      "epoch 85: loss 0.2909846305847168\n",
      "epoch 86: loss 0.33605846762657166\n",
      "epoch 87: loss 0.3305979371070862\n",
      "epoch 88: loss 0.2845497727394104\n",
      "epoch 89: loss 0.3698495328426361\n",
      "epoch 90: loss 0.31871724128723145\n",
      "epoch 91: loss 0.4304998815059662\n",
      "epoch 92: loss 0.3941646218299866\n",
      "epoch 93: loss 0.3082427978515625\n",
      "epoch 94: loss 0.31262198090553284\n",
      "epoch 95: loss 0.18092873692512512\n",
      "epoch 96: loss 0.23895560204982758\n",
      "epoch 97: loss 0.31175631284713745\n",
      "epoch 98: loss 0.34942832589149475\n",
      "epoch 99: loss 0.3320325016975403\n",
      "epoch 100: loss 0.46923983097076416\n",
      "epoch 101: loss 0.29559868574142456\n",
      "epoch 102: loss 0.24593964219093323\n",
      "epoch 103: loss 0.30217722058296204\n",
      "epoch 104: loss 0.25590401887893677\n",
      "epoch 105: loss 0.3594440817832947\n",
      "epoch 106: loss 0.30679360032081604\n",
      "epoch 107: loss 0.2678232789039612\n",
      "epoch 108: loss 0.33497554063796997\n",
      "epoch 109: loss 0.34464824199676514\n",
      "epoch 110: loss 0.33587050437927246\n",
      "epoch 111: loss 0.38495439291000366\n",
      "epoch 112: loss 0.36947935819625854\n",
      "epoch 113: loss 0.2665989398956299\n",
      "epoch 114: loss 0.14751970767974854\n",
      "epoch 115: loss 0.3967161178588867\n",
      "epoch 116: loss 0.2797643542289734\n",
      "epoch 117: loss 0.2467922568321228\n",
      "epoch 118: loss 0.31805670261383057\n",
      "epoch 119: loss 0.39481019973754883\n",
      "epoch 120: loss 0.26364952325820923\n",
      "epoch 121: loss 0.3586508631706238\n",
      "epoch 122: loss 0.2361636608839035\n",
      "epoch 123: loss 0.379541277885437\n",
      "epoch 124: loss 0.3041052520275116\n",
      "epoch 125: loss 0.3386840522289276\n",
      "epoch 126: loss 0.3616785407066345\n",
      "epoch 127: loss 0.3856422007083893\n",
      "epoch 128: loss 0.250166118144989\n",
      "epoch 129: loss 0.33374330401420593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 130: loss 0.36005353927612305\n",
      "epoch 131: loss 0.316286563873291\n",
      "epoch 132: loss 0.30837100744247437\n",
      "epoch 133: loss 0.33926916122436523\n",
      "epoch 134: loss 0.30764591693878174\n",
      "epoch 135: loss 0.24225112795829773\n",
      "epoch 136: loss 0.2260817289352417\n",
      "epoch 0: loss 0.3268429934978485\n",
      "epoch 1: loss 0.340197890996933\n",
      "epoch 2: loss 0.36356011033058167\n",
      "epoch 3: loss 0.35323119163513184\n",
      "epoch 4: loss 0.29881420731544495\n",
      "epoch 5: loss 0.24335384368896484\n",
      "epoch 6: loss 0.27171021699905396\n",
      "epoch 7: loss 0.211988627910614\n",
      "epoch 8: loss 0.41381022334098816\n",
      "epoch 9: loss 0.40501099824905396\n",
      "epoch 10: loss 0.30982688069343567\n",
      "epoch 11: loss 0.25851932168006897\n",
      "epoch 12: loss 0.3134726285934448\n",
      "epoch 13: loss 0.4193427264690399\n",
      "epoch 14: loss 0.3382640480995178\n",
      "epoch 15: loss 0.261152982711792\n",
      "epoch 16: loss 0.3470063805580139\n",
      "epoch 17: loss 0.33057647943496704\n",
      "epoch 18: loss 0.3702359199523926\n",
      "epoch 19: loss 0.2536100149154663\n",
      "epoch 20: loss 0.2862998843193054\n",
      "epoch 21: loss 0.33203327655792236\n",
      "epoch 22: loss 0.2972472906112671\n",
      "epoch 23: loss 0.35101819038391113\n",
      "epoch 24: loss 0.3203492760658264\n",
      "epoch 25: loss 0.3792612552642822\n",
      "epoch 26: loss 0.3207740783691406\n",
      "epoch 27: loss 0.33388376235961914\n",
      "epoch 28: loss 0.39981985092163086\n",
      "epoch 29: loss 0.3066202998161316\n",
      "epoch 30: loss 0.28545504808425903\n",
      "epoch 31: loss 0.3333779573440552\n",
      "epoch 32: loss 0.33583465218544006\n",
      "epoch 33: loss 0.3405371308326721\n",
      "epoch 34: loss 0.3777616620063782\n",
      "epoch 35: loss 0.25790935754776\n",
      "epoch 36: loss 0.3289336562156677\n",
      "epoch 37: loss 0.30653703212738037\n",
      "epoch 38: loss 0.21372830867767334\n",
      "epoch 39: loss 0.2857237458229065\n",
      "epoch 40: loss 0.28486448526382446\n",
      "epoch 41: loss 0.2765042185783386\n",
      "epoch 42: loss 0.24508875608444214\n",
      "epoch 43: loss 0.31694191694259644\n",
      "epoch 44: loss 0.3248176574707031\n",
      "epoch 45: loss 0.28099769353866577\n",
      "epoch 46: loss 0.28031182289123535\n",
      "epoch 47: loss 0.28200089931488037\n",
      "epoch 48: loss 0.23871473968029022\n",
      "epoch 49: loss 0.33423542976379395\n",
      "epoch 50: loss 0.27977025508880615\n",
      "epoch 51: loss 0.39243263006210327\n",
      "epoch 52: loss 0.27492251992225647\n",
      "epoch 53: loss 0.3532178997993469\n",
      "epoch 54: loss 0.34393948316574097\n",
      "epoch 55: loss 0.2902393341064453\n",
      "epoch 56: loss 0.34867575764656067\n",
      "epoch 57: loss 0.23805798590183258\n",
      "epoch 58: loss 0.31842872500419617\n",
      "epoch 59: loss 0.25807708501815796\n",
      "epoch 60: loss 0.31484490633010864\n",
      "epoch 61: loss 0.21567818522453308\n",
      "epoch 62: loss 0.23706546425819397\n",
      "epoch 63: loss 0.33182913064956665\n",
      "epoch 64: loss 0.2783336639404297\n",
      "epoch 65: loss 0.37951162457466125\n",
      "epoch 66: loss 0.32924044132232666\n",
      "epoch 67: loss 0.38135942816734314\n",
      "epoch 68: loss 0.3526355028152466\n",
      "epoch 69: loss 0.4184097647666931\n",
      "epoch 70: loss 0.3721701204776764\n",
      "epoch 71: loss 0.3103887438774109\n",
      "epoch 72: loss 0.2508189082145691\n",
      "epoch 73: loss 0.31222397089004517\n",
      "epoch 74: loss 0.252619206905365\n",
      "epoch 75: loss 0.34296590089797974\n",
      "epoch 76: loss 0.30950501561164856\n",
      "epoch 77: loss 0.39804524183273315\n",
      "epoch 78: loss 0.31262651085853577\n",
      "epoch 79: loss 0.3101647198200226\n",
      "epoch 80: loss 0.3696531057357788\n",
      "epoch 81: loss 0.26173919439315796\n",
      "epoch 82: loss 0.3125552535057068\n",
      "epoch 83: loss 0.3728198707103729\n",
      "epoch 84: loss 0.26781219244003296\n",
      "epoch 85: loss 0.29547175765037537\n",
      "epoch 86: loss 0.33408617973327637\n",
      "epoch 87: loss 0.33614858984947205\n",
      "epoch 88: loss 0.2791856527328491\n",
      "epoch 89: loss 0.3560965657234192\n",
      "epoch 90: loss 0.3042644262313843\n",
      "epoch 91: loss 0.40282902121543884\n",
      "epoch 92: loss 0.37499991059303284\n",
      "epoch 93: loss 0.30765247344970703\n",
      "epoch 94: loss 0.30100059509277344\n",
      "epoch 95: loss 0.1844213902950287\n",
      "epoch 96: loss 0.2322900891304016\n",
      "epoch 97: loss 0.30324068665504456\n",
      "epoch 98: loss 0.3302394151687622\n",
      "epoch 99: loss 0.326468288898468\n",
      "epoch 100: loss 0.46353018283843994\n",
      "epoch 101: loss 0.29181063175201416\n",
      "epoch 102: loss 0.24312889575958252\n",
      "epoch 103: loss 0.29772186279296875\n",
      "epoch 104: loss 0.24798758327960968\n",
      "epoch 105: loss 0.35995811223983765\n",
      "epoch 106: loss 0.30233249068260193\n",
      "epoch 107: loss 0.2667783200740814\n",
      "epoch 108: loss 0.3327115774154663\n",
      "epoch 109: loss 0.34361201524734497\n",
      "epoch 110: loss 0.3344210386276245\n",
      "epoch 111: loss 0.3678293824195862\n",
      "epoch 112: loss 0.3599697947502136\n",
      "epoch 113: loss 0.2737683057785034\n",
      "epoch 114: loss 0.1447955071926117\n",
      "epoch 115: loss 0.39763158559799194\n",
      "epoch 116: loss 0.28179773688316345\n",
      "epoch 117: loss 0.24584534764289856\n",
      "epoch 118: loss 0.31562748551368713\n",
      "epoch 119: loss 0.3965795636177063\n",
      "epoch 120: loss 0.26203757524490356\n",
      "epoch 121: loss 0.3528760075569153\n",
      "epoch 122: loss 0.23195016384124756\n",
      "epoch 123: loss 0.37783461809158325\n",
      "epoch 124: loss 0.29946285486221313\n",
      "epoch 125: loss 0.3417002558708191\n",
      "epoch 126: loss 0.3715749979019165\n",
      "epoch 127: loss 0.39358699321746826\n",
      "epoch 128: loss 0.2602928876876831\n",
      "epoch 129: loss 0.34090518951416016\n",
      "epoch 130: loss 0.3559618294239044\n",
      "epoch 131: loss 0.31916627287864685\n",
      "epoch 132: loss 0.31661611795425415\n",
      "epoch 133: loss 0.33405357599258423\n",
      "epoch 134: loss 0.31112149357795715\n",
      "epoch 135: loss 0.24911724030971527\n",
      "epoch 136: loss 0.23256421089172363\n",
      "epoch 0: loss 0.3463059961795807\n",
      "epoch 1: loss 0.3189656734466553\n",
      "epoch 2: loss 0.3937329053878784\n",
      "epoch 3: loss 0.34141993522644043\n",
      "epoch 4: loss 0.34416061639785767\n",
      "epoch 5: loss 0.2498282790184021\n",
      "epoch 6: loss 0.2739618122577667\n",
      "epoch 7: loss 0.22861647605895996\n",
      "epoch 8: loss 0.4183792173862457\n",
      "epoch 9: loss 0.4290494918823242\n",
      "epoch 10: loss 0.3568945527076721\n",
      "epoch 11: loss 0.28309914469718933\n",
      "epoch 12: loss 0.3093258738517761\n",
      "epoch 13: loss 0.4084053039550781\n",
      "epoch 14: loss 0.38468408584594727\n",
      "epoch 15: loss 0.2693297564983368\n",
      "epoch 16: loss 0.34685415029525757\n",
      "epoch 17: loss 0.33393594622612\n",
      "epoch 18: loss 0.3866038918495178\n",
      "epoch 19: loss 0.2771412134170532\n",
      "epoch 20: loss 0.31696757674217224\n",
      "epoch 21: loss 0.333150714635849\n",
      "epoch 22: loss 0.324840784072876\n",
      "epoch 23: loss 0.37659913301467896\n",
      "epoch 24: loss 0.33715200424194336\n",
      "epoch 25: loss 0.41601282358169556\n",
      "epoch 26: loss 0.3280026316642761\n",
      "epoch 27: loss 0.33847054839134216\n",
      "epoch 28: loss 0.3930601179599762\n",
      "epoch 29: loss 0.31604433059692383\n",
      "epoch 30: loss 0.2844216823577881\n",
      "epoch 31: loss 0.33056849241256714\n",
      "epoch 32: loss 0.33527645468711853\n",
      "epoch 33: loss 0.3490709364414215\n",
      "epoch 34: loss 0.391255646944046\n",
      "epoch 35: loss 0.2541660666465759\n",
      "epoch 36: loss 0.32879331707954407\n",
      "epoch 37: loss 0.3206270933151245\n",
      "epoch 38: loss 0.2219138890504837\n",
      "epoch 39: loss 0.3107954263687134\n",
      "epoch 40: loss 0.3031988739967346\n",
      "epoch 41: loss 0.2797715365886688\n",
      "epoch 42: loss 0.24392245709896088\n",
      "epoch 43: loss 0.32982856035232544\n",
      "epoch 44: loss 0.32872146368026733\n",
      "epoch 45: loss 0.277018666267395\n",
      "epoch 46: loss 0.2804538905620575\n",
      "epoch 47: loss 0.28268322348594666\n",
      "epoch 48: loss 0.2400369644165039\n",
      "epoch 49: loss 0.32102200388908386\n",
      "epoch 50: loss 0.28037789463996887\n",
      "epoch 51: loss 0.4024392366409302\n",
      "epoch 52: loss 0.2991867661476135\n",
      "epoch 53: loss 0.3566921353340149\n",
      "epoch 54: loss 0.3437737822532654\n",
      "epoch 55: loss 0.3017235994338989\n",
      "epoch 56: loss 0.34204721450805664\n",
      "epoch 57: loss 0.23489904403686523\n",
      "epoch 58: loss 0.317807137966156\n",
      "epoch 59: loss 0.2627667486667633\n",
      "epoch 60: loss 0.3173670470714569\n",
      "epoch 61: loss 0.2137475609779358\n",
      "epoch 62: loss 0.2371969223022461\n",
      "epoch 63: loss 0.33002278208732605\n",
      "epoch 64: loss 0.27372223138809204\n",
      "epoch 65: loss 0.387782484292984\n",
      "epoch 66: loss 0.3226203918457031\n",
      "epoch 67: loss 0.3726298213005066\n",
      "epoch 68: loss 0.3555336892604828\n",
      "epoch 69: loss 0.42331641912460327\n",
      "epoch 70: loss 0.36870163679122925\n",
      "epoch 71: loss 0.315478652715683\n",
      "epoch 72: loss 0.2597298324108124\n",
      "epoch 73: loss 0.3081286549568176\n",
      "epoch 74: loss 0.2553960084915161\n",
      "epoch 75: loss 0.3448481559753418\n",
      "epoch 76: loss 0.31064409017562866\n",
      "epoch 77: loss 0.3942362666130066\n",
      "epoch 78: loss 0.3032349646091461\n",
      "epoch 79: loss 0.3240976333618164\n",
      "epoch 80: loss 0.3778155744075775\n",
      "epoch 81: loss 0.2703344225883484\n",
      "epoch 82: loss 0.31814607977867126\n",
      "epoch 83: loss 0.37683552503585815\n",
      "epoch 84: loss 0.2915024161338806\n",
      "epoch 85: loss 0.29214709997177124\n",
      "epoch 86: loss 0.3369871973991394\n",
      "epoch 87: loss 0.32728949189186096\n",
      "epoch 88: loss 0.29321378469467163\n",
      "epoch 89: loss 0.3801845908164978\n",
      "epoch 90: loss 0.3264249265193939\n",
      "epoch 91: loss 0.44930708408355713\n",
      "epoch 92: loss 0.40343552827835083\n",
      "epoch 93: loss 0.3052009046077728\n",
      "epoch 94: loss 0.31727057695388794\n",
      "epoch 95: loss 0.1840742826461792\n",
      "epoch 96: loss 0.23363903164863586\n",
      "epoch 97: loss 0.3146686255931854\n",
      "epoch 98: loss 0.3629114031791687\n",
      "epoch 99: loss 0.34391841292381287\n",
      "epoch 100: loss 0.4886588454246521\n",
      "epoch 101: loss 0.3022894561290741\n",
      "epoch 102: loss 0.24708117544651031\n",
      "epoch 103: loss 0.289249449968338\n",
      "epoch 104: loss 0.24277442693710327\n",
      "epoch 105: loss 0.36086758971214294\n",
      "epoch 106: loss 0.3057563900947571\n",
      "epoch 107: loss 0.2651931047439575\n",
      "epoch 108: loss 0.33577316999435425\n",
      "epoch 109: loss 0.3467429280281067\n",
      "epoch 110: loss 0.33369767665863037\n",
      "epoch 111: loss 0.392974853515625\n",
      "epoch 112: loss 0.37244266271591187\n",
      "epoch 113: loss 0.2713882327079773\n",
      "epoch 114: loss 0.1573229283094406\n",
      "epoch 115: loss 0.39715301990509033\n",
      "epoch 116: loss 0.2846100628376007\n",
      "epoch 117: loss 0.25028103590011597\n",
      "epoch 118: loss 0.31651172041893005\n",
      "epoch 119: loss 0.3945575952529907\n",
      "epoch 120: loss 0.26238951086997986\n",
      "epoch 121: loss 0.36199748516082764\n",
      "epoch 122: loss 0.23596534132957458\n",
      "epoch 123: loss 0.3816160559654236\n",
      "epoch 124: loss 0.30225956439971924\n",
      "epoch 125: loss 0.3392501771450043\n",
      "epoch 126: loss 0.3594789505004883\n",
      "epoch 127: loss 0.38296401500701904\n",
      "epoch 128: loss 0.2491655945777893\n",
      "epoch 129: loss 0.33243292570114136\n",
      "epoch 130: loss 0.36456698179244995\n",
      "epoch 131: loss 0.31519731879234314\n",
      "epoch 132: loss 0.30983951687812805\n",
      "epoch 133: loss 0.3425024747848511\n",
      "epoch 134: loss 0.3060498833656311\n",
      "epoch 135: loss 0.24292974174022675\n",
      "epoch 136: loss 0.22097136080265045\n",
      "epoch 0: loss 0.3261925280094147\n",
      "epoch 1: loss 0.3376477062702179\n",
      "epoch 2: loss 0.36273032426834106\n",
      "epoch 3: loss 0.34974342584609985\n",
      "epoch 4: loss 0.2970265746116638\n",
      "epoch 5: loss 0.23971408605575562\n",
      "epoch 6: loss 0.26831892132759094\n",
      "epoch 7: loss 0.2148485779762268\n",
      "epoch 8: loss 0.4130679666996002\n",
      "epoch 9: loss 0.40941178798675537\n",
      "epoch 10: loss 0.3148152828216553\n",
      "epoch 11: loss 0.257678747177124\n",
      "epoch 12: loss 0.31210756301879883\n",
      "epoch 13: loss 0.41146230697631836\n",
      "epoch 14: loss 0.34370338916778564\n",
      "epoch 15: loss 0.2610512375831604\n",
      "epoch 16: loss 0.34028422832489014\n",
      "epoch 17: loss 0.32983240485191345\n",
      "epoch 18: loss 0.38670802116394043\n",
      "epoch 19: loss 0.2545947730541229\n",
      "epoch 20: loss 0.2889794111251831\n",
      "epoch 21: loss 0.32613176107406616\n",
      "epoch 22: loss 0.2970585525035858\n",
      "epoch 23: loss 0.3512715697288513\n",
      "epoch 24: loss 0.3256152868270874\n",
      "epoch 25: loss 0.3767623007297516\n",
      "epoch 26: loss 0.31653326749801636\n",
      "epoch 27: loss 0.3310362696647644\n",
      "epoch 28: loss 0.3902902901172638\n",
      "epoch 29: loss 0.30594995617866516\n",
      "epoch 30: loss 0.2810894846916199\n",
      "epoch 31: loss 0.3359982967376709\n",
      "epoch 32: loss 0.33069777488708496\n",
      "epoch 33: loss 0.34579935669898987\n",
      "epoch 34: loss 0.3792048692703247\n",
      "epoch 35: loss 0.2544829547405243\n",
      "epoch 36: loss 0.329810231924057\n",
      "epoch 37: loss 0.30825263261795044\n",
      "epoch 38: loss 0.2089647799730301\n",
      "epoch 39: loss 0.2861076891422272\n",
      "epoch 40: loss 0.2902945280075073\n",
      "epoch 41: loss 0.27753210067749023\n",
      "epoch 42: loss 0.2449440062046051\n",
      "epoch 43: loss 0.3153113126754761\n",
      "epoch 44: loss 0.32180261611938477\n",
      "epoch 45: loss 0.28001874685287476\n",
      "epoch 46: loss 0.2758840322494507\n",
      "epoch 47: loss 0.2800629734992981\n",
      "epoch 48: loss 0.2400018870830536\n",
      "epoch 49: loss 0.3357064723968506\n",
      "epoch 50: loss 0.27962201833724976\n",
      "epoch 51: loss 0.39573168754577637\n",
      "epoch 52: loss 0.2823026180267334\n",
      "epoch 53: loss 0.350590318441391\n",
      "epoch 54: loss 0.34170687198638916\n",
      "epoch 55: loss 0.28964561223983765\n",
      "epoch 56: loss 0.3520212769508362\n",
      "epoch 57: loss 0.24407193064689636\n",
      "epoch 58: loss 0.3161657452583313\n",
      "epoch 59: loss 0.2599186301231384\n",
      "epoch 60: loss 0.31840014457702637\n",
      "epoch 61: loss 0.2207324057817459\n",
      "epoch 62: loss 0.24260073900222778\n",
      "epoch 63: loss 0.33961889147758484\n",
      "epoch 64: loss 0.2773072123527527\n",
      "epoch 65: loss 0.3883981704711914\n",
      "epoch 66: loss 0.3250923454761505\n",
      "epoch 67: loss 0.3821774125099182\n",
      "epoch 68: loss 0.357205867767334\n",
      "epoch 69: loss 0.4110351800918579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 70: loss 0.3707268238067627\n",
      "epoch 71: loss 0.30392444133758545\n",
      "epoch 72: loss 0.24927294254302979\n",
      "epoch 73: loss 0.3151649832725525\n",
      "epoch 74: loss 0.25019219517707825\n",
      "epoch 75: loss 0.34039878845214844\n",
      "epoch 76: loss 0.3132723569869995\n",
      "epoch 77: loss 0.39066746830940247\n",
      "epoch 78: loss 0.3163890242576599\n",
      "epoch 79: loss 0.30371785163879395\n",
      "epoch 80: loss 0.36597979068756104\n",
      "epoch 81: loss 0.2627801299095154\n",
      "epoch 82: loss 0.3167411684989929\n",
      "epoch 83: loss 0.3637023866176605\n",
      "epoch 84: loss 0.2719128727912903\n",
      "epoch 85: loss 0.293847918510437\n",
      "epoch 86: loss 0.33088934421539307\n",
      "epoch 87: loss 0.34015634655952454\n",
      "epoch 88: loss 0.27121180295944214\n",
      "epoch 89: loss 0.3329894542694092\n",
      "epoch 90: loss 0.29468196630477905\n",
      "epoch 91: loss 0.38555169105529785\n",
      "epoch 92: loss 0.3531973659992218\n",
      "epoch 93: loss 0.3047012388706207\n",
      "epoch 94: loss 0.2935105264186859\n",
      "epoch 95: loss 0.19847510755062103\n",
      "epoch 96: loss 0.2403775006532669\n",
      "epoch 97: loss 0.29097992181777954\n",
      "epoch 98: loss 0.3159899115562439\n",
      "epoch 99: loss 0.331304669380188\n",
      "epoch 100: loss 0.46297499537467957\n",
      "epoch 101: loss 0.29950666427612305\n",
      "epoch 102: loss 0.2539626359939575\n",
      "epoch 103: loss 0.31639230251312256\n",
      "epoch 104: loss 0.25688689947128296\n",
      "epoch 105: loss 0.36661165952682495\n",
      "epoch 106: loss 0.3104758858680725\n",
      "epoch 107: loss 0.2648780643939972\n",
      "epoch 108: loss 0.3477662205696106\n",
      "epoch 109: loss 0.3423038721084595\n",
      "epoch 110: loss 0.3421562910079956\n",
      "epoch 111: loss 0.36177778244018555\n",
      "epoch 112: loss 0.3565942049026489\n",
      "epoch 113: loss 0.2726476788520813\n",
      "epoch 114: loss 0.1528986096382141\n",
      "epoch 115: loss 0.39143168926239014\n",
      "epoch 116: loss 0.2778623104095459\n",
      "epoch 117: loss 0.25025975704193115\n",
      "epoch 118: loss 0.3120608329772949\n",
      "epoch 119: loss 0.3902483582496643\n",
      "epoch 120: loss 0.2565692663192749\n",
      "epoch 121: loss 0.35046789050102234\n",
      "epoch 122: loss 0.23340249061584473\n",
      "epoch 123: loss 0.37636250257492065\n",
      "epoch 124: loss 0.29437875747680664\n",
      "epoch 125: loss 0.34026017785072327\n",
      "epoch 126: loss 0.3691611588001251\n",
      "epoch 127: loss 0.38590389490127563\n",
      "epoch 128: loss 0.250679075717926\n",
      "epoch 129: loss 0.33065879344940186\n",
      "epoch 130: loss 0.3587281107902527\n",
      "epoch 131: loss 0.32383304834365845\n",
      "epoch 132: loss 0.3097253739833832\n",
      "epoch 133: loss 0.33943799138069153\n",
      "epoch 134: loss 0.3099781274795532\n",
      "epoch 135: loss 0.24484360218048096\n",
      "epoch 136: loss 0.22734655439853668\n",
      "epoch 0: loss 0.3311741352081299\n",
      "epoch 1: loss 0.3284980058670044\n",
      "epoch 2: loss 0.361194372177124\n",
      "epoch 3: loss 0.3466814160346985\n",
      "epoch 4: loss 0.30079880356788635\n",
      "epoch 5: loss 0.2451166808605194\n",
      "epoch 6: loss 0.26943856477737427\n",
      "epoch 7: loss 0.21933335065841675\n",
      "epoch 8: loss 0.4086173176765442\n",
      "epoch 9: loss 0.40335965156555176\n",
      "epoch 10: loss 0.3362833857536316\n",
      "epoch 11: loss 0.27380746603012085\n",
      "epoch 12: loss 0.30901873111724854\n",
      "epoch 13: loss 0.4015549421310425\n",
      "epoch 14: loss 0.3985174000263214\n",
      "epoch 15: loss 0.2622382640838623\n",
      "epoch 16: loss 0.33990949392318726\n",
      "epoch 17: loss 0.3793887197971344\n",
      "epoch 18: loss 0.4307193160057068\n",
      "epoch 19: loss 0.3072882890701294\n",
      "epoch 20: loss 0.34186530113220215\n",
      "epoch 21: loss 0.34791165590286255\n",
      "epoch 22: loss 0.33063462376594543\n",
      "epoch 23: loss 0.40022939443588257\n",
      "epoch 24: loss 0.36891767382621765\n",
      "epoch 25: loss 0.4304429590702057\n",
      "epoch 26: loss 0.3331650197505951\n",
      "epoch 27: loss 0.3473415970802307\n",
      "epoch 28: loss 0.4027019143104553\n",
      "epoch 29: loss 0.31584101915359497\n",
      "epoch 30: loss 0.27629756927490234\n",
      "epoch 31: loss 0.333316445350647\n",
      "epoch 32: loss 0.3458232283592224\n",
      "epoch 33: loss 0.34811070561408997\n",
      "epoch 34: loss 0.4030187726020813\n",
      "epoch 35: loss 0.253557950258255\n",
      "epoch 36: loss 0.3328002691268921\n",
      "epoch 37: loss 0.31481897830963135\n",
      "epoch 38: loss 0.21354883909225464\n",
      "epoch 39: loss 0.30171871185302734\n",
      "epoch 40: loss 0.28688836097717285\n",
      "epoch 41: loss 0.26904064416885376\n",
      "epoch 42: loss 0.25997066497802734\n",
      "epoch 43: loss 0.3298330307006836\n",
      "epoch 44: loss 0.33215707540512085\n",
      "epoch 45: loss 0.27744191884994507\n",
      "epoch 46: loss 0.287080854177475\n",
      "epoch 47: loss 0.2829609811306\n",
      "epoch 48: loss 0.24818246066570282\n",
      "epoch 49: loss 0.31843245029449463\n",
      "epoch 50: loss 0.2759592533111572\n",
      "epoch 51: loss 0.39183998107910156\n",
      "epoch 52: loss 0.2814404368400574\n",
      "epoch 53: loss 0.3529535233974457\n",
      "epoch 54: loss 0.33934271335601807\n",
      "epoch 55: loss 0.2914750576019287\n",
      "epoch 56: loss 0.34921810030937195\n",
      "epoch 57: loss 0.24255169928073883\n",
      "epoch 58: loss 0.32220375537872314\n",
      "epoch 59: loss 0.2619342803955078\n",
      "epoch 60: loss 0.32078489661216736\n",
      "epoch 61: loss 0.21493956446647644\n",
      "epoch 62: loss 0.23724797368049622\n",
      "epoch 63: loss 0.3317687511444092\n",
      "epoch 64: loss 0.2754529118537903\n",
      "epoch 65: loss 0.3817795515060425\n",
      "epoch 66: loss 0.3275686502456665\n",
      "epoch 67: loss 0.3769958019256592\n",
      "epoch 68: loss 0.35447490215301514\n",
      "epoch 69: loss 0.4095544219017029\n",
      "epoch 70: loss 0.366148442029953\n",
      "epoch 71: loss 0.3105265200138092\n",
      "epoch 72: loss 0.24916042387485504\n",
      "epoch 73: loss 0.31365156173706055\n",
      "epoch 74: loss 0.24988716840744019\n",
      "epoch 75: loss 0.34576112031936646\n",
      "epoch 76: loss 0.311715692281723\n",
      "epoch 77: loss 0.4001008868217468\n",
      "epoch 78: loss 0.31689178943634033\n",
      "epoch 79: loss 0.3093542754650116\n",
      "epoch 80: loss 0.36982911825180054\n",
      "epoch 81: loss 0.2629464268684387\n",
      "epoch 82: loss 0.314544677734375\n",
      "epoch 83: loss 0.36690080165863037\n",
      "epoch 84: loss 0.28287917375564575\n",
      "epoch 85: loss 0.2938419580459595\n",
      "epoch 86: loss 0.33277207612991333\n",
      "epoch 87: loss 0.3398498594760895\n",
      "epoch 88: loss 0.27431777119636536\n",
      "epoch 89: loss 0.34560370445251465\n",
      "epoch 90: loss 0.29766929149627686\n",
      "epoch 91: loss 0.3938407599925995\n",
      "epoch 92: loss 0.36715036630630493\n",
      "epoch 93: loss 0.3065369129180908\n",
      "epoch 94: loss 0.2949684262275696\n",
      "epoch 95: loss 0.18156442046165466\n",
      "epoch 96: loss 0.22589267790317535\n",
      "epoch 97: loss 0.3028953969478607\n",
      "epoch 98: loss 0.3336262106895447\n",
      "epoch 99: loss 0.3251565098762512\n",
      "epoch 100: loss 0.4604564905166626\n",
      "epoch 101: loss 0.29732680320739746\n",
      "epoch 102: loss 0.24857863783836365\n",
      "epoch 103: loss 0.30040818452835083\n",
      "epoch 104: loss 0.2501906752586365\n",
      "epoch 105: loss 0.35862621665000916\n",
      "epoch 106: loss 0.3080582916736603\n",
      "epoch 107: loss 0.266753613948822\n",
      "epoch 108: loss 0.3382892608642578\n",
      "epoch 109: loss 0.3438849449157715\n",
      "epoch 110: loss 0.3366406559944153\n",
      "epoch 111: loss 0.3688391447067261\n",
      "epoch 112: loss 0.3662033677101135\n",
      "epoch 113: loss 0.2764378786087036\n",
      "epoch 114: loss 0.1501072645187378\n",
      "epoch 115: loss 0.39664122462272644\n",
      "epoch 116: loss 0.28299659490585327\n",
      "epoch 117: loss 0.24682408571243286\n",
      "epoch 118: loss 0.3148057460784912\n",
      "epoch 119: loss 0.398841917514801\n",
      "epoch 120: loss 0.261411190032959\n",
      "epoch 121: loss 0.35424143075942993\n",
      "epoch 122: loss 0.2402884066104889\n",
      "epoch 123: loss 0.37703171372413635\n",
      "epoch 124: loss 0.29029136896133423\n",
      "epoch 125: loss 0.3404594659805298\n",
      "epoch 126: loss 0.36767321825027466\n",
      "epoch 127: loss 0.3825538456439972\n",
      "epoch 128: loss 0.2533373236656189\n",
      "epoch 129: loss 0.33509159088134766\n",
      "epoch 130: loss 0.35374560952186584\n",
      "epoch 131: loss 0.3263823986053467\n",
      "epoch 132: loss 0.30954575538635254\n",
      "epoch 133: loss 0.3407951593399048\n",
      "epoch 134: loss 0.30996087193489075\n",
      "epoch 135: loss 0.24604706466197968\n",
      "epoch 136: loss 0.23019742965698242\n",
      "epoch 0: loss 0.3361659049987793\n",
      "epoch 1: loss 0.32194802165031433\n",
      "epoch 2: loss 0.3687098026275635\n",
      "epoch 3: loss 0.350593239068985\n",
      "epoch 4: loss 0.321766197681427\n",
      "epoch 5: loss 0.24923905730247498\n",
      "epoch 6: loss 0.2766285836696625\n",
      "epoch 7: loss 0.22880300879478455\n",
      "epoch 8: loss 0.4244697690010071\n",
      "epoch 9: loss 0.4362151026725769\n",
      "epoch 10: loss 0.3537788391113281\n",
      "epoch 11: loss 0.28277671337127686\n",
      "epoch 12: loss 0.3099783957004547\n",
      "epoch 13: loss 0.40014755725860596\n",
      "epoch 14: loss 0.4168635606765747\n",
      "epoch 15: loss 0.2680244445800781\n",
      "epoch 16: loss 0.34326231479644775\n",
      "epoch 17: loss 0.3959103226661682\n",
      "epoch 18: loss 0.44364285469055176\n",
      "epoch 19: loss 0.32287389039993286\n",
      "epoch 20: loss 0.3519119322299957\n",
      "epoch 21: loss 0.3619096279144287\n",
      "epoch 22: loss 0.3342914283275604\n",
      "epoch 23: loss 0.3972809314727783\n",
      "epoch 24: loss 0.363930881023407\n",
      "epoch 25: loss 0.43743184208869934\n",
      "epoch 26: loss 0.3502545654773712\n",
      "epoch 27: loss 0.35428115725517273\n",
      "epoch 28: loss 0.4030606150627136\n",
      "epoch 29: loss 0.3139483332633972\n",
      "epoch 30: loss 0.28587663173675537\n",
      "epoch 31: loss 0.33301234245300293\n",
      "epoch 32: loss 0.3581315875053406\n",
      "epoch 33: loss 0.3373323678970337\n",
      "epoch 34: loss 0.39738211035728455\n",
      "epoch 35: loss 0.2611018419265747\n",
      "epoch 36: loss 0.3276180922985077\n",
      "epoch 37: loss 0.3250786066055298\n",
      "epoch 38: loss 0.22217510640621185\n",
      "epoch 39: loss 0.30912312865257263\n",
      "epoch 40: loss 0.2905750572681427\n",
      "epoch 41: loss 0.27139508724212646\n",
      "epoch 42: loss 0.28275391459465027\n",
      "epoch 43: loss 0.3315194845199585\n",
      "epoch 44: loss 0.35683655738830566\n",
      "epoch 45: loss 0.2827024459838867\n",
      "epoch 46: loss 0.2923329770565033\n",
      "epoch 47: loss 0.2985946536064148\n",
      "epoch 48: loss 0.2516402006149292\n",
      "epoch 49: loss 0.3330336809158325\n",
      "epoch 50: loss 0.2757675349712372\n",
      "epoch 51: loss 0.3937714695930481\n",
      "epoch 52: loss 0.2937697172164917\n",
      "epoch 53: loss 0.3609136641025543\n",
      "epoch 54: loss 0.3447970747947693\n",
      "epoch 55: loss 0.2985440492630005\n",
      "epoch 56: loss 0.34368014335632324\n",
      "epoch 57: loss 0.2579033374786377\n",
      "epoch 58: loss 0.3348335325717926\n",
      "epoch 59: loss 0.26292550563812256\n",
      "epoch 60: loss 0.334293007850647\n",
      "epoch 61: loss 0.21703743934631348\n",
      "epoch 62: loss 0.23543846607208252\n",
      "epoch 63: loss 0.3514901399612427\n",
      "epoch 64: loss 0.301651269197464\n",
      "epoch 65: loss 0.398998886346817\n",
      "epoch 66: loss 0.3318418264389038\n",
      "epoch 67: loss 0.37201112508773804\n",
      "epoch 68: loss 0.3639931082725525\n",
      "epoch 69: loss 0.40276262164115906\n",
      "epoch 70: loss 0.3753424286842346\n",
      "epoch 71: loss 0.31897640228271484\n",
      "epoch 72: loss 0.25590357184410095\n",
      "epoch 73: loss 0.33187800645828247\n",
      "epoch 74: loss 0.26459988951683044\n",
      "epoch 75: loss 0.3406159281730652\n",
      "epoch 76: loss 0.30734068155288696\n",
      "epoch 77: loss 0.3937840461730957\n",
      "epoch 78: loss 0.32681238651275635\n",
      "epoch 79: loss 0.3297146260738373\n",
      "epoch 80: loss 0.35660481452941895\n",
      "epoch 81: loss 0.2705467641353607\n",
      "epoch 82: loss 0.3246760964393616\n",
      "epoch 83: loss 0.3723742365837097\n",
      "epoch 84: loss 0.2424151748418808\n",
      "epoch 85: loss 0.3024543821811676\n",
      "epoch 86: loss 0.33685383200645447\n",
      "epoch 87: loss 0.3486834168434143\n",
      "epoch 88: loss 0.26996392011642456\n",
      "epoch 89: loss 0.32105496525764465\n",
      "epoch 90: loss 0.27921783924102783\n",
      "epoch 91: loss 0.32325881719589233\n",
      "epoch 92: loss 0.25860530138015747\n",
      "epoch 93: loss 0.3169897794723511\n",
      "epoch 94: loss 0.3097304701805115\n",
      "epoch 95: loss 0.2026827335357666\n",
      "epoch 96: loss 0.20626692473888397\n",
      "epoch 97: loss 0.3091161251068115\n",
      "epoch 98: loss 0.341083288192749\n",
      "epoch 99: loss 0.37316471338272095\n",
      "epoch 100: loss 0.5254532098770142\n",
      "epoch 101: loss 0.3032233715057373\n",
      "epoch 102: loss 0.2518339157104492\n",
      "epoch 103: loss 0.3083289861679077\n",
      "epoch 104: loss 0.2821034789085388\n",
      "epoch 105: loss 0.36628592014312744\n",
      "epoch 106: loss 0.3142716884613037\n",
      "epoch 107: loss 0.2649824917316437\n",
      "epoch 108: loss 0.34152141213417053\n",
      "epoch 109: loss 0.3457830548286438\n",
      "epoch 110: loss 0.3549457788467407\n",
      "epoch 111: loss 0.3621523380279541\n",
      "epoch 112: loss 0.37744176387786865\n",
      "epoch 113: loss 0.28899866342544556\n",
      "epoch 114: loss 0.17683354020118713\n",
      "epoch 115: loss 0.3928408920764923\n",
      "epoch 116: loss 0.2901323735713959\n",
      "epoch 117: loss 0.2595004439353943\n",
      "epoch 118: loss 0.31677162647247314\n",
      "epoch 119: loss 0.3819091320037842\n",
      "epoch 120: loss 0.2637985050678253\n",
      "epoch 121: loss 0.3738242983818054\n",
      "epoch 122: loss 0.24407707154750824\n",
      "epoch 123: loss 0.3768993318080902\n",
      "epoch 124: loss 0.32978808879852295\n",
      "epoch 125: loss 0.33097678422927856\n",
      "epoch 126: loss 0.3514978885650635\n",
      "epoch 127: loss 0.391292929649353\n",
      "epoch 128: loss 0.2484528124332428\n",
      "epoch 129: loss 0.33458250761032104\n",
      "epoch 130: loss 0.3571017384529114\n",
      "epoch 131: loss 0.31362247467041016\n",
      "epoch 132: loss 0.3097950518131256\n",
      "epoch 133: loss 0.33741074800491333\n",
      "epoch 134: loss 0.31173667311668396\n",
      "epoch 135: loss 0.24537408351898193\n",
      "epoch 136: loss 0.22990566492080688\n",
      "epoch 0: loss 0.33967912197113037\n",
      "epoch 1: loss 0.3317398428916931\n",
      "epoch 2: loss 0.38168570399284363\n",
      "epoch 3: loss 0.3638257086277008\n",
      "epoch 4: loss 0.32479381561279297\n",
      "epoch 5: loss 0.24922578036785126\n",
      "epoch 6: loss 0.27949023246765137\n",
      "epoch 7: loss 0.22756527364253998\n",
      "epoch 8: loss 0.4224395155906677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: loss 0.428519606590271\n",
      "epoch 10: loss 0.3453025221824646\n",
      "epoch 11: loss 0.28022071719169617\n",
      "epoch 12: loss 0.3172633647918701\n",
      "epoch 13: loss 0.41202884912490845\n",
      "epoch 14: loss 0.3672657310962677\n",
      "epoch 15: loss 0.2699914574623108\n",
      "epoch 16: loss 0.3531573414802551\n",
      "epoch 17: loss 0.36889344453811646\n",
      "epoch 18: loss 0.41130369901657104\n",
      "epoch 19: loss 0.274050772190094\n",
      "epoch 20: loss 0.30626529455184937\n",
      "epoch 21: loss 0.331628680229187\n",
      "epoch 22: loss 0.3009895980358124\n",
      "epoch 23: loss 0.3681856393814087\n",
      "epoch 24: loss 0.3336835503578186\n",
      "epoch 25: loss 0.3812679946422577\n",
      "epoch 26: loss 0.3063371181488037\n",
      "epoch 27: loss 0.34951046109199524\n",
      "epoch 28: loss 0.40456095337867737\n",
      "epoch 29: loss 0.317969411611557\n",
      "epoch 30: loss 0.2909688353538513\n",
      "epoch 31: loss 0.33157894015312195\n",
      "epoch 32: loss 0.3380376994609833\n",
      "epoch 33: loss 0.3428182005882263\n",
      "epoch 34: loss 0.38537973165512085\n",
      "epoch 35: loss 0.26013249158859253\n",
      "epoch 36: loss 0.33075112104415894\n",
      "epoch 37: loss 0.3086417019367218\n",
      "epoch 38: loss 0.21352531015872955\n",
      "epoch 39: loss 0.29284465312957764\n",
      "epoch 40: loss 0.28967440128326416\n",
      "epoch 41: loss 0.27621352672576904\n",
      "epoch 42: loss 0.25006353855133057\n",
      "epoch 43: loss 0.3204565644264221\n",
      "epoch 44: loss 0.3338656425476074\n",
      "epoch 45: loss 0.2812819480895996\n",
      "epoch 46: loss 0.2750760316848755\n",
      "epoch 47: loss 0.2821965515613556\n",
      "epoch 48: loss 0.2442176342010498\n",
      "epoch 49: loss 0.3418789207935333\n",
      "epoch 50: loss 0.28185051679611206\n",
      "epoch 51: loss 0.39718103408813477\n",
      "epoch 52: loss 0.2731670141220093\n",
      "epoch 53: loss 0.35532838106155396\n",
      "epoch 54: loss 0.3424380421638489\n",
      "epoch 55: loss 0.29316842555999756\n",
      "epoch 56: loss 0.3481149971485138\n",
      "epoch 57: loss 0.24209411442279816\n",
      "epoch 58: loss 0.32257360219955444\n",
      "epoch 59: loss 0.2622375786304474\n",
      "epoch 60: loss 0.3254973888397217\n",
      "epoch 61: loss 0.21900907158851624\n",
      "epoch 62: loss 0.24010680615901947\n",
      "epoch 63: loss 0.3391081690788269\n",
      "epoch 64: loss 0.2824908196926117\n",
      "epoch 65: loss 0.38565483689308167\n",
      "epoch 66: loss 0.32549387216567993\n",
      "epoch 67: loss 0.37882113456726074\n",
      "epoch 68: loss 0.35708776116371155\n",
      "epoch 69: loss 0.4091416299343109\n",
      "epoch 70: loss 0.3733425736427307\n",
      "epoch 71: loss 0.3089987337589264\n",
      "epoch 72: loss 0.24740095436573029\n",
      "epoch 73: loss 0.3153020739555359\n",
      "epoch 74: loss 0.25641363859176636\n",
      "epoch 75: loss 0.3434305787086487\n",
      "epoch 76: loss 0.3066514730453491\n",
      "epoch 77: loss 0.40141960978507996\n",
      "epoch 78: loss 0.3152720332145691\n",
      "epoch 79: loss 0.3143256604671478\n",
      "epoch 80: loss 0.3669338822364807\n",
      "epoch 81: loss 0.26273295283317566\n",
      "epoch 82: loss 0.3157576024532318\n",
      "epoch 83: loss 0.3678259253501892\n",
      "epoch 84: loss 0.27697110176086426\n",
      "epoch 85: loss 0.29595696926116943\n",
      "epoch 86: loss 0.33202946186065674\n",
      "epoch 87: loss 0.3438483774662018\n",
      "epoch 88: loss 0.27520984411239624\n",
      "epoch 89: loss 0.3519364893436432\n",
      "epoch 90: loss 0.3004138469696045\n",
      "epoch 91: loss 0.39310121536254883\n",
      "epoch 92: loss 0.3474242091178894\n",
      "epoch 93: loss 0.30928316712379456\n",
      "epoch 94: loss 0.2926624119281769\n",
      "epoch 95: loss 0.2539794147014618\n",
      "epoch 96: loss 0.23668581247329712\n",
      "epoch 97: loss 0.29347240924835205\n",
      "epoch 98: loss 0.3173462152481079\n",
      "epoch 99: loss 0.3510221242904663\n",
      "epoch 100: loss 0.49598655104637146\n",
      "epoch 101: loss 0.3103439509868622\n",
      "epoch 102: loss 0.2666725814342499\n",
      "epoch 103: loss 0.3203544020652771\n",
      "epoch 104: loss 0.25372034311294556\n",
      "epoch 105: loss 0.36564958095550537\n",
      "epoch 106: loss 0.3155260682106018\n",
      "epoch 107: loss 0.30610060691833496\n",
      "epoch 108: loss 0.348056435585022\n",
      "epoch 109: loss 0.34553036093711853\n",
      "epoch 110: loss 0.3502168357372284\n",
      "epoch 111: loss 0.3737235367298126\n",
      "epoch 112: loss 0.38423624634742737\n",
      "epoch 113: loss 0.30980125069618225\n",
      "epoch 114: loss 0.18280625343322754\n",
      "epoch 115: loss 0.4195544421672821\n",
      "epoch 116: loss 0.2993641495704651\n",
      "epoch 117: loss 0.2753283381462097\n",
      "epoch 118: loss 0.32451850175857544\n",
      "epoch 119: loss 0.37129050493240356\n",
      "epoch 120: loss 0.2684457302093506\n",
      "epoch 121: loss 0.3611288070678711\n",
      "epoch 122: loss 0.23933783173561096\n",
      "epoch 123: loss 0.38197872042655945\n",
      "epoch 124: loss 0.29228729009628296\n",
      "epoch 125: loss 0.3314376473426819\n",
      "epoch 126: loss 0.33261334896087646\n",
      "epoch 127: loss 0.36788633465766907\n",
      "epoch 128: loss 0.2615610659122467\n",
      "epoch 129: loss 0.3403083086013794\n",
      "epoch 130: loss 0.36441701650619507\n",
      "epoch 131: loss 0.32684534788131714\n",
      "epoch 132: loss 0.31567105650901794\n",
      "epoch 133: loss 0.351205050945282\n",
      "epoch 134: loss 0.3142009973526001\n",
      "epoch 135: loss 0.2397492229938507\n",
      "epoch 136: loss 0.22162558138370514\n",
      "epoch 0: loss 0.321974515914917\n",
      "epoch 1: loss 0.3432074785232544\n",
      "epoch 2: loss 0.3610348701477051\n",
      "epoch 3: loss 0.3505016267299652\n",
      "epoch 4: loss 0.3005264103412628\n",
      "epoch 5: loss 0.23990483582019806\n",
      "epoch 6: loss 0.2671045660972595\n",
      "epoch 7: loss 0.2117803990840912\n",
      "epoch 8: loss 0.40525057911872864\n",
      "epoch 9: loss 0.4007551670074463\n",
      "epoch 10: loss 0.32224780321121216\n",
      "epoch 11: loss 0.26249098777770996\n",
      "epoch 12: loss 0.2987715005874634\n",
      "epoch 13: loss 0.4013221561908722\n",
      "epoch 14: loss 0.36691442131996155\n",
      "epoch 15: loss 0.26058149337768555\n",
      "epoch 16: loss 0.34741970896720886\n",
      "epoch 17: loss 0.34290191531181335\n",
      "epoch 18: loss 0.40401461720466614\n",
      "epoch 19: loss 0.2696415185928345\n",
      "epoch 20: loss 0.30883532762527466\n",
      "epoch 21: loss 0.33235442638397217\n",
      "epoch 22: loss 0.3105964660644531\n",
      "epoch 23: loss 0.35904768109321594\n",
      "epoch 24: loss 0.39005619287490845\n",
      "epoch 25: loss 0.3860107660293579\n",
      "epoch 26: loss 0.30127984285354614\n",
      "epoch 27: loss 0.363375186920166\n",
      "epoch 28: loss 0.42921632528305054\n",
      "epoch 29: loss 0.3298841416835785\n",
      "epoch 30: loss 0.31390896439552307\n",
      "epoch 31: loss 0.3238163888454437\n",
      "epoch 32: loss 0.3356495499610901\n",
      "epoch 33: loss 0.35158950090408325\n",
      "epoch 34: loss 0.388430118560791\n",
      "epoch 35: loss 0.2639181613922119\n",
      "epoch 36: loss 0.34071946144104004\n",
      "epoch 37: loss 0.30766797065734863\n",
      "epoch 38: loss 0.22411856055259705\n",
      "epoch 39: loss 0.28875166177749634\n",
      "epoch 40: loss 0.2949106991291046\n",
      "epoch 41: loss 0.280600905418396\n",
      "epoch 42: loss 0.24438448250293732\n",
      "epoch 43: loss 0.31365686655044556\n",
      "epoch 44: loss 0.3303040564060211\n",
      "epoch 45: loss 0.2801441550254822\n",
      "epoch 46: loss 0.28018078207969666\n",
      "epoch 47: loss 0.2806961238384247\n",
      "epoch 48: loss 0.25509607791900635\n",
      "epoch 49: loss 0.3417850434780121\n",
      "epoch 50: loss 0.2780301570892334\n",
      "epoch 51: loss 0.3940437436103821\n",
      "epoch 52: loss 0.2827778458595276\n",
      "epoch 53: loss 0.3564508557319641\n",
      "epoch 54: loss 0.3414173722267151\n",
      "epoch 55: loss 0.30029910802841187\n",
      "epoch 56: loss 0.3399050831794739\n",
      "epoch 57: loss 0.23718494176864624\n",
      "epoch 58: loss 0.3201881945133209\n",
      "epoch 59: loss 0.26208406686782837\n",
      "epoch 60: loss 0.3199108839035034\n",
      "epoch 61: loss 0.21203380823135376\n",
      "epoch 62: loss 0.23608608543872833\n",
      "epoch 63: loss 0.32970547676086426\n",
      "epoch 64: loss 0.27550098299980164\n",
      "epoch 65: loss 0.3889000117778778\n",
      "epoch 66: loss 0.32486677169799805\n",
      "epoch 67: loss 0.37524300813674927\n",
      "epoch 68: loss 0.35270968079566956\n",
      "epoch 69: loss 0.4075571298599243\n",
      "epoch 70: loss 0.37592756748199463\n",
      "epoch 71: loss 0.32088083028793335\n",
      "epoch 72: loss 0.2416187971830368\n",
      "epoch 73: loss 0.3199504017829895\n",
      "epoch 74: loss 0.26313909888267517\n",
      "epoch 75: loss 0.3504676818847656\n",
      "epoch 76: loss 0.30958911776542664\n",
      "epoch 77: loss 0.4060339331626892\n",
      "epoch 78: loss 0.30321621894836426\n",
      "epoch 79: loss 0.3273009955883026\n",
      "epoch 80: loss 0.37444907426834106\n",
      "epoch 81: loss 0.2662614583969116\n",
      "epoch 82: loss 0.314887136220932\n",
      "epoch 83: loss 0.3912012577056885\n",
      "epoch 84: loss 0.2552529275417328\n",
      "epoch 85: loss 0.29216188192367554\n",
      "epoch 86: loss 0.3386836349964142\n",
      "epoch 87: loss 0.335833877325058\n",
      "epoch 88: loss 0.2810825705528259\n",
      "epoch 89: loss 0.36140918731689453\n",
      "epoch 90: loss 0.30815353989601135\n",
      "epoch 91: loss 0.3980080187320709\n",
      "epoch 92: loss 0.3521465063095093\n",
      "epoch 93: loss 0.3105924725532532\n",
      "epoch 94: loss 0.2914048433303833\n",
      "epoch 95: loss 0.23547595739364624\n",
      "epoch 96: loss 0.22796714305877686\n",
      "epoch 97: loss 0.29016509652137756\n",
      "epoch 98: loss 0.3120001554489136\n",
      "epoch 99: loss 0.3494483232498169\n",
      "epoch 100: loss 0.48335957527160645\n",
      "epoch 101: loss 0.308834433555603\n",
      "epoch 102: loss 0.26547765731811523\n",
      "epoch 103: loss 0.3213363587856293\n",
      "epoch 104: loss 0.2507333755493164\n",
      "epoch 105: loss 0.35855188965797424\n",
      "epoch 106: loss 0.30915600061416626\n",
      "epoch 107: loss 0.297239750623703\n",
      "epoch 108: loss 0.3457421660423279\n",
      "epoch 109: loss 0.34411048889160156\n",
      "epoch 110: loss 0.3485996425151825\n",
      "epoch 111: loss 0.36141741275787354\n",
      "epoch 112: loss 0.3645876348018646\n",
      "epoch 113: loss 0.28773537278175354\n",
      "epoch 114: loss 0.16174165904521942\n",
      "epoch 115: loss 0.38748985528945923\n",
      "epoch 116: loss 0.28085699677467346\n",
      "epoch 117: loss 0.2710934281349182\n",
      "epoch 118: loss 0.31111323833465576\n",
      "epoch 119: loss 0.3863837718963623\n",
      "epoch 120: loss 0.27113083004951477\n",
      "epoch 121: loss 0.3909849226474762\n",
      "epoch 122: loss 0.26075154542922974\n",
      "epoch 123: loss 0.4302375912666321\n",
      "epoch 124: loss 0.32174545526504517\n",
      "epoch 125: loss 0.3194770812988281\n",
      "epoch 126: loss 0.36240991950035095\n",
      "epoch 127: loss 0.4024009108543396\n",
      "epoch 128: loss 0.25967729091644287\n",
      "epoch 129: loss 0.33618077635765076\n",
      "epoch 130: loss 0.370486319065094\n",
      "epoch 131: loss 0.3587326407432556\n",
      "epoch 132: loss 0.3143933415412903\n",
      "epoch 133: loss 0.3343702554702759\n",
      "epoch 134: loss 0.3167504668235779\n",
      "epoch 135: loss 0.26385700702667236\n",
      "epoch 136: loss 0.24954307079315186\n",
      "epoch 0: loss 0.35214731097221375\n",
      "epoch 1: loss 0.3287487030029297\n",
      "epoch 2: loss 0.3992794156074524\n",
      "epoch 3: loss 0.3634662330150604\n",
      "epoch 4: loss 0.32289624214172363\n",
      "epoch 5: loss 0.25382333993911743\n",
      "epoch 6: loss 0.2765420079231262\n",
      "epoch 7: loss 0.2146356999874115\n",
      "epoch 8: loss 0.4036676287651062\n",
      "epoch 9: loss 0.3928064703941345\n",
      "epoch 10: loss 0.3320518732070923\n",
      "epoch 11: loss 0.2830714285373688\n",
      "epoch 12: loss 0.3113499581813812\n",
      "epoch 13: loss 0.42805981636047363\n",
      "epoch 14: loss 0.35328418016433716\n",
      "epoch 15: loss 0.27263525128364563\n",
      "epoch 16: loss 0.3605217933654785\n",
      "epoch 17: loss 0.32932546734809875\n",
      "epoch 18: loss 0.37241217494010925\n",
      "epoch 19: loss 0.2507813572883606\n",
      "epoch 20: loss 0.2869061529636383\n",
      "epoch 21: loss 0.32685166597366333\n",
      "epoch 22: loss 0.2974231243133545\n",
      "epoch 23: loss 0.3483142852783203\n",
      "epoch 24: loss 0.3239009976387024\n",
      "epoch 25: loss 0.37945282459259033\n",
      "epoch 26: loss 0.31212806701660156\n",
      "epoch 27: loss 0.3337883949279785\n",
      "epoch 28: loss 0.395008385181427\n",
      "epoch 29: loss 0.3037342429161072\n",
      "epoch 30: loss 0.28131937980651855\n",
      "epoch 31: loss 0.3346676230430603\n",
      "epoch 32: loss 0.3348850607872009\n",
      "epoch 33: loss 0.3395231366157532\n",
      "epoch 34: loss 0.37858572602272034\n",
      "epoch 35: loss 0.25769609212875366\n",
      "epoch 36: loss 0.3284715414047241\n",
      "epoch 37: loss 0.3143545389175415\n",
      "epoch 38: loss 0.21141809225082397\n",
      "epoch 39: loss 0.2875796854496002\n",
      "epoch 40: loss 0.28678643703460693\n",
      "epoch 41: loss 0.27523744106292725\n",
      "epoch 42: loss 0.25882816314697266\n",
      "epoch 43: loss 0.3189536929130554\n",
      "epoch 44: loss 0.3314399719238281\n",
      "epoch 45: loss 0.28918614983558655\n",
      "epoch 46: loss 0.2938942611217499\n",
      "epoch 47: loss 0.28700515627861023\n",
      "epoch 48: loss 0.23748712241649628\n",
      "epoch 49: loss 0.31980767846107483\n",
      "epoch 50: loss 0.2816203832626343\n",
      "epoch 51: loss 0.38945281505584717\n",
      "epoch 52: loss 0.26870197057724\n",
      "epoch 53: loss 0.3479253649711609\n",
      "epoch 54: loss 0.34678229689598083\n",
      "epoch 55: loss 0.2943311035633087\n",
      "epoch 56: loss 0.35067206621170044\n",
      "epoch 57: loss 0.24236398935317993\n",
      "epoch 58: loss 0.3187188506126404\n",
      "epoch 59: loss 0.2619113028049469\n",
      "epoch 60: loss 0.32164889574050903\n",
      "epoch 61: loss 0.22130657732486725\n",
      "epoch 62: loss 0.24267636239528656\n",
      "epoch 63: loss 0.33473479747772217\n",
      "epoch 64: loss 0.28112536668777466\n",
      "epoch 65: loss 0.38611292839050293\n",
      "epoch 66: loss 0.32531923055648804\n",
      "epoch 67: loss 0.37797391414642334\n",
      "epoch 68: loss 0.35438141226768494\n",
      "epoch 69: loss 0.40856969356536865\n",
      "epoch 70: loss 0.37018346786499023\n",
      "epoch 71: loss 0.31256815791130066\n",
      "epoch 72: loss 0.2487332671880722\n",
      "epoch 73: loss 0.3125861585140228\n",
      "epoch 74: loss 0.2545631527900696\n",
      "epoch 75: loss 0.34622448682785034\n",
      "epoch 76: loss 0.31047195196151733\n",
      "epoch 77: loss 0.40420931577682495\n",
      "epoch 78: loss 0.30721473693847656\n",
      "epoch 79: loss 0.3138188123703003\n",
      "epoch 80: loss 0.37395980954170227\n",
      "epoch 81: loss 0.26493149995803833\n",
      "epoch 82: loss 0.3145942687988281\n",
      "epoch 83: loss 0.37590327858924866\n",
      "epoch 84: loss 0.27244114875793457\n",
      "epoch 85: loss 0.2913462817668915\n",
      "epoch 86: loss 0.33846062421798706\n",
      "epoch 87: loss 0.3362620174884796\n",
      "epoch 88: loss 0.28528550267219543\n",
      "epoch 89: loss 0.37715986371040344\n",
      "epoch 90: loss 0.32348763942718506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 91: loss 0.4368284344673157\n",
      "epoch 92: loss 0.37822452187538147\n",
      "epoch 93: loss 0.3101873993873596\n",
      "epoch 94: loss 0.2851082384586334\n",
      "epoch 95: loss 0.2193281054496765\n",
      "epoch 96: loss 0.27396446466445923\n",
      "epoch 97: loss 0.29759952425956726\n",
      "epoch 98: loss 0.3105901777744293\n",
      "epoch 99: loss 0.3435211777687073\n",
      "epoch 100: loss 0.4714413285255432\n",
      "epoch 101: loss 0.3053668737411499\n",
      "epoch 102: loss 0.26418358087539673\n",
      "epoch 103: loss 0.3266269266605377\n",
      "epoch 104: loss 0.2565106749534607\n",
      "epoch 105: loss 0.3691854476928711\n",
      "epoch 106: loss 0.31533777713775635\n",
      "epoch 107: loss 0.27564916014671326\n",
      "epoch 108: loss 0.3579578399658203\n",
      "epoch 109: loss 0.3502829968929291\n",
      "epoch 110: loss 0.3430691063404083\n",
      "epoch 111: loss 0.359334260225296\n",
      "epoch 112: loss 0.3629423975944519\n",
      "epoch 113: loss 0.27239567041397095\n",
      "epoch 114: loss 0.16123037040233612\n",
      "epoch 115: loss 0.3946608304977417\n",
      "epoch 116: loss 0.2856481671333313\n",
      "epoch 117: loss 0.2504405677318573\n",
      "epoch 118: loss 0.3168705105781555\n",
      "epoch 119: loss 0.38843145966529846\n",
      "epoch 120: loss 0.2577054500579834\n",
      "epoch 121: loss 0.3571529984474182\n",
      "epoch 122: loss 0.2345931977033615\n",
      "epoch 123: loss 0.3736279606819153\n",
      "epoch 124: loss 0.31365519762039185\n",
      "epoch 125: loss 0.34117990732192993\n",
      "epoch 126: loss 0.3755381107330322\n",
      "epoch 127: loss 0.4270429015159607\n",
      "epoch 128: loss 0.2875065803527832\n",
      "epoch 129: loss 0.34514230489730835\n",
      "epoch 130: loss 0.4188651442527771\n",
      "epoch 131: loss 0.34404852986335754\n",
      "epoch 132: loss 0.34857019782066345\n",
      "epoch 133: loss 0.3732858896255493\n",
      "epoch 134: loss 0.3175097107887268\n",
      "epoch 135: loss 0.2603531777858734\n",
      "epoch 136: loss 0.23622599244117737\n",
      "epoch 0: loss 0.33561795949935913\n",
      "epoch 1: loss 0.34733855724334717\n",
      "epoch 2: loss 0.36450549960136414\n",
      "epoch 3: loss 0.35263919830322266\n",
      "epoch 4: loss 0.30360472202301025\n",
      "epoch 5: loss 0.2566562294960022\n",
      "epoch 6: loss 0.2760320007801056\n",
      "epoch 7: loss 0.21886107325553894\n",
      "epoch 8: loss 0.3985564708709717\n",
      "epoch 9: loss 0.3806798756122589\n",
      "epoch 10: loss 0.3189545273780823\n",
      "epoch 11: loss 0.2643095850944519\n",
      "epoch 12: loss 0.3028308153152466\n",
      "epoch 13: loss 0.4116068482398987\n",
      "epoch 14: loss 0.3532566726207733\n",
      "epoch 15: loss 0.27098047733306885\n",
      "epoch 16: loss 0.3449093997478485\n",
      "epoch 17: loss 0.3249591886997223\n",
      "epoch 18: loss 0.38406258821487427\n",
      "epoch 19: loss 0.2586261034011841\n",
      "epoch 20: loss 0.29692670702934265\n",
      "epoch 21: loss 0.3271089494228363\n",
      "epoch 22: loss 0.3146677315235138\n",
      "epoch 23: loss 0.3602081537246704\n",
      "epoch 24: loss 0.31712597608566284\n",
      "epoch 25: loss 0.4045369625091553\n",
      "epoch 26: loss 0.31410062313079834\n",
      "epoch 27: loss 0.34070515632629395\n",
      "epoch 28: loss 0.39535433053970337\n",
      "epoch 29: loss 0.3178889751434326\n",
      "epoch 30: loss 0.28649041056632996\n",
      "epoch 31: loss 0.3258301019668579\n",
      "epoch 32: loss 0.3351795971393585\n",
      "epoch 33: loss 0.3397131562232971\n",
      "epoch 34: loss 0.3786833882331848\n",
      "epoch 35: loss 0.2585986256599426\n",
      "epoch 36: loss 0.3244191110134125\n",
      "epoch 37: loss 0.32444465160369873\n",
      "epoch 38: loss 0.22242189943790436\n",
      "epoch 39: loss 0.3086918890476227\n",
      "epoch 40: loss 0.3008027970790863\n",
      "epoch 41: loss 0.27869749069213867\n",
      "epoch 42: loss 0.24408453702926636\n",
      "epoch 43: loss 0.33637869358062744\n",
      "epoch 44: loss 0.327401340007782\n",
      "epoch 45: loss 0.27560076117515564\n",
      "epoch 46: loss 0.2870715856552124\n",
      "epoch 47: loss 0.2830413579940796\n",
      "epoch 48: loss 0.23893602192401886\n",
      "epoch 49: loss 0.31580305099487305\n",
      "epoch 50: loss 0.2806730270385742\n",
      "epoch 51: loss 0.39530515670776367\n",
      "epoch 52: loss 0.28589919209480286\n",
      "epoch 53: loss 0.3552934527397156\n",
      "epoch 54: loss 0.3471490740776062\n",
      "epoch 55: loss 0.3002833425998688\n",
      "epoch 56: loss 0.34286046028137207\n",
      "epoch 57: loss 0.24082762002944946\n",
      "epoch 58: loss 0.31690213084220886\n",
      "epoch 59: loss 0.2631492614746094\n",
      "epoch 60: loss 0.32028210163116455\n",
      "epoch 61: loss 0.2141723781824112\n",
      "epoch 62: loss 0.23761427402496338\n",
      "epoch 63: loss 0.3286980986595154\n",
      "epoch 64: loss 0.2736378014087677\n",
      "epoch 65: loss 0.38461998105049133\n",
      "epoch 66: loss 0.3241339325904846\n",
      "epoch 67: loss 0.37898123264312744\n",
      "epoch 68: loss 0.35262829065322876\n",
      "epoch 69: loss 0.41408246755599976\n",
      "epoch 70: loss 0.37342870235443115\n",
      "epoch 71: loss 0.32083234190940857\n",
      "epoch 72: loss 0.24856331944465637\n",
      "epoch 73: loss 0.31229180097579956\n",
      "epoch 74: loss 0.25644397735595703\n",
      "epoch 75: loss 0.3494877815246582\n",
      "epoch 76: loss 0.3095543384552002\n",
      "epoch 77: loss 0.40218716859817505\n",
      "epoch 78: loss 0.30073821544647217\n",
      "epoch 79: loss 0.3361620604991913\n",
      "epoch 80: loss 0.3814684748649597\n",
      "epoch 81: loss 0.2667200267314911\n",
      "epoch 82: loss 0.31115448474884033\n",
      "epoch 83: loss 0.4218931198120117\n",
      "epoch 84: loss 0.24238552153110504\n",
      "epoch 85: loss 0.2956550121307373\n",
      "epoch 86: loss 0.3445657193660736\n",
      "epoch 87: loss 0.3335004448890686\n",
      "epoch 88: loss 0.2800008952617645\n",
      "epoch 89: loss 0.36335790157318115\n",
      "epoch 90: loss 0.3113592267036438\n",
      "epoch 91: loss 0.40511661767959595\n",
      "epoch 92: loss 0.3562670350074768\n",
      "epoch 93: loss 0.3129103183746338\n",
      "epoch 94: loss 0.29055649042129517\n",
      "epoch 95: loss 0.21472430229187012\n",
      "epoch 96: loss 0.24631528556346893\n",
      "epoch 97: loss 0.29411208629608154\n",
      "epoch 98: loss 0.3103754222393036\n",
      "epoch 99: loss 0.3445618748664856\n",
      "epoch 100: loss 0.46408891677856445\n",
      "epoch 101: loss 0.3082261383533478\n",
      "epoch 102: loss 0.2642081379890442\n",
      "epoch 103: loss 0.3261905908584595\n",
      "epoch 104: loss 0.25611430406570435\n",
      "epoch 105: loss 0.36300626397132874\n",
      "epoch 106: loss 0.3116803765296936\n",
      "epoch 107: loss 0.275245726108551\n",
      "epoch 108: loss 0.3484647870063782\n",
      "epoch 109: loss 0.3525487184524536\n",
      "epoch 110: loss 0.3367736339569092\n",
      "epoch 111: loss 0.3584129214286804\n",
      "epoch 112: loss 0.36164307594299316\n",
      "epoch 113: loss 0.2704322338104248\n",
      "epoch 114: loss 0.15540333092212677\n",
      "epoch 115: loss 0.39285749197006226\n",
      "epoch 116: loss 0.27785050868988037\n",
      "epoch 117: loss 0.2529098689556122\n",
      "epoch 118: loss 0.31529340147972107\n",
      "epoch 119: loss 0.39503705501556396\n",
      "epoch 120: loss 0.2575964033603668\n",
      "epoch 121: loss 0.36805373430252075\n",
      "epoch 122: loss 0.23022565245628357\n",
      "epoch 123: loss 0.3842353820800781\n",
      "epoch 124: loss 0.35312122106552124\n",
      "epoch 125: loss 0.34309089183807373\n",
      "epoch 126: loss 0.37785953283309937\n",
      "epoch 127: loss 0.431566447019577\n",
      "epoch 128: loss 0.30312949419021606\n",
      "epoch 129: loss 0.3421677350997925\n",
      "epoch 130: loss 0.44770872592926025\n",
      "epoch 131: loss 0.3643258810043335\n",
      "epoch 132: loss 0.387543648481369\n",
      "epoch 133: loss 0.39692094922065735\n",
      "epoch 134: loss 0.32301095128059387\n",
      "epoch 135: loss 0.2839416265487671\n",
      "epoch 136: loss 0.2633093297481537\n",
      "epoch 0: loss 0.35471051931381226\n",
      "epoch 1: loss 0.3436394929885864\n",
      "epoch 2: loss 0.37504345178604126\n",
      "epoch 3: loss 0.3596665859222412\n",
      "epoch 4: loss 0.3071468472480774\n",
      "epoch 5: loss 0.25913649797439575\n",
      "epoch 6: loss 0.27533605694770813\n",
      "epoch 7: loss 0.22283828258514404\n",
      "epoch 8: loss 0.39658036828041077\n",
      "epoch 9: loss 0.3982577323913574\n",
      "epoch 10: loss 0.3307933807373047\n",
      "epoch 11: loss 0.27597999572753906\n",
      "epoch 12: loss 0.31698930263519287\n",
      "epoch 13: loss 0.4241127371788025\n",
      "epoch 14: loss 0.35501253604888916\n",
      "epoch 15: loss 0.27242425084114075\n",
      "epoch 16: loss 0.3516751527786255\n",
      "epoch 17: loss 0.3254747688770294\n",
      "epoch 18: loss 0.3723522126674652\n",
      "epoch 19: loss 0.25507932901382446\n",
      "epoch 20: loss 0.2908201217651367\n",
      "epoch 21: loss 0.3226199746131897\n",
      "epoch 22: loss 0.3041077256202698\n",
      "epoch 23: loss 0.36232417821884155\n",
      "epoch 24: loss 0.33225810527801514\n",
      "epoch 25: loss 0.392831951379776\n",
      "epoch 26: loss 0.3224194049835205\n",
      "epoch 27: loss 0.33511173725128174\n",
      "epoch 28: loss 0.3976525068283081\n",
      "epoch 29: loss 0.31044721603393555\n",
      "epoch 30: loss 0.279433935880661\n",
      "epoch 31: loss 0.3349825143814087\n",
      "epoch 32: loss 0.3343692421913147\n",
      "epoch 33: loss 0.34544897079467773\n",
      "epoch 34: loss 0.38571345806121826\n",
      "epoch 35: loss 0.2548556327819824\n",
      "epoch 36: loss 0.32922062277793884\n",
      "epoch 37: loss 0.31570181250572205\n",
      "epoch 38: loss 0.2115432769060135\n",
      "epoch 39: loss 0.30490589141845703\n",
      "epoch 40: loss 0.2998350262641907\n",
      "epoch 41: loss 0.2785211205482483\n",
      "epoch 42: loss 0.25057047605514526\n",
      "epoch 43: loss 0.32865408062934875\n",
      "epoch 44: loss 0.3266562819480896\n",
      "epoch 45: loss 0.2795577645301819\n",
      "epoch 46: loss 0.2835678458213806\n",
      "epoch 47: loss 0.27877599000930786\n",
      "epoch 48: loss 0.24337859451770782\n",
      "epoch 49: loss 0.3188241422176361\n",
      "epoch 50: loss 0.27915140986442566\n",
      "epoch 51: loss 0.39345407485961914\n",
      "epoch 52: loss 0.2811066508293152\n",
      "epoch 53: loss 0.3514537215232849\n",
      "epoch 54: loss 0.3485538959503174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 55: loss 0.29561877250671387\n",
      "epoch 56: loss 0.35041552782058716\n",
      "epoch 57: loss 0.2471565306186676\n",
      "epoch 58: loss 0.31737810373306274\n",
      "epoch 59: loss 0.2621697783470154\n",
      "epoch 60: loss 0.323333203792572\n",
      "epoch 61: loss 0.21784771978855133\n",
      "epoch 62: loss 0.24047954380512238\n",
      "epoch 63: loss 0.33470818400382996\n",
      "epoch 64: loss 0.27568233013153076\n",
      "epoch 65: loss 0.38759341835975647\n",
      "epoch 66: loss 0.32525742053985596\n",
      "epoch 67: loss 0.3773554563522339\n",
      "epoch 68: loss 0.35323235392570496\n",
      "epoch 69: loss 0.40425482392311096\n",
      "epoch 70: loss 0.38368457555770874\n",
      "epoch 71: loss 0.3089712858200073\n",
      "epoch 72: loss 0.2442503720521927\n",
      "epoch 73: loss 0.32322680950164795\n",
      "epoch 74: loss 0.26534324884414673\n",
      "epoch 75: loss 0.3486699163913727\n",
      "epoch 76: loss 0.30844783782958984\n",
      "epoch 77: loss 0.4085434675216675\n",
      "epoch 78: loss 0.3033505082130432\n",
      "epoch 79: loss 0.3247942328453064\n",
      "epoch 80: loss 0.37458598613739014\n",
      "epoch 81: loss 0.2632434070110321\n",
      "epoch 82: loss 0.3124579191207886\n",
      "epoch 83: loss 0.3991609811782837\n",
      "epoch 84: loss 0.24108153581619263\n",
      "epoch 85: loss 0.29538044333457947\n",
      "epoch 86: loss 0.3409882187843323\n",
      "epoch 87: loss 0.3384087383747101\n",
      "epoch 88: loss 0.2730042636394501\n",
      "epoch 89: loss 0.3516726493835449\n",
      "epoch 90: loss 0.3038597106933594\n",
      "epoch 91: loss 0.38599541783332825\n",
      "epoch 92: loss 0.3452179431915283\n",
      "epoch 93: loss 0.3103042244911194\n",
      "epoch 94: loss 0.28594037890434265\n",
      "epoch 95: loss 0.20851469039916992\n",
      "epoch 96: loss 0.2507445812225342\n",
      "epoch 97: loss 0.29570603370666504\n",
      "epoch 98: loss 0.3099731504917145\n",
      "epoch 99: loss 0.33686763048171997\n",
      "epoch 100: loss 0.45557722449302673\n",
      "epoch 101: loss 0.30526116490364075\n",
      "epoch 102: loss 0.25929269194602966\n",
      "epoch 103: loss 0.32733896374702454\n",
      "epoch 104: loss 0.25668013095855713\n",
      "epoch 105: loss 0.36651092767715454\n",
      "epoch 106: loss 0.3141188621520996\n",
      "epoch 107: loss 0.2721502184867859\n",
      "epoch 108: loss 0.35480716824531555\n",
      "epoch 109: loss 0.3565608263015747\n",
      "epoch 110: loss 0.33806875348091125\n",
      "epoch 111: loss 0.3576240837574005\n",
      "epoch 112: loss 0.36044472455978394\n",
      "epoch 113: loss 0.26996350288391113\n",
      "epoch 114: loss 0.15259632468223572\n",
      "epoch 115: loss 0.39963823556900024\n",
      "epoch 116: loss 0.2865000367164612\n",
      "epoch 117: loss 0.24918881058692932\n",
      "epoch 118: loss 0.31852293014526367\n",
      "epoch 119: loss 0.39057493209838867\n",
      "epoch 120: loss 0.25477856397628784\n",
      "epoch 121: loss 0.3517422080039978\n",
      "epoch 122: loss 0.2376500368118286\n",
      "epoch 123: loss 0.380138635635376\n",
      "epoch 124: loss 0.3080836832523346\n",
      "epoch 125: loss 0.3448830842971802\n",
      "epoch 126: loss 0.37264466285705566\n",
      "epoch 127: loss 0.3970050811767578\n",
      "epoch 128: loss 0.25153034925460815\n",
      "epoch 129: loss 0.34375032782554626\n",
      "epoch 130: loss 0.3643746078014374\n",
      "epoch 131: loss 0.33557993173599243\n",
      "epoch 132: loss 0.31821203231811523\n",
      "epoch 133: loss 0.33265209197998047\n",
      "epoch 134: loss 0.30651164054870605\n",
      "epoch 135: loss 0.24523232877254486\n",
      "epoch 136: loss 0.22028642892837524\n",
      "epoch 0: loss 0.33237749338150024\n",
      "epoch 1: loss 0.33115291595458984\n",
      "epoch 2: loss 0.36662331223487854\n",
      "epoch 3: loss 0.35900652408599854\n",
      "epoch 4: loss 0.3291197419166565\n",
      "epoch 5: loss 0.23996374011039734\n",
      "epoch 6: loss 0.2744814157485962\n",
      "epoch 7: loss 0.2227826714515686\n",
      "epoch 8: loss 0.41404351592063904\n",
      "epoch 9: loss 0.4301189184188843\n",
      "epoch 10: loss 0.3565319776535034\n",
      "epoch 11: loss 0.285888135433197\n",
      "epoch 12: loss 0.3211389482021332\n",
      "epoch 13: loss 0.40927445888519287\n",
      "epoch 14: loss 0.3734194040298462\n",
      "epoch 15: loss 0.27719083428382874\n",
      "epoch 16: loss 0.3572547733783722\n",
      "epoch 17: loss 0.3299063444137573\n",
      "epoch 18: loss 0.38217246532440186\n",
      "epoch 19: loss 0.2696927487850189\n",
      "epoch 20: loss 0.3115023970603943\n",
      "epoch 21: loss 0.3279900550842285\n",
      "epoch 22: loss 0.3189091980457306\n",
      "epoch 23: loss 0.374764084815979\n",
      "epoch 24: loss 0.33193016052246094\n",
      "epoch 25: loss 0.422235369682312\n",
      "epoch 26: loss 0.32537829875946045\n",
      "epoch 27: loss 0.3369368314743042\n",
      "epoch 28: loss 0.39109331369400024\n",
      "epoch 29: loss 0.3177887201309204\n",
      "epoch 30: loss 0.2835334539413452\n",
      "epoch 31: loss 0.3288895785808563\n",
      "epoch 32: loss 0.3347809910774231\n",
      "epoch 33: loss 0.347109317779541\n",
      "epoch 34: loss 0.3907936215400696\n",
      "epoch 35: loss 0.2565840482711792\n",
      "epoch 36: loss 0.3312033414840698\n",
      "epoch 37: loss 0.3179273009300232\n",
      "epoch 38: loss 0.2181932032108307\n",
      "epoch 39: loss 0.3091939091682434\n",
      "epoch 40: loss 0.3009302020072937\n",
      "epoch 41: loss 0.2781471014022827\n",
      "epoch 42: loss 0.2447809875011444\n",
      "epoch 43: loss 0.33607691526412964\n",
      "epoch 44: loss 0.32788607478141785\n",
      "epoch 45: loss 0.27868983149528503\n",
      "epoch 46: loss 0.2806240916252136\n",
      "epoch 47: loss 0.2799021303653717\n",
      "epoch 48: loss 0.24266944825649261\n",
      "epoch 49: loss 0.3189465403556824\n",
      "epoch 50: loss 0.2768825888633728\n",
      "epoch 51: loss 0.3890368342399597\n",
      "epoch 52: loss 0.2769626975059509\n",
      "epoch 53: loss 0.3536207675933838\n",
      "epoch 54: loss 0.34789571166038513\n",
      "epoch 55: loss 0.29416540265083313\n",
      "epoch 56: loss 0.35266411304473877\n",
      "epoch 57: loss 0.24671295285224915\n",
      "epoch 58: loss 0.318606436252594\n",
      "epoch 59: loss 0.2595445513725281\n",
      "epoch 60: loss 0.32121437788009644\n",
      "epoch 61: loss 0.2186499685049057\n",
      "epoch 62: loss 0.241328164935112\n",
      "epoch 63: loss 0.3356308937072754\n",
      "epoch 64: loss 0.27386730909347534\n",
      "epoch 65: loss 0.38976579904556274\n",
      "epoch 66: loss 0.32680317759513855\n",
      "epoch 67: loss 0.37556135654449463\n",
      "epoch 68: loss 0.353579580783844\n",
      "epoch 69: loss 0.4064200520515442\n",
      "epoch 70: loss 0.3851185739040375\n",
      "epoch 71: loss 0.3113449811935425\n",
      "epoch 72: loss 0.24347761273384094\n",
      "epoch 73: loss 0.32109493017196655\n",
      "epoch 74: loss 0.2650531530380249\n",
      "epoch 75: loss 0.3472896218299866\n",
      "epoch 76: loss 0.3077777028083801\n",
      "epoch 77: loss 0.4033379554748535\n",
      "epoch 78: loss 0.2998809814453125\n",
      "epoch 79: loss 0.3267250061035156\n",
      "epoch 80: loss 0.377302348613739\n",
      "epoch 81: loss 0.26626884937286377\n",
      "epoch 82: loss 0.3151867091655731\n",
      "epoch 83: loss 0.3864191770553589\n",
      "epoch 84: loss 0.26976943016052246\n",
      "epoch 85: loss 0.28835463523864746\n",
      "epoch 86: loss 0.33991527557373047\n",
      "epoch 87: loss 0.33721160888671875\n",
      "epoch 88: loss 0.27681490778923035\n",
      "epoch 89: loss 0.364218145608902\n",
      "epoch 90: loss 0.3200209438800812\n",
      "epoch 91: loss 0.42017215490341187\n",
      "epoch 92: loss 0.36520150303840637\n",
      "epoch 93: loss 0.3088000416755676\n",
      "epoch 94: loss 0.28509241342544556\n",
      "epoch 95: loss 0.19962243735790253\n",
      "epoch 96: loss 0.24480485916137695\n",
      "epoch 97: loss 0.30914852023124695\n",
      "epoch 98: loss 0.3155243396759033\n",
      "epoch 99: loss 0.32710614800453186\n",
      "epoch 100: loss 0.45606839656829834\n",
      "epoch 101: loss 0.302787184715271\n",
      "epoch 102: loss 0.25193342566490173\n",
      "epoch 103: loss 0.3122178912162781\n",
      "epoch 104: loss 0.24729101359844208\n",
      "epoch 105: loss 0.36762937903404236\n",
      "epoch 106: loss 0.3100467920303345\n",
      "epoch 107: loss 0.26512864232063293\n",
      "epoch 108: loss 0.34865808486938477\n",
      "epoch 109: loss 0.3546021282672882\n",
      "epoch 110: loss 0.33669614791870117\n",
      "epoch 111: loss 0.3640044331550598\n",
      "epoch 112: loss 0.3620205223560333\n",
      "epoch 113: loss 0.2755495309829712\n",
      "epoch 114: loss 0.15686285495758057\n",
      "epoch 115: loss 0.4009166359901428\n",
      "epoch 116: loss 0.28767675161361694\n",
      "epoch 117: loss 0.250375896692276\n",
      "epoch 118: loss 0.3151155710220337\n",
      "epoch 119: loss 0.39258405566215515\n",
      "epoch 120: loss 0.2626495063304901\n",
      "epoch 121: loss 0.346469521522522\n",
      "epoch 122: loss 0.23447389900684357\n",
      "epoch 123: loss 0.381045937538147\n",
      "epoch 124: loss 0.2998780310153961\n",
      "epoch 125: loss 0.34123414754867554\n",
      "epoch 126: loss 0.3615690767765045\n",
      "epoch 127: loss 0.3757317066192627\n",
      "epoch 128: loss 0.2485770434141159\n",
      "epoch 129: loss 0.33569979667663574\n",
      "epoch 130: loss 0.3587873876094818\n",
      "epoch 131: loss 0.31925901770591736\n",
      "epoch 132: loss 0.3107963502407074\n",
      "epoch 133: loss 0.33501219749450684\n",
      "epoch 134: loss 0.3019171357154846\n",
      "epoch 135: loss 0.2423999309539795\n",
      "epoch 136: loss 0.2209533303976059\n",
      "epoch 0: loss 0.3252258896827698\n",
      "epoch 1: loss 0.33367425203323364\n",
      "epoch 2: loss 0.3566187620162964\n",
      "epoch 3: loss 0.3499464988708496\n",
      "epoch 4: loss 0.3053794205188751\n",
      "epoch 5: loss 0.23918640613555908\n",
      "epoch 6: loss 0.26815444231033325\n",
      "epoch 7: loss 0.21253934502601624\n",
      "epoch 8: loss 0.41231706738471985\n",
      "epoch 9: loss 0.3973982036113739\n",
      "epoch 10: loss 0.32636040449142456\n",
      "epoch 11: loss 0.2681120038032532\n",
      "epoch 12: loss 0.30437546968460083\n",
      "epoch 13: loss 0.4052923619747162\n",
      "epoch 14: loss 0.3685314655303955\n",
      "epoch 15: loss 0.2610735297203064\n",
      "epoch 16: loss 0.34483426809310913\n",
      "epoch 17: loss 0.3372676968574524\n",
      "epoch 18: loss 0.39568468928337097\n",
      "epoch 19: loss 0.2768506407737732\n",
      "epoch 20: loss 0.31737828254699707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: loss 0.33249616622924805\n",
      "epoch 22: loss 0.32193806767463684\n",
      "epoch 23: loss 0.37626227736473083\n",
      "epoch 24: loss 0.3361700773239136\n",
      "epoch 25: loss 0.40672773122787476\n",
      "epoch 26: loss 0.3413233160972595\n",
      "epoch 27: loss 0.34072715044021606\n",
      "epoch 28: loss 0.38605672121047974\n",
      "epoch 29: loss 0.31714940071105957\n",
      "epoch 30: loss 0.2787998914718628\n",
      "epoch 31: loss 0.32772260904312134\n",
      "epoch 32: loss 0.338351845741272\n",
      "epoch 33: loss 0.3483883738517761\n",
      "epoch 34: loss 0.38750529289245605\n",
      "epoch 35: loss 0.2555290758609772\n",
      "epoch 36: loss 0.32897329330444336\n",
      "epoch 37: loss 0.32404348254203796\n",
      "epoch 38: loss 0.22455540299415588\n",
      "epoch 39: loss 0.3086673617362976\n",
      "epoch 40: loss 0.30301031470298767\n",
      "epoch 41: loss 0.2772231101989746\n",
      "epoch 42: loss 0.24237750470638275\n",
      "epoch 43: loss 0.33819055557250977\n",
      "epoch 44: loss 0.32861095666885376\n",
      "epoch 45: loss 0.27759552001953125\n",
      "epoch 46: loss 0.2829933166503906\n",
      "epoch 47: loss 0.2819657027721405\n",
      "epoch 48: loss 0.24228109419345856\n",
      "epoch 49: loss 0.3188455104827881\n",
      "epoch 50: loss 0.27705758810043335\n",
      "epoch 51: loss 0.39674460887908936\n",
      "epoch 52: loss 0.2851742208003998\n",
      "epoch 53: loss 0.3593013882637024\n",
      "epoch 54: loss 0.3448939323425293\n",
      "epoch 55: loss 0.3002426326274872\n",
      "epoch 56: loss 0.34394311904907227\n",
      "epoch 57: loss 0.237534761428833\n",
      "epoch 58: loss 0.31899726390838623\n",
      "epoch 59: loss 0.26211488246917725\n",
      "epoch 60: loss 0.3168834447860718\n",
      "epoch 61: loss 0.21171125769615173\n",
      "epoch 62: loss 0.23687046766281128\n",
      "epoch 63: loss 0.3294467031955719\n",
      "epoch 64: loss 0.27594229578971863\n",
      "epoch 65: loss 0.3834630250930786\n",
      "epoch 66: loss 0.32369667291641235\n",
      "epoch 67: loss 0.37879490852355957\n",
      "epoch 68: loss 0.35565197467803955\n",
      "epoch 69: loss 0.41551488637924194\n",
      "epoch 70: loss 0.3676503896713257\n",
      "epoch 71: loss 0.32989367842674255\n",
      "epoch 72: loss 0.24829664826393127\n",
      "epoch 73: loss 0.30856508016586304\n",
      "epoch 74: loss 0.25504350662231445\n",
      "epoch 75: loss 0.35032153129577637\n",
      "epoch 76: loss 0.31064373254776\n",
      "epoch 77: loss 0.4002110958099365\n",
      "epoch 78: loss 0.30104368925094604\n",
      "epoch 79: loss 0.356087863445282\n",
      "epoch 80: loss 0.39326566457748413\n",
      "epoch 81: loss 0.2757433354854584\n",
      "epoch 82: loss 0.32321780920028687\n",
      "epoch 83: loss 0.3915713429450989\n",
      "epoch 84: loss 0.30115264654159546\n",
      "epoch 85: loss 0.2879723012447357\n",
      "epoch 86: loss 0.34808531403541565\n",
      "epoch 87: loss 0.32222580909729004\n",
      "epoch 88: loss 0.3063880205154419\n",
      "epoch 89: loss 0.3824496865272522\n",
      "epoch 90: loss 0.32410097122192383\n",
      "epoch 91: loss 0.4252409040927887\n",
      "epoch 92: loss 0.3896982669830322\n",
      "epoch 93: loss 0.31963232159614563\n",
      "epoch 94: loss 0.32591795921325684\n",
      "epoch 95: loss 0.19095352292060852\n",
      "epoch 96: loss 0.2433745563030243\n",
      "epoch 97: loss 0.3123692274093628\n",
      "epoch 98: loss 0.35485196113586426\n",
      "epoch 99: loss 0.3386249244213104\n",
      "epoch 100: loss 0.4790568947792053\n",
      "epoch 101: loss 0.31308746337890625\n",
      "epoch 102: loss 0.25855937600135803\n",
      "epoch 103: loss 0.31255626678466797\n",
      "epoch 104: loss 0.24569225311279297\n",
      "epoch 105: loss 0.3613743185997009\n",
      "epoch 106: loss 0.3112758696079254\n",
      "epoch 107: loss 0.2699427902698517\n",
      "epoch 108: loss 0.3382808566093445\n",
      "epoch 109: loss 0.34723034501075745\n",
      "epoch 110: loss 0.3396497666835785\n",
      "epoch 111: loss 0.36434343457221985\n",
      "epoch 112: loss 0.3660483956336975\n",
      "epoch 113: loss 0.2744624614715576\n",
      "epoch 114: loss 0.1563953459262848\n",
      "epoch 115: loss 0.39724892377853394\n",
      "epoch 116: loss 0.2867300510406494\n",
      "epoch 117: loss 0.24277284741401672\n",
      "epoch 118: loss 0.3171849846839905\n",
      "epoch 119: loss 0.391164630651474\n",
      "epoch 120: loss 0.2567892074584961\n",
      "epoch 121: loss 0.35865479707717896\n",
      "epoch 122: loss 0.24357914924621582\n",
      "epoch 123: loss 0.3749942183494568\n",
      "epoch 124: loss 0.2975948452949524\n",
      "epoch 125: loss 0.34719133377075195\n",
      "epoch 126: loss 0.3586485683917999\n",
      "epoch 127: loss 0.372522234916687\n",
      "epoch 128: loss 0.25139039754867554\n",
      "epoch 129: loss 0.33704227209091187\n",
      "epoch 130: loss 0.3553973138332367\n",
      "epoch 131: loss 0.3122554421424866\n",
      "epoch 132: loss 0.3103412389755249\n",
      "epoch 133: loss 0.33855557441711426\n",
      "epoch 134: loss 0.29665324091911316\n",
      "epoch 135: loss 0.24727734923362732\n",
      "epoch 136: loss 0.21789471805095673\n",
      "epoch 0: loss 0.3270685076713562\n",
      "epoch 1: loss 0.3296164572238922\n",
      "epoch 2: loss 0.38450413942337036\n",
      "epoch 3: loss 0.34796202182769775\n",
      "epoch 4: loss 0.3308073878288269\n",
      "epoch 5: loss 0.23794859647750854\n",
      "epoch 6: loss 0.2747887969017029\n",
      "epoch 7: loss 0.22393231093883514\n",
      "epoch 8: loss 0.42876917123794556\n",
      "epoch 9: loss 0.42445552349090576\n",
      "epoch 10: loss 0.3490144610404968\n",
      "epoch 11: loss 0.2819770574569702\n",
      "epoch 12: loss 0.2979564666748047\n",
      "epoch 13: loss 0.4056243300437927\n",
      "epoch 14: loss 0.38433706760406494\n",
      "epoch 15: loss 0.2638005316257477\n",
      "epoch 16: loss 0.3452329635620117\n",
      "epoch 17: loss 0.33765700459480286\n",
      "epoch 18: loss 0.3920922875404358\n",
      "epoch 19: loss 0.2784869968891144\n",
      "epoch 20: loss 0.3226052522659302\n",
      "epoch 21: loss 0.33825182914733887\n",
      "epoch 22: loss 0.32460588216781616\n",
      "epoch 23: loss 0.3729604482650757\n",
      "epoch 24: loss 0.34388259053230286\n",
      "epoch 25: loss 0.3900367319583893\n",
      "epoch 26: loss 0.3526783585548401\n",
      "epoch 27: loss 0.34467726945877075\n",
      "epoch 28: loss 0.37908846139907837\n",
      "epoch 29: loss 0.3119103014469147\n",
      "epoch 30: loss 0.2763655185699463\n",
      "epoch 31: loss 0.3321806788444519\n",
      "epoch 32: loss 0.3331943154335022\n",
      "epoch 33: loss 0.3458552658557892\n",
      "epoch 34: loss 0.3865988254547119\n",
      "epoch 35: loss 0.2614884376525879\n",
      "epoch 36: loss 0.3349827229976654\n",
      "epoch 37: loss 0.3335181474685669\n",
      "epoch 38: loss 0.22341328859329224\n",
      "epoch 39: loss 0.314042329788208\n",
      "epoch 40: loss 0.31323450803756714\n",
      "epoch 41: loss 0.27786338329315186\n",
      "epoch 42: loss 0.24035227298736572\n",
      "epoch 43: loss 0.32953935861587524\n",
      "epoch 44: loss 0.33450818061828613\n",
      "epoch 45: loss 0.2806629538536072\n",
      "epoch 46: loss 0.2829209566116333\n",
      "epoch 47: loss 0.2815917432308197\n",
      "epoch 48: loss 0.24445506930351257\n",
      "epoch 49: loss 0.31487879157066345\n",
      "epoch 50: loss 0.2771385908126831\n",
      "epoch 51: loss 0.3995736837387085\n",
      "epoch 52: loss 0.29969358444213867\n",
      "epoch 53: loss 0.35478395223617554\n",
      "epoch 54: loss 0.33962756395339966\n",
      "epoch 55: loss 0.3071126341819763\n",
      "epoch 56: loss 0.3422256112098694\n",
      "epoch 57: loss 0.24331817030906677\n",
      "epoch 58: loss 0.32065796852111816\n",
      "epoch 59: loss 0.25884559750556946\n",
      "epoch 60: loss 0.3220215141773224\n",
      "epoch 61: loss 0.21283680200576782\n",
      "epoch 62: loss 0.2351900339126587\n",
      "epoch 63: loss 0.32700324058532715\n",
      "epoch 64: loss 0.27762383222579956\n",
      "epoch 65: loss 0.3801983594894409\n",
      "epoch 66: loss 0.32357388734817505\n",
      "epoch 67: loss 0.37708860635757446\n",
      "epoch 68: loss 0.35456377267837524\n",
      "epoch 69: loss 0.4148867130279541\n",
      "epoch 70: loss 0.36660486459732056\n",
      "epoch 71: loss 0.3159635663032532\n",
      "epoch 72: loss 0.25470033288002014\n",
      "epoch 73: loss 0.31221258640289307\n",
      "epoch 74: loss 0.25626590847969055\n",
      "epoch 75: loss 0.3427007496356964\n",
      "epoch 76: loss 0.30826857686042786\n",
      "epoch 77: loss 0.39592260122299194\n",
      "epoch 78: loss 0.30687206983566284\n",
      "epoch 79: loss 0.3109220266342163\n",
      "epoch 80: loss 0.3804333209991455\n",
      "epoch 81: loss 0.27218732237815857\n",
      "epoch 82: loss 0.3318934440612793\n",
      "epoch 83: loss 0.372562050819397\n",
      "epoch 84: loss 0.27046874165534973\n",
      "epoch 85: loss 0.30244725942611694\n",
      "epoch 86: loss 0.3302688002586365\n",
      "epoch 87: loss 0.3368932008743286\n",
      "epoch 88: loss 0.274654746055603\n",
      "epoch 89: loss 0.30880191922187805\n",
      "epoch 90: loss 0.28657567501068115\n",
      "epoch 91: loss 0.33412185311317444\n",
      "epoch 92: loss 0.29907506704330444\n",
      "epoch 93: loss 0.3085768520832062\n",
      "epoch 94: loss 0.29444387555122375\n",
      "epoch 95: loss 0.19748984277248383\n",
      "epoch 96: loss 0.22621214389801025\n",
      "epoch 97: loss 0.2873070240020752\n",
      "epoch 98: loss 0.31123703718185425\n",
      "epoch 99: loss 0.3288348615169525\n",
      "epoch 100: loss 0.4569927155971527\n",
      "epoch 101: loss 0.2978862524032593\n",
      "epoch 102: loss 0.2489895224571228\n",
      "epoch 103: loss 0.30444133281707764\n",
      "epoch 104: loss 0.24522915482521057\n",
      "epoch 105: loss 0.3577369153499603\n",
      "epoch 106: loss 0.3052547872066498\n",
      "epoch 107: loss 0.264440655708313\n",
      "epoch 108: loss 0.34261780977249146\n",
      "epoch 109: loss 0.34660494327545166\n",
      "epoch 110: loss 0.3405147194862366\n",
      "epoch 111: loss 0.35532867908477783\n",
      "epoch 112: loss 0.35979098081588745\n",
      "epoch 113: loss 0.2734164595603943\n",
      "epoch 114: loss 0.14725926518440247\n",
      "epoch 115: loss 0.3955502510070801\n",
      "epoch 116: loss 0.2818582057952881\n",
      "epoch 117: loss 0.2434290498495102\n",
      "epoch 118: loss 0.3128381669521332\n",
      "epoch 119: loss 0.39468914270401\n",
      "epoch 120: loss 0.2559656500816345\n",
      "epoch 121: loss 0.3517948091030121\n",
      "epoch 122: loss 0.23467138409614563\n",
      "epoch 123: loss 0.3733759820461273\n",
      "epoch 124: loss 0.31018078327178955\n",
      "epoch 125: loss 0.34224268794059753\n",
      "epoch 126: loss 0.37680691480636597\n",
      "epoch 127: loss 0.40674012899398804\n",
      "epoch 128: loss 0.27182722091674805\n",
      "epoch 129: loss 0.3462604284286499\n",
      "epoch 130: loss 0.36273646354675293\n",
      "epoch 131: loss 0.3194773197174072\n",
      "epoch 132: loss 0.3110195994377136\n",
      "epoch 133: loss 0.338881254196167\n",
      "epoch 134: loss 0.3033974766731262\n",
      "epoch 135: loss 0.26553475856781006\n",
      "epoch 136: loss 0.2336733639240265\n",
      "epoch 0: loss 0.3339099884033203\n",
      "epoch 1: loss 0.32049524784088135\n",
      "epoch 2: loss 0.3843787908554077\n",
      "epoch 3: loss 0.3422040045261383\n",
      "epoch 4: loss 0.29170870780944824\n",
      "epoch 5: loss 0.25393736362457275\n",
      "epoch 6: loss 0.27076631784439087\n",
      "epoch 7: loss 0.21992933750152588\n",
      "epoch 8: loss 0.39970558881759644\n",
      "epoch 9: loss 0.39901310205459595\n",
      "epoch 10: loss 0.3195367157459259\n",
      "epoch 11: loss 0.27097105979919434\n",
      "epoch 12: loss 0.294502854347229\n",
      "epoch 13: loss 0.4133848547935486\n",
      "epoch 14: loss 0.33451682329177856\n",
      "epoch 15: loss 0.2703687846660614\n",
      "epoch 16: loss 0.3348119854927063\n",
      "epoch 17: loss 0.3310621976852417\n",
      "epoch 18: loss 0.3652251064777374\n",
      "epoch 19: loss 0.2724599838256836\n",
      "epoch 20: loss 0.29496604204177856\n",
      "epoch 21: loss 0.3250463604927063\n",
      "epoch 22: loss 0.3018561601638794\n",
      "epoch 23: loss 0.3623049855232239\n",
      "epoch 24: loss 0.3284027576446533\n",
      "epoch 25: loss 0.3911287784576416\n",
      "epoch 26: loss 0.3481351137161255\n",
      "epoch 27: loss 0.3364063501358032\n",
      "epoch 28: loss 0.3983549177646637\n",
      "epoch 29: loss 0.3183619976043701\n",
      "epoch 30: loss 0.3009878396987915\n",
      "epoch 31: loss 0.32860809564590454\n",
      "epoch 32: loss 0.3496490716934204\n",
      "epoch 33: loss 0.32960212230682373\n",
      "epoch 34: loss 0.3745686113834381\n",
      "epoch 35: loss 0.26723986864089966\n",
      "epoch 36: loss 0.32295554876327515\n",
      "epoch 37: loss 0.3300386965274811\n",
      "epoch 38: loss 0.22347599267959595\n",
      "epoch 39: loss 0.2972102761268616\n",
      "epoch 40: loss 0.2832657992839813\n",
      "epoch 41: loss 0.27467188239097595\n",
      "epoch 42: loss 0.2660205662250519\n",
      "epoch 43: loss 0.33613285422325134\n",
      "epoch 44: loss 0.3263680040836334\n",
      "epoch 45: loss 0.2795436978340149\n",
      "epoch 46: loss 0.28835010528564453\n",
      "epoch 47: loss 0.284974604845047\n",
      "epoch 48: loss 0.23555287718772888\n",
      "epoch 49: loss 0.3141641914844513\n",
      "epoch 50: loss 0.2796160578727722\n",
      "epoch 51: loss 0.3910086154937744\n",
      "epoch 52: loss 0.273202121257782\n",
      "epoch 53: loss 0.3520287871360779\n",
      "epoch 54: loss 0.34588074684143066\n",
      "epoch 55: loss 0.2940903306007385\n",
      "epoch 56: loss 0.3458622694015503\n",
      "epoch 57: loss 0.239712193608284\n",
      "epoch 58: loss 0.3191106915473938\n",
      "epoch 59: loss 0.26123422384262085\n",
      "epoch 60: loss 0.3150508999824524\n",
      "epoch 61: loss 0.21621167659759521\n",
      "epoch 62: loss 0.23904407024383545\n",
      "epoch 63: loss 0.3278208374977112\n",
      "epoch 64: loss 0.27722829580307007\n",
      "epoch 65: loss 0.3832945227622986\n",
      "epoch 66: loss 0.32453620433807373\n",
      "epoch 67: loss 0.3813447654247284\n",
      "epoch 68: loss 0.3507726788520813\n",
      "epoch 69: loss 0.40885549783706665\n",
      "epoch 70: loss 0.3644511103630066\n",
      "epoch 71: loss 0.3172529637813568\n",
      "epoch 72: loss 0.2454824149608612\n",
      "epoch 73: loss 0.30843716859817505\n",
      "epoch 74: loss 0.251351535320282\n",
      "epoch 75: loss 0.35128694772720337\n",
      "epoch 76: loss 0.3135598599910736\n",
      "epoch 77: loss 0.4021269977092743\n",
      "epoch 78: loss 0.30462750792503357\n",
      "epoch 79: loss 0.3237459659576416\n",
      "epoch 80: loss 0.37855103611946106\n",
      "epoch 81: loss 0.2702006995677948\n",
      "epoch 82: loss 0.31505003571510315\n",
      "epoch 83: loss 0.37757161259651184\n",
      "epoch 84: loss 0.2840225398540497\n",
      "epoch 85: loss 0.2891382575035095\n",
      "epoch 86: loss 0.3375725746154785\n",
      "epoch 87: loss 0.3293440341949463\n",
      "epoch 88: loss 0.2883560061454773\n",
      "epoch 89: loss 0.3742135465145111\n",
      "epoch 90: loss 0.33203327655792236\n",
      "epoch 91: loss 0.45215845108032227\n",
      "epoch 92: loss 0.40529704093933105\n",
      "epoch 93: loss 0.3034880757331848\n",
      "epoch 94: loss 0.31705227494239807\n",
      "epoch 95: loss 0.17289508879184723\n",
      "epoch 96: loss 0.22259923815727234\n",
      "epoch 97: loss 0.3157520294189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98: loss 0.3635126054286957\n",
      "epoch 99: loss 0.343782901763916\n",
      "epoch 100: loss 0.4827636182308197\n",
      "epoch 101: loss 0.30012622475624084\n",
      "epoch 102: loss 0.24726144969463348\n",
      "epoch 103: loss 0.29618561267852783\n",
      "epoch 104: loss 0.24553750455379486\n",
      "epoch 105: loss 0.3562156558036804\n",
      "epoch 106: loss 0.30924326181411743\n",
      "epoch 107: loss 0.26840388774871826\n",
      "epoch 108: loss 0.332786500453949\n",
      "epoch 109: loss 0.3459123969078064\n",
      "epoch 110: loss 0.33393770456314087\n",
      "epoch 111: loss 0.3657735288143158\n",
      "epoch 112: loss 0.3733024001121521\n",
      "epoch 113: loss 0.2759818136692047\n",
      "epoch 114: loss 0.1558767557144165\n",
      "epoch 115: loss 0.3981754779815674\n",
      "epoch 116: loss 0.2884233593940735\n",
      "epoch 117: loss 0.24514886736869812\n",
      "epoch 118: loss 0.31273192167282104\n",
      "epoch 119: loss 0.3952539265155792\n",
      "epoch 120: loss 0.2581787407398224\n",
      "epoch 121: loss 0.3602895140647888\n",
      "epoch 122: loss 0.2360978126525879\n",
      "epoch 123: loss 0.3727768659591675\n",
      "epoch 124: loss 0.3297151029109955\n",
      "epoch 125: loss 0.343746542930603\n",
      "epoch 126: loss 0.3763974606990814\n",
      "epoch 127: loss 0.42902496457099915\n",
      "epoch 128: loss 0.30128800868988037\n",
      "epoch 129: loss 0.341522216796875\n",
      "epoch 130: loss 0.438256174325943\n",
      "epoch 131: loss 0.36195334792137146\n",
      "epoch 132: loss 0.38496705889701843\n",
      "epoch 133: loss 0.4080287218093872\n",
      "epoch 134: loss 0.334219753742218\n",
      "epoch 135: loss 0.300132691860199\n",
      "epoch 136: loss 0.2817908227443695\n",
      "epoch 0: loss 0.3752738833427429\n",
      "epoch 1: loss 0.32929378747940063\n",
      "epoch 2: loss 0.3942645490169525\n",
      "epoch 3: loss 0.35671883821487427\n",
      "epoch 4: loss 0.35374125838279724\n",
      "epoch 5: loss 0.26111721992492676\n",
      "epoch 6: loss 0.27377092838287354\n",
      "epoch 7: loss 0.21889032423496246\n",
      "epoch 8: loss 0.4212392270565033\n",
      "epoch 9: loss 0.44585177302360535\n",
      "epoch 10: loss 0.3547424077987671\n",
      "epoch 11: loss 0.2899385392665863\n",
      "epoch 12: loss 0.3146510124206543\n",
      "epoch 13: loss 0.425356388092041\n",
      "epoch 14: loss 0.36545175313949585\n",
      "epoch 15: loss 0.300963819026947\n",
      "epoch 16: loss 0.35468170046806335\n",
      "epoch 17: loss 0.33327627182006836\n",
      "epoch 18: loss 0.3880322575569153\n",
      "epoch 19: loss 0.2609955668449402\n",
      "epoch 20: loss 0.3062424659729004\n",
      "epoch 21: loss 0.3430273234844208\n",
      "epoch 22: loss 0.3224389851093292\n",
      "epoch 23: loss 0.3708781599998474\n",
      "epoch 24: loss 0.34096524119377136\n",
      "epoch 25: loss 0.3950197100639343\n",
      "epoch 26: loss 0.3725842833518982\n",
      "epoch 27: loss 0.3362160325050354\n",
      "epoch 28: loss 0.39950990676879883\n",
      "epoch 29: loss 0.3139106035232544\n",
      "epoch 30: loss 0.2909400165081024\n",
      "epoch 31: loss 0.33039939403533936\n",
      "epoch 32: loss 0.3492949604988098\n",
      "epoch 33: loss 0.3309611678123474\n",
      "epoch 34: loss 0.3779502511024475\n",
      "epoch 35: loss 0.268190860748291\n",
      "epoch 36: loss 0.329668790102005\n",
      "epoch 37: loss 0.32991230487823486\n",
      "epoch 38: loss 0.22692206501960754\n",
      "epoch 39: loss 0.31645065546035767\n",
      "epoch 40: loss 0.30815476179122925\n",
      "epoch 41: loss 0.2822480797767639\n",
      "epoch 42: loss 0.2518391013145447\n",
      "epoch 43: loss 0.32988786697387695\n",
      "epoch 44: loss 0.3355349600315094\n",
      "epoch 45: loss 0.2797325551509857\n",
      "epoch 46: loss 0.2914230227470398\n",
      "epoch 47: loss 0.28561127185821533\n",
      "epoch 48: loss 0.2454170286655426\n",
      "epoch 49: loss 0.31825166940689087\n",
      "epoch 50: loss 0.2830832898616791\n",
      "epoch 51: loss 0.40929365158081055\n",
      "epoch 52: loss 0.3198656141757965\n",
      "epoch 53: loss 0.35799747705459595\n",
      "epoch 54: loss 0.33403831720352173\n",
      "epoch 55: loss 0.30340099334716797\n",
      "epoch 56: loss 0.3460158705711365\n",
      "epoch 57: loss 0.24813175201416016\n",
      "epoch 58: loss 0.3288308382034302\n",
      "epoch 59: loss 0.2558523416519165\n",
      "epoch 60: loss 0.32994455099105835\n",
      "epoch 61: loss 0.21880123019218445\n",
      "epoch 62: loss 0.23902718722820282\n",
      "epoch 63: loss 0.3297736942768097\n",
      "epoch 64: loss 0.2798275947570801\n",
      "epoch 65: loss 0.3842524588108063\n",
      "epoch 66: loss 0.3290603756904602\n",
      "epoch 67: loss 0.37109375\n",
      "epoch 68: loss 0.3545153737068176\n",
      "epoch 69: loss 0.41103774309158325\n",
      "epoch 70: loss 0.3658185601234436\n",
      "epoch 71: loss 0.3129112720489502\n",
      "epoch 72: loss 0.2555411159992218\n",
      "epoch 73: loss 0.3159359097480774\n",
      "epoch 74: loss 0.25694146752357483\n",
      "epoch 75: loss 0.34102538228034973\n",
      "epoch 76: loss 0.3150985836982727\n",
      "epoch 77: loss 0.3935481905937195\n",
      "epoch 78: loss 0.3118414878845215\n",
      "epoch 79: loss 0.3080845773220062\n",
      "epoch 80: loss 0.3759572505950928\n",
      "epoch 81: loss 0.2735590934753418\n",
      "epoch 82: loss 0.33591794967651367\n",
      "epoch 83: loss 0.3847694396972656\n",
      "epoch 84: loss 0.24229054152965546\n",
      "epoch 85: loss 0.2990299463272095\n",
      "epoch 86: loss 0.3262885808944702\n",
      "epoch 87: loss 0.32837975025177\n",
      "epoch 88: loss 0.2793617248535156\n",
      "epoch 89: loss 0.30110853910446167\n",
      "epoch 90: loss 0.24636510014533997\n",
      "epoch 91: loss 0.3104252815246582\n",
      "epoch 92: loss 0.2580883800983429\n",
      "epoch 93: loss 0.30451759696006775\n",
      "epoch 94: loss 0.2968437671661377\n",
      "epoch 95: loss 0.19259962439537048\n",
      "epoch 96: loss 0.20584245026111603\n",
      "epoch 97: loss 0.28271499276161194\n",
      "epoch 98: loss 0.31407630443573\n",
      "epoch 99: loss 0.3520050346851349\n",
      "epoch 100: loss 0.4866281747817993\n",
      "epoch 101: loss 0.3010563254356384\n",
      "epoch 102: loss 0.24740445613861084\n",
      "epoch 103: loss 0.3024463355541229\n",
      "epoch 104: loss 0.2449328899383545\n",
      "epoch 105: loss 0.36746007204055786\n",
      "epoch 106: loss 0.3088873624801636\n",
      "epoch 107: loss 0.2690070867538452\n",
      "epoch 108: loss 0.3475594222545624\n",
      "epoch 109: loss 0.3434866666793823\n",
      "epoch 110: loss 0.34091055393218994\n",
      "epoch 111: loss 0.3609413206577301\n",
      "epoch 112: loss 0.35698646306991577\n",
      "epoch 113: loss 0.27146419882774353\n",
      "epoch 114: loss 0.15959598124027252\n",
      "epoch 115: loss 0.3914182782173157\n",
      "epoch 116: loss 0.28038424253463745\n",
      "epoch 117: loss 0.2483496367931366\n",
      "epoch 118: loss 0.312122642993927\n",
      "epoch 119: loss 0.3900434970855713\n",
      "epoch 120: loss 0.2593676447868347\n",
      "epoch 121: loss 0.3550415635108948\n",
      "epoch 122: loss 0.23161005973815918\n",
      "epoch 123: loss 0.3764033019542694\n",
      "epoch 124: loss 0.3286629021167755\n",
      "epoch 125: loss 0.3390517234802246\n",
      "epoch 126: loss 0.3750663995742798\n",
      "epoch 127: loss 0.4229390025138855\n",
      "epoch 128: loss 0.2902311086654663\n",
      "epoch 129: loss 0.3413693606853485\n",
      "epoch 130: loss 0.42913272976875305\n",
      "epoch 131: loss 0.3535428047180176\n",
      "epoch 132: loss 0.37308672070503235\n",
      "epoch 133: loss 0.39005330204963684\n",
      "epoch 134: loss 0.31768661737442017\n",
      "epoch 135: loss 0.2676599621772766\n",
      "epoch 136: loss 0.2476329356431961\n",
      "epoch 0: loss 0.35284751653671265\n",
      "epoch 1: loss 0.3256959915161133\n",
      "epoch 2: loss 0.35684165358543396\n",
      "epoch 3: loss 0.3757359981536865\n",
      "epoch 4: loss 0.31855738162994385\n",
      "epoch 5: loss 0.25605329871177673\n",
      "epoch 6: loss 0.27847447991371155\n",
      "epoch 7: loss 0.22318768501281738\n",
      "epoch 8: loss 0.4022793173789978\n",
      "epoch 9: loss 0.4107709527015686\n",
      "epoch 10: loss 0.3451007604598999\n",
      "epoch 11: loss 0.281438410282135\n",
      "epoch 12: loss 0.29718756675720215\n",
      "epoch 13: loss 0.4362567663192749\n",
      "epoch 14: loss 0.33511459827423096\n",
      "epoch 15: loss 0.2697259783744812\n",
      "epoch 16: loss 0.35663560032844543\n",
      "epoch 17: loss 0.3281571865081787\n",
      "epoch 18: loss 0.35502511262893677\n",
      "epoch 19: loss 0.26852717995643616\n",
      "epoch 20: loss 0.2968008518218994\n",
      "epoch 21: loss 0.322390615940094\n",
      "epoch 22: loss 0.3082941174507141\n",
      "epoch 23: loss 0.36326009035110474\n",
      "epoch 24: loss 0.3515016734600067\n",
      "epoch 25: loss 0.37421783804893494\n",
      "epoch 26: loss 0.310102641582489\n",
      "epoch 27: loss 0.33261942863464355\n",
      "epoch 28: loss 0.39108824729919434\n",
      "epoch 29: loss 0.3196321725845337\n",
      "epoch 30: loss 0.28765544295310974\n",
      "epoch 31: loss 0.3346945345401764\n",
      "epoch 32: loss 0.33064499497413635\n",
      "epoch 33: loss 0.33622807264328003\n",
      "epoch 34: loss 0.3747699558734894\n",
      "epoch 35: loss 0.26504290103912354\n",
      "epoch 36: loss 0.32617926597595215\n",
      "epoch 37: loss 0.3418542146682739\n",
      "epoch 38: loss 0.2386309951543808\n",
      "epoch 39: loss 0.3280956745147705\n",
      "epoch 40: loss 0.33859753608703613\n",
      "epoch 41: loss 0.2949700951576233\n",
      "epoch 42: loss 0.2457226663827896\n",
      "epoch 43: loss 0.3161922097206116\n",
      "epoch 44: loss 0.3278319537639618\n",
      "epoch 45: loss 0.2946956157684326\n",
      "epoch 46: loss 0.2917194068431854\n",
      "epoch 47: loss 0.2827451229095459\n",
      "epoch 48: loss 0.2356095314025879\n",
      "epoch 49: loss 0.31306442618370056\n",
      "epoch 50: loss 0.2807677686214447\n",
      "epoch 51: loss 0.4134083390235901\n",
      "epoch 52: loss 0.3244086503982544\n",
      "epoch 53: loss 0.3609645962715149\n",
      "epoch 54: loss 0.3326312303543091\n",
      "epoch 55: loss 0.31158849596977234\n",
      "epoch 56: loss 0.33911052346229553\n",
      "epoch 57: loss 0.25543713569641113\n",
      "epoch 58: loss 0.3235465884208679\n",
      "epoch 59: loss 0.25818532705307007\n",
      "epoch 60: loss 0.32804667949676514\n",
      "epoch 61: loss 0.21933487057685852\n",
      "epoch 62: loss 0.23732160031795502\n",
      "epoch 63: loss 0.3320351839065552\n",
      "epoch 64: loss 0.27337974309921265\n",
      "epoch 65: loss 0.3791159391403198\n",
      "epoch 66: loss 0.3298669755458832\n",
      "epoch 67: loss 0.3905542492866516\n",
      "epoch 68: loss 0.35554933547973633\n",
      "epoch 69: loss 0.4071032404899597\n",
      "epoch 70: loss 0.3656865954399109\n",
      "epoch 71: loss 0.31264907121658325\n",
      "epoch 72: loss 0.25615280866622925\n",
      "epoch 73: loss 0.3144382834434509\n",
      "epoch 74: loss 0.24938476085662842\n",
      "epoch 75: loss 0.3451633155345917\n",
      "epoch 76: loss 0.31985852122306824\n",
      "epoch 77: loss 0.3910684883594513\n",
      "epoch 78: loss 0.3144875764846802\n",
      "epoch 79: loss 0.303911030292511\n",
      "epoch 80: loss 0.37210047245025635\n",
      "epoch 81: loss 0.27059024572372437\n",
      "epoch 82: loss 0.3302828073501587\n",
      "epoch 83: loss 0.37556663155555725\n",
      "epoch 84: loss 0.267473965883255\n",
      "epoch 85: loss 0.30310899019241333\n",
      "epoch 86: loss 0.3223001956939697\n",
      "epoch 87: loss 0.32827574014663696\n",
      "epoch 88: loss 0.27474862337112427\n",
      "epoch 89: loss 0.30054566264152527\n",
      "epoch 90: loss 0.26407086849212646\n",
      "epoch 91: loss 0.319740891456604\n",
      "epoch 92: loss 0.2739410102367401\n",
      "epoch 93: loss 0.30418604612350464\n",
      "epoch 94: loss 0.3026287257671356\n",
      "epoch 95: loss 0.20550799369812012\n",
      "epoch 96: loss 0.20943492650985718\n",
      "epoch 97: loss 0.282543808221817\n",
      "epoch 98: loss 0.3139534592628479\n",
      "epoch 99: loss 0.3502315282821655\n",
      "epoch 100: loss 0.4833776652812958\n",
      "epoch 101: loss 0.30425402522087097\n",
      "epoch 102: loss 0.25434017181396484\n",
      "epoch 103: loss 0.31981685757637024\n",
      "epoch 104: loss 0.24789415299892426\n",
      "epoch 105: loss 0.36324021220207214\n",
      "epoch 106: loss 0.3064923882484436\n",
      "epoch 107: loss 0.2697058916091919\n",
      "epoch 108: loss 0.3524109721183777\n",
      "epoch 109: loss 0.35001981258392334\n",
      "epoch 110: loss 0.34046298265457153\n",
      "epoch 111: loss 0.35243237018585205\n",
      "epoch 112: loss 0.356250524520874\n",
      "epoch 113: loss 0.27297312021255493\n",
      "epoch 114: loss 0.1595776379108429\n",
      "epoch 115: loss 0.38839101791381836\n",
      "epoch 116: loss 0.27604812383651733\n",
      "epoch 117: loss 0.25827932357788086\n",
      "epoch 118: loss 0.31188005208969116\n",
      "epoch 119: loss 0.3900159001350403\n",
      "epoch 120: loss 0.2563883662223816\n",
      "epoch 121: loss 0.3617152273654938\n",
      "epoch 122: loss 0.23173895478248596\n",
      "epoch 123: loss 0.3853667974472046\n",
      "epoch 124: loss 0.3259524703025818\n",
      "epoch 125: loss 0.338593065738678\n",
      "epoch 126: loss 0.36754077672958374\n",
      "epoch 127: loss 0.3889666795730591\n",
      "epoch 128: loss 0.25412702560424805\n",
      "epoch 129: loss 0.34266984462738037\n",
      "epoch 130: loss 0.3559402823448181\n",
      "epoch 131: loss 0.3175521492958069\n",
      "epoch 132: loss 0.31453970074653625\n",
      "epoch 133: loss 0.3422802686691284\n",
      "epoch 134: loss 0.2994464635848999\n",
      "epoch 135: loss 0.24098481237888336\n",
      "epoch 136: loss 0.22243976593017578\n",
      "epoch 0: loss 0.3353058993816376\n",
      "epoch 1: loss 0.32163724303245544\n",
      "epoch 2: loss 0.38581204414367676\n",
      "epoch 3: loss 0.3456944525241852\n",
      "epoch 4: loss 0.2997395992279053\n",
      "epoch 5: loss 0.25726616382598877\n",
      "epoch 6: loss 0.26715755462646484\n",
      "epoch 7: loss 0.21378067135810852\n",
      "epoch 8: loss 0.40371307730674744\n",
      "epoch 9: loss 0.3899499773979187\n",
      "epoch 10: loss 0.3423122763633728\n",
      "epoch 11: loss 0.28489911556243896\n",
      "epoch 12: loss 0.29151102900505066\n",
      "epoch 13: loss 0.44183945655822754\n",
      "epoch 14: loss 0.3268313407897949\n",
      "epoch 15: loss 0.2636098861694336\n",
      "epoch 16: loss 0.3462992310523987\n",
      "epoch 17: loss 0.3256199061870575\n",
      "epoch 18: loss 0.3511747717857361\n",
      "epoch 19: loss 0.283138632774353\n",
      "epoch 20: loss 0.2985539138317108\n",
      "epoch 21: loss 0.32359176874160767\n",
      "epoch 22: loss 0.31824791431427\n",
      "epoch 23: loss 0.3645276427268982\n",
      "epoch 24: loss 0.35701483488082886\n",
      "epoch 25: loss 0.37458765506744385\n",
      "epoch 26: loss 0.29765084385871887\n",
      "epoch 27: loss 0.34976398944854736\n",
      "epoch 28: loss 0.3840506076812744\n",
      "epoch 29: loss 0.3149513006210327\n",
      "epoch 30: loss 0.2818347215652466\n",
      "epoch 31: loss 0.3363571763038635\n",
      "epoch 32: loss 0.3215569257736206\n",
      "epoch 33: loss 0.35059183835983276\n",
      "epoch 34: loss 0.36545878648757935\n",
      "epoch 35: loss 0.26728445291519165\n",
      "epoch 36: loss 0.3389463722705841\n",
      "epoch 37: loss 0.30944889783859253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38: loss 0.21854038536548615\n",
      "epoch 39: loss 0.29140418767929077\n",
      "epoch 40: loss 0.30268245935440063\n",
      "epoch 41: loss 0.2867208421230316\n",
      "epoch 42: loss 0.23297269642353058\n",
      "epoch 43: loss 0.3179869055747986\n",
      "epoch 44: loss 0.32109546661376953\n",
      "epoch 45: loss 0.2865135073661804\n",
      "epoch 46: loss 0.2747305631637573\n",
      "epoch 47: loss 0.27628737688064575\n",
      "epoch 48: loss 0.24735920131206512\n",
      "epoch 49: loss 0.33698558807373047\n",
      "epoch 50: loss 0.2772131562232971\n",
      "epoch 51: loss 0.3937343657016754\n",
      "epoch 52: loss 0.2865464985370636\n",
      "epoch 53: loss 0.35737520456314087\n",
      "epoch 54: loss 0.33787500858306885\n",
      "epoch 55: loss 0.3062310516834259\n",
      "epoch 56: loss 0.3388064205646515\n",
      "epoch 57: loss 0.23933197557926178\n",
      "epoch 58: loss 0.3162091374397278\n",
      "epoch 59: loss 0.2643021047115326\n",
      "epoch 60: loss 0.3125631809234619\n",
      "epoch 61: loss 0.20697027444839478\n",
      "epoch 62: loss 0.23372897505760193\n",
      "epoch 63: loss 0.3312484622001648\n",
      "epoch 64: loss 0.2723703384399414\n",
      "epoch 65: loss 0.38263580203056335\n",
      "epoch 66: loss 0.3223678767681122\n",
      "epoch 67: loss 0.3829345703125\n",
      "epoch 68: loss 0.3548034131526947\n",
      "epoch 69: loss 0.4130459427833557\n",
      "epoch 70: loss 0.37728261947631836\n",
      "epoch 71: loss 0.3172665238380432\n",
      "epoch 72: loss 0.24478620290756226\n",
      "epoch 73: loss 0.31021302938461304\n",
      "epoch 74: loss 0.25910240411758423\n",
      "epoch 75: loss 0.3460988402366638\n",
      "epoch 76: loss 0.3082383871078491\n",
      "epoch 77: loss 0.3933507800102234\n",
      "epoch 78: loss 0.30234575271606445\n",
      "epoch 79: loss 0.3220762610435486\n",
      "epoch 80: loss 0.37711769342422485\n",
      "epoch 81: loss 0.2713663876056671\n",
      "epoch 82: loss 0.32127052545547485\n",
      "epoch 83: loss 0.37680530548095703\n",
      "epoch 84: loss 0.29373374581336975\n",
      "epoch 85: loss 0.29123932123184204\n",
      "epoch 86: loss 0.3364368677139282\n",
      "epoch 87: loss 0.3258082866668701\n",
      "epoch 88: loss 0.29381129145622253\n",
      "epoch 89: loss 0.37894463539123535\n",
      "epoch 90: loss 0.3287014961242676\n",
      "epoch 91: loss 0.44830793142318726\n",
      "epoch 92: loss 0.4047122895717621\n",
      "epoch 93: loss 0.30319514870643616\n",
      "epoch 94: loss 0.3210962414741516\n",
      "epoch 95: loss 0.1790965497493744\n",
      "epoch 96: loss 0.23309361934661865\n",
      "epoch 97: loss 0.3200201392173767\n",
      "epoch 98: loss 0.36627477407455444\n",
      "epoch 99: loss 0.3377155065536499\n",
      "epoch 100: loss 0.4834594428539276\n",
      "epoch 101: loss 0.31259310245513916\n",
      "epoch 102: loss 0.25613656640052795\n",
      "epoch 103: loss 0.3143664002418518\n",
      "epoch 104: loss 0.24458180367946625\n",
      "epoch 105: loss 0.3600923717021942\n",
      "epoch 106: loss 0.31131619215011597\n",
      "epoch 107: loss 0.2692309021949768\n",
      "epoch 108: loss 0.3334813714027405\n",
      "epoch 109: loss 0.3484756350517273\n",
      "epoch 110: loss 0.3363139033317566\n",
      "epoch 111: loss 0.3653267025947571\n",
      "epoch 112: loss 0.3658737242221832\n",
      "epoch 113: loss 0.2769390940666199\n",
      "epoch 114: loss 0.16209429502487183\n",
      "epoch 115: loss 0.39636674523353577\n",
      "epoch 116: loss 0.28228306770324707\n",
      "epoch 117: loss 0.2450675368309021\n",
      "epoch 118: loss 0.31533706188201904\n",
      "epoch 119: loss 0.38919398188591003\n",
      "epoch 120: loss 0.25649088621139526\n",
      "epoch 121: loss 0.35806506872177124\n",
      "epoch 122: loss 0.23685279488563538\n",
      "epoch 123: loss 0.3758334815502167\n",
      "epoch 124: loss 0.3018952012062073\n",
      "epoch 125: loss 0.342095285654068\n",
      "epoch 126: loss 0.361952006816864\n",
      "epoch 127: loss 0.37424561381340027\n",
      "epoch 128: loss 0.24758435785770416\n",
      "epoch 129: loss 0.32926928997039795\n",
      "epoch 130: loss 0.3603081703186035\n",
      "epoch 131: loss 0.31335389614105225\n",
      "epoch 132: loss 0.3129294514656067\n",
      "epoch 133: loss 0.34180358052253723\n",
      "epoch 134: loss 0.3005563020706177\n",
      "epoch 135: loss 0.2427186667919159\n",
      "epoch 136: loss 0.21461348235607147\n",
      "epoch 0: loss 0.31748712062835693\n",
      "epoch 1: loss 0.34096765518188477\n",
      "epoch 2: loss 0.3575100004673004\n",
      "epoch 3: loss 0.34518495202064514\n",
      "epoch 4: loss 0.29399925470352173\n",
      "epoch 5: loss 0.23723897337913513\n",
      "epoch 6: loss 0.2662321925163269\n",
      "epoch 7: loss 0.21498799324035645\n",
      "epoch 8: loss 0.4175403416156769\n",
      "epoch 9: loss 0.40920549631118774\n",
      "epoch 10: loss 0.3199533522129059\n",
      "epoch 11: loss 0.26077455282211304\n",
      "epoch 12: loss 0.3033714294433594\n",
      "epoch 13: loss 0.4048967957496643\n",
      "epoch 14: loss 0.35262614488601685\n",
      "epoch 15: loss 0.25868064165115356\n",
      "epoch 16: loss 0.33921149373054504\n",
      "epoch 17: loss 0.3387228846549988\n",
      "epoch 18: loss 0.4016570448875427\n",
      "epoch 19: loss 0.266634076833725\n",
      "epoch 20: loss 0.3061046004295349\n",
      "epoch 21: loss 0.32660412788391113\n",
      "epoch 22: loss 0.3039805293083191\n",
      "epoch 23: loss 0.36023539304733276\n",
      "epoch 24: loss 0.34321606159210205\n",
      "epoch 25: loss 0.38329094648361206\n",
      "epoch 26: loss 0.3099980652332306\n",
      "epoch 27: loss 0.33800092339515686\n",
      "epoch 28: loss 0.38969939947128296\n",
      "epoch 29: loss 0.313021183013916\n",
      "epoch 30: loss 0.28306326270103455\n",
      "epoch 31: loss 0.33781927824020386\n",
      "epoch 32: loss 0.3321077823638916\n",
      "epoch 33: loss 0.3501395583152771\n",
      "epoch 34: loss 0.3789595067501068\n",
      "epoch 35: loss 0.25429344177246094\n",
      "epoch 36: loss 0.3309013843536377\n",
      "epoch 37: loss 0.30625149607658386\n",
      "epoch 38: loss 0.21109403669834137\n",
      "epoch 39: loss 0.28885236382484436\n",
      "epoch 40: loss 0.2928280830383301\n",
      "epoch 41: loss 0.2797652781009674\n",
      "epoch 42: loss 0.2454814910888672\n",
      "epoch 43: loss 0.3192111849784851\n",
      "epoch 44: loss 0.32210081815719604\n",
      "epoch 45: loss 0.2793715298175812\n",
      "epoch 46: loss 0.2752990126609802\n",
      "epoch 47: loss 0.2777124047279358\n",
      "epoch 48: loss 0.2366630733013153\n",
      "epoch 49: loss 0.32708847522735596\n",
      "epoch 50: loss 0.2791101336479187\n",
      "epoch 51: loss 0.3939856290817261\n",
      "epoch 52: loss 0.27666452527046204\n",
      "epoch 53: loss 0.3464686870574951\n",
      "epoch 54: loss 0.34487858414649963\n",
      "epoch 55: loss 0.29421210289001465\n",
      "epoch 56: loss 0.34943729639053345\n",
      "epoch 57: loss 0.2471875101327896\n",
      "epoch 58: loss 0.31649544835090637\n",
      "epoch 59: loss 0.2574382424354553\n",
      "epoch 60: loss 0.31794029474258423\n",
      "epoch 61: loss 0.2195688933134079\n",
      "epoch 62: loss 0.24257118999958038\n",
      "epoch 63: loss 0.33186545968055725\n",
      "epoch 64: loss 0.2777215540409088\n",
      "epoch 65: loss 0.3818603754043579\n",
      "epoch 66: loss 0.3258804380893707\n",
      "epoch 67: loss 0.3848803639411926\n",
      "epoch 68: loss 0.3571432828903198\n",
      "epoch 69: loss 0.413533091545105\n",
      "epoch 70: loss 0.36935877799987793\n",
      "epoch 71: loss 0.2995680272579193\n",
      "epoch 72: loss 0.24598652124404907\n",
      "epoch 73: loss 0.31295204162597656\n",
      "epoch 74: loss 0.24782587587833405\n",
      "epoch 75: loss 0.3390779197216034\n",
      "epoch 76: loss 0.31359389424324036\n",
      "epoch 77: loss 0.3884545564651489\n",
      "epoch 78: loss 0.32016807794570923\n",
      "epoch 79: loss 0.30389508605003357\n",
      "epoch 80: loss 0.36238157749176025\n",
      "epoch 81: loss 0.26208317279815674\n",
      "epoch 82: loss 0.3131404519081116\n",
      "epoch 83: loss 0.36522597074508667\n",
      "epoch 84: loss 0.2735241651535034\n",
      "epoch 85: loss 0.2931586503982544\n",
      "epoch 86: loss 0.333726704120636\n",
      "epoch 87: loss 0.3415953516960144\n",
      "epoch 88: loss 0.27164649963378906\n",
      "epoch 89: loss 0.3337867856025696\n",
      "epoch 90: loss 0.29813385009765625\n",
      "epoch 91: loss 0.3849862515926361\n",
      "epoch 92: loss 0.3632361888885498\n",
      "epoch 93: loss 0.30321004986763\n",
      "epoch 94: loss 0.2926745116710663\n",
      "epoch 95: loss 0.17912444472312927\n",
      "epoch 96: loss 0.2189236283302307\n",
      "epoch 97: loss 0.297166109085083\n",
      "epoch 98: loss 0.32670241594314575\n",
      "epoch 99: loss 0.31853729486465454\n",
      "epoch 100: loss 0.4590691030025482\n",
      "epoch 101: loss 0.2935400605201721\n",
      "epoch 102: loss 0.24138179421424866\n",
      "epoch 103: loss 0.2965555489063263\n",
      "epoch 104: loss 0.24857625365257263\n",
      "epoch 105: loss 0.36128556728363037\n",
      "epoch 106: loss 0.30616939067840576\n",
      "epoch 107: loss 0.2627905011177063\n",
      "epoch 108: loss 0.34131336212158203\n",
      "epoch 109: loss 0.345406174659729\n",
      "epoch 110: loss 0.33990564942359924\n",
      "epoch 111: loss 0.3495335578918457\n",
      "epoch 112: loss 0.36836493015289307\n",
      "epoch 113: loss 0.27900487184524536\n",
      "epoch 114: loss 0.15347075462341309\n",
      "epoch 115: loss 0.4009571969509125\n",
      "epoch 116: loss 0.28507131338119507\n",
      "epoch 117: loss 0.2500356435775757\n",
      "epoch 118: loss 0.31327059864997864\n",
      "epoch 119: loss 0.3877515494823456\n",
      "epoch 120: loss 0.25821128487586975\n",
      "epoch 121: loss 0.3603716790676117\n",
      "epoch 122: loss 0.2349482774734497\n",
      "epoch 123: loss 0.3673640787601471\n",
      "epoch 124: loss 0.3138052821159363\n",
      "epoch 125: loss 0.33852332830429077\n",
      "epoch 126: loss 0.3722044825553894\n",
      "epoch 127: loss 0.41840970516204834\n",
      "epoch 128: loss 0.2847444415092468\n",
      "epoch 129: loss 0.33522510528564453\n",
      "epoch 130: loss 0.4148063361644745\n",
      "epoch 131: loss 0.3459678292274475\n",
      "epoch 132: loss 0.36027640104293823\n",
      "epoch 133: loss 0.38235166668891907\n",
      "epoch 134: loss 0.319943368434906\n",
      "epoch 135: loss 0.2630280554294586\n",
      "epoch 136: loss 0.25239691138267517\n",
      "epoch 0: loss 0.3482077121734619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 0.32296234369277954\n",
      "epoch 2: loss 0.35152629017829895\n",
      "epoch 3: loss 0.36459028720855713\n",
      "epoch 4: loss 0.30557700991630554\n",
      "epoch 5: loss 0.25554460287094116\n",
      "epoch 6: loss 0.2698517143726349\n",
      "epoch 7: loss 0.22054193913936615\n",
      "epoch 8: loss 0.39907217025756836\n",
      "epoch 9: loss 0.3956480622291565\n",
      "epoch 10: loss 0.34374213218688965\n",
      "epoch 11: loss 0.2792333662509918\n",
      "epoch 12: loss 0.29628896713256836\n",
      "epoch 13: loss 0.42732012271881104\n",
      "epoch 14: loss 0.3366473317146301\n",
      "epoch 15: loss 0.2732558250427246\n",
      "epoch 16: loss 0.34381186962127686\n",
      "epoch 17: loss 0.3293374478816986\n",
      "epoch 18: loss 0.3608532249927521\n",
      "epoch 19: loss 0.2757530212402344\n",
      "epoch 20: loss 0.29677796363830566\n",
      "epoch 21: loss 0.3216992914676666\n",
      "epoch 22: loss 0.308915376663208\n",
      "epoch 23: loss 0.3642447292804718\n",
      "epoch 24: loss 0.3524776101112366\n",
      "epoch 25: loss 0.37439078092575073\n",
      "epoch 26: loss 0.33270764350891113\n",
      "epoch 27: loss 0.32972824573516846\n",
      "epoch 28: loss 0.3844940960407257\n",
      "epoch 29: loss 0.3084883689880371\n",
      "epoch 30: loss 0.2816201448440552\n",
      "epoch 31: loss 0.3278346657752991\n",
      "epoch 32: loss 0.3353143036365509\n",
      "epoch 33: loss 0.33300232887268066\n",
      "epoch 34: loss 0.377869188785553\n",
      "epoch 35: loss 0.26813438534736633\n",
      "epoch 36: loss 0.32401949167251587\n",
      "epoch 37: loss 0.33793383836746216\n",
      "epoch 38: loss 0.2334291636943817\n",
      "epoch 39: loss 0.32446223497390747\n",
      "epoch 40: loss 0.3267199993133545\n",
      "epoch 41: loss 0.2878916561603546\n",
      "epoch 42: loss 0.24002772569656372\n",
      "epoch 43: loss 0.32217878103256226\n",
      "epoch 44: loss 0.336698055267334\n",
      "epoch 45: loss 0.2900708019733429\n",
      "epoch 46: loss 0.27670788764953613\n",
      "epoch 47: loss 0.2816457748413086\n",
      "epoch 48: loss 0.23168650269508362\n",
      "epoch 49: loss 0.3140825629234314\n",
      "epoch 50: loss 0.28102853894233704\n",
      "epoch 51: loss 0.41021138429641724\n",
      "epoch 52: loss 0.3199350833892822\n",
      "epoch 53: loss 0.35706835985183716\n",
      "epoch 54: loss 0.33173590898513794\n",
      "epoch 55: loss 0.30935609340667725\n",
      "epoch 56: loss 0.33768072724342346\n",
      "epoch 57: loss 0.2506582736968994\n",
      "epoch 58: loss 0.3238050937652588\n",
      "epoch 59: loss 0.2608901858329773\n",
      "epoch 60: loss 0.32525092363357544\n",
      "epoch 61: loss 0.21996775269508362\n",
      "epoch 62: loss 0.23736262321472168\n",
      "epoch 63: loss 0.32903337478637695\n",
      "epoch 64: loss 0.2772352695465088\n",
      "epoch 65: loss 0.3760197162628174\n",
      "epoch 66: loss 0.3277660012245178\n",
      "epoch 67: loss 0.38477104902267456\n",
      "epoch 68: loss 0.3511291742324829\n",
      "epoch 69: loss 0.40639251470565796\n",
      "epoch 70: loss 0.3580484390258789\n",
      "epoch 71: loss 0.3139688968658447\n",
      "epoch 72: loss 0.25218236446380615\n",
      "epoch 73: loss 0.3106989860534668\n",
      "epoch 74: loss 0.24326619505882263\n",
      "epoch 75: loss 0.35071009397506714\n",
      "epoch 76: loss 0.3215641975402832\n",
      "epoch 77: loss 0.39525389671325684\n",
      "epoch 78: loss 0.3130555748939514\n",
      "epoch 79: loss 0.30915525555610657\n",
      "epoch 80: loss 0.3746086359024048\n",
      "epoch 81: loss 0.26611024141311646\n",
      "epoch 82: loss 0.31966128945350647\n",
      "epoch 83: loss 0.36980149149894714\n",
      "epoch 84: loss 0.3048158884048462\n",
      "epoch 85: loss 0.29291391372680664\n",
      "epoch 86: loss 0.33031463623046875\n",
      "epoch 87: loss 0.3355879783630371\n",
      "epoch 88: loss 0.2695448100566864\n",
      "epoch 89: loss 0.3420560657978058\n",
      "epoch 90: loss 0.29614999890327454\n",
      "epoch 91: loss 0.38811036944389343\n",
      "epoch 92: loss 0.3781762719154358\n",
      "epoch 93: loss 0.302431583404541\n",
      "epoch 94: loss 0.3026070296764374\n",
      "epoch 95: loss 0.1755121946334839\n",
      "epoch 96: loss 0.22177338600158691\n",
      "epoch 97: loss 0.2984112501144409\n",
      "epoch 98: loss 0.3392781615257263\n",
      "epoch 99: loss 0.3190855383872986\n",
      "epoch 100: loss 0.45600977540016174\n",
      "epoch 101: loss 0.29707425832748413\n",
      "epoch 102: loss 0.2471996396780014\n",
      "epoch 103: loss 0.3077848553657532\n",
      "epoch 104: loss 0.2549118995666504\n",
      "epoch 105: loss 0.35662439465522766\n",
      "epoch 106: loss 0.30416297912597656\n",
      "epoch 107: loss 0.2652111351490021\n",
      "epoch 108: loss 0.3324054777622223\n",
      "epoch 109: loss 0.3439330458641052\n",
      "epoch 110: loss 0.3349222242832184\n",
      "epoch 111: loss 0.3523220419883728\n",
      "epoch 112: loss 0.3639482855796814\n",
      "epoch 113: loss 0.27786919474601746\n",
      "epoch 114: loss 0.1488552689552307\n",
      "epoch 115: loss 0.39136067032814026\n",
      "epoch 116: loss 0.2789424657821655\n",
      "epoch 117: loss 0.24524065852165222\n",
      "epoch 118: loss 0.3123045861721039\n",
      "epoch 119: loss 0.3955443501472473\n",
      "epoch 120: loss 0.2560550570487976\n",
      "epoch 121: loss 0.3546749949455261\n",
      "epoch 122: loss 0.23150485754013062\n",
      "epoch 123: loss 0.37315383553504944\n",
      "epoch 124: loss 0.3152003884315491\n",
      "epoch 125: loss 0.3439849317073822\n",
      "epoch 126: loss 0.37632131576538086\n",
      "epoch 127: loss 0.41185906529426575\n",
      "epoch 128: loss 0.2753521800041199\n",
      "epoch 129: loss 0.3458560109138489\n",
      "epoch 130: loss 0.37673819065093994\n",
      "epoch 131: loss 0.3226469159126282\n",
      "epoch 132: loss 0.3119526207447052\n",
      "epoch 133: loss 0.3524877727031708\n",
      "epoch 134: loss 0.31804054975509644\n",
      "epoch 135: loss 0.25210538506507874\n",
      "epoch 136: loss 0.250724732875824\n",
      "epoch 0: loss 0.34026002883911133\n",
      "epoch 1: loss 0.34490957856178284\n",
      "epoch 2: loss 0.3514270782470703\n",
      "epoch 3: loss 0.3415927588939667\n",
      "epoch 4: loss 0.2923852205276489\n",
      "epoch 5: loss 0.25193625688552856\n",
      "epoch 6: loss 0.2673712372779846\n",
      "epoch 7: loss 0.2187374234199524\n",
      "epoch 8: loss 0.3935796022415161\n",
      "epoch 9: loss 0.39302384853363037\n",
      "epoch 10: loss 0.3270856738090515\n",
      "epoch 11: loss 0.2706795334815979\n",
      "epoch 12: loss 0.29529839754104614\n",
      "epoch 13: loss 0.4072250425815582\n",
      "epoch 14: loss 0.33911705017089844\n",
      "epoch 15: loss 0.2712062895298004\n",
      "epoch 16: loss 0.3375881314277649\n",
      "epoch 17: loss 0.32385414838790894\n",
      "epoch 18: loss 0.3716745376586914\n",
      "epoch 19: loss 0.2657085061073303\n",
      "epoch 20: loss 0.2962695360183716\n",
      "epoch 21: loss 0.32423973083496094\n",
      "epoch 22: loss 0.29187530279159546\n",
      "epoch 23: loss 0.35385948419570923\n",
      "epoch 24: loss 0.3227245807647705\n",
      "epoch 25: loss 0.38592809438705444\n",
      "epoch 26: loss 0.32767581939697266\n",
      "epoch 27: loss 0.33742374181747437\n",
      "epoch 28: loss 0.3931787610054016\n",
      "epoch 29: loss 0.3149695098400116\n",
      "epoch 30: loss 0.2927325367927551\n",
      "epoch 31: loss 0.3277839422225952\n",
      "epoch 32: loss 0.3375551998615265\n",
      "epoch 33: loss 0.3364371657371521\n",
      "epoch 34: loss 0.3748377561569214\n",
      "epoch 35: loss 0.2628973424434662\n",
      "epoch 36: loss 0.32446253299713135\n",
      "epoch 37: loss 0.3180205821990967\n",
      "epoch 38: loss 0.21922127902507782\n",
      "epoch 39: loss 0.2869330644607544\n",
      "epoch 40: loss 0.28575876355171204\n",
      "epoch 41: loss 0.27934473752975464\n",
      "epoch 42: loss 0.2516710162162781\n",
      "epoch 43: loss 0.32262545824050903\n",
      "epoch 44: loss 0.3175063133239746\n",
      "epoch 45: loss 0.2867377996444702\n",
      "epoch 46: loss 0.2863452732563019\n",
      "epoch 47: loss 0.2824416756629944\n",
      "epoch 48: loss 0.2295490950345993\n",
      "epoch 49: loss 0.3208494186401367\n",
      "epoch 50: loss 0.2789275348186493\n",
      "epoch 51: loss 0.38648009300231934\n",
      "epoch 52: loss 0.26738932728767395\n",
      "epoch 53: loss 0.3466973900794983\n",
      "epoch 54: loss 0.34650230407714844\n",
      "epoch 55: loss 0.2929939031600952\n",
      "epoch 56: loss 0.34127217531204224\n",
      "epoch 57: loss 0.2394164502620697\n",
      "epoch 58: loss 0.3159032464027405\n",
      "epoch 59: loss 0.2604562044143677\n",
      "epoch 60: loss 0.3137543797492981\n",
      "epoch 61: loss 0.2104952037334442\n",
      "epoch 62: loss 0.23700958490371704\n",
      "epoch 63: loss 0.3267918825149536\n",
      "epoch 64: loss 0.274589478969574\n",
      "epoch 65: loss 0.3789340853691101\n",
      "epoch 66: loss 0.32200801372528076\n",
      "epoch 67: loss 0.3838050067424774\n",
      "epoch 68: loss 0.35048961639404297\n",
      "epoch 69: loss 0.4114551246166229\n",
      "epoch 70: loss 0.3631250858306885\n",
      "epoch 71: loss 0.3138312101364136\n",
      "epoch 72: loss 0.24288520216941833\n",
      "epoch 73: loss 0.3032612204551697\n",
      "epoch 74: loss 0.24868565797805786\n",
      "epoch 75: loss 0.3520873486995697\n",
      "epoch 76: loss 0.3111839294433594\n",
      "epoch 77: loss 0.3991541564464569\n",
      "epoch 78: loss 0.30518409609794617\n",
      "epoch 79: loss 0.3100818395614624\n",
      "epoch 80: loss 0.3732616603374481\n",
      "epoch 81: loss 0.272228479385376\n",
      "epoch 82: loss 0.3237449526786804\n",
      "epoch 83: loss 0.36217471957206726\n",
      "epoch 84: loss 0.3220600485801697\n",
      "epoch 85: loss 0.2896926999092102\n",
      "epoch 86: loss 0.33952897787094116\n",
      "epoch 87: loss 0.32319462299346924\n",
      "epoch 88: loss 0.30573970079421997\n",
      "epoch 89: loss 0.38943350315093994\n",
      "epoch 90: loss 0.33627045154571533\n",
      "epoch 91: loss 0.44853657484054565\n",
      "epoch 92: loss 0.4046521782875061\n",
      "epoch 93: loss 0.3032386004924774\n",
      "epoch 94: loss 0.32056695222854614\n",
      "epoch 95: loss 0.16883812844753265\n",
      "epoch 96: loss 0.2260686457157135\n",
      "epoch 97: loss 0.31659674644470215\n",
      "epoch 98: loss 0.3629506826400757\n",
      "epoch 99: loss 0.3376527428627014\n",
      "epoch 100: loss 0.4899255037307739\n",
      "epoch 101: loss 0.31036436557769775\n",
      "epoch 102: loss 0.25221437215805054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 103: loss 0.30504563450813293\n",
      "epoch 104: loss 0.24410361051559448\n",
      "epoch 105: loss 0.35815900564193726\n",
      "epoch 106: loss 0.30919474363327026\n",
      "epoch 107: loss 0.2652505040168762\n",
      "epoch 108: loss 0.3300447463989258\n",
      "epoch 109: loss 0.34995609521865845\n",
      "epoch 110: loss 0.33124837279319763\n",
      "epoch 111: loss 0.3607313334941864\n",
      "epoch 112: loss 0.36932867765426636\n",
      "epoch 113: loss 0.2732003331184387\n",
      "epoch 114: loss 0.15845754742622375\n",
      "epoch 115: loss 0.39883601665496826\n",
      "epoch 116: loss 0.28438419103622437\n",
      "epoch 117: loss 0.24719329178333282\n",
      "epoch 118: loss 0.3145916163921356\n",
      "epoch 119: loss 0.38544750213623047\n",
      "epoch 120: loss 0.25857073068618774\n",
      "epoch 121: loss 0.35865849256515503\n",
      "epoch 122: loss 0.23434431850910187\n",
      "epoch 123: loss 0.36927223205566406\n",
      "epoch 124: loss 0.2985554337501526\n",
      "epoch 125: loss 0.3434601426124573\n",
      "epoch 126: loss 0.35790497064590454\n",
      "epoch 127: loss 0.376490980386734\n",
      "epoch 128: loss 0.24875129759311676\n",
      "epoch 129: loss 0.3380429148674011\n",
      "epoch 130: loss 0.3533017039299011\n",
      "epoch 131: loss 0.3140242099761963\n",
      "epoch 132: loss 0.3088044226169586\n",
      "epoch 133: loss 0.3388299345970154\n",
      "epoch 134: loss 0.297766774892807\n",
      "epoch 135: loss 0.24502752721309662\n",
      "epoch 136: loss 0.21473413705825806\n",
      "epoch 0: loss 0.31927624344825745\n",
      "epoch 1: loss 0.3295729160308838\n",
      "epoch 2: loss 0.3595306873321533\n",
      "epoch 3: loss 0.3418015241622925\n",
      "epoch 4: loss 0.2922561466693878\n",
      "epoch 5: loss 0.2371019423007965\n",
      "epoch 6: loss 0.26404863595962524\n",
      "epoch 7: loss 0.21784472465515137\n",
      "epoch 8: loss 0.4130653440952301\n",
      "epoch 9: loss 0.4032190442085266\n",
      "epoch 10: loss 0.32149338722229004\n",
      "epoch 11: loss 0.26389795541763306\n",
      "epoch 12: loss 0.3003385066986084\n",
      "epoch 13: loss 0.40585431456565857\n",
      "epoch 14: loss 0.36301207542419434\n",
      "epoch 15: loss 0.2596714496612549\n",
      "epoch 16: loss 0.33940190076828003\n",
      "epoch 17: loss 0.35172754526138306\n",
      "epoch 18: loss 0.41486024856567383\n",
      "epoch 19: loss 0.27318888902664185\n",
      "epoch 20: loss 0.3207206130027771\n",
      "epoch 21: loss 0.3327612280845642\n",
      "epoch 22: loss 0.31831905245780945\n",
      "epoch 23: loss 0.38825905323028564\n",
      "epoch 24: loss 0.355437308549881\n",
      "epoch 25: loss 0.4066898822784424\n",
      "epoch 26: loss 0.31559112668037415\n",
      "epoch 27: loss 0.3377305567264557\n",
      "epoch 28: loss 0.3835306167602539\n",
      "epoch 29: loss 0.31189608573913574\n",
      "epoch 30: loss 0.28018835186958313\n",
      "epoch 31: loss 0.346097469329834\n",
      "epoch 32: loss 0.33393627405166626\n",
      "epoch 33: loss 0.3627971410751343\n",
      "epoch 34: loss 0.39398014545440674\n",
      "epoch 35: loss 0.25175631046295166\n",
      "epoch 36: loss 0.3385719358921051\n",
      "epoch 37: loss 0.30672091245651245\n",
      "epoch 38: loss 0.2087542861700058\n",
      "epoch 39: loss 0.29138901829719543\n",
      "epoch 40: loss 0.2923211455345154\n",
      "epoch 41: loss 0.27367210388183594\n",
      "epoch 42: loss 0.25364920496940613\n",
      "epoch 43: loss 0.32123008370399475\n",
      "epoch 44: loss 0.32829493284225464\n",
      "epoch 45: loss 0.2843676209449768\n",
      "epoch 46: loss 0.2725277543067932\n",
      "epoch 47: loss 0.27776777744293213\n",
      "epoch 48: loss 0.24362054467201233\n",
      "epoch 49: loss 0.3275192975997925\n",
      "epoch 50: loss 0.27276551723480225\n",
      "epoch 51: loss 0.3867013156414032\n",
      "epoch 52: loss 0.2635538876056671\n",
      "epoch 53: loss 0.3496946692466736\n",
      "epoch 54: loss 0.3418300747871399\n",
      "epoch 55: loss 0.295539915561676\n",
      "epoch 56: loss 0.34500300884246826\n",
      "epoch 57: loss 0.24329400062561035\n",
      "epoch 58: loss 0.3214520812034607\n",
      "epoch 59: loss 0.2623213529586792\n",
      "epoch 60: loss 0.3178775906562805\n",
      "epoch 61: loss 0.20788997411727905\n",
      "epoch 62: loss 0.23323112726211548\n",
      "epoch 63: loss 0.3274843096733093\n",
      "epoch 64: loss 0.27843788266181946\n",
      "epoch 65: loss 0.37953025102615356\n",
      "epoch 66: loss 0.3231997787952423\n",
      "epoch 67: loss 0.3810574412345886\n",
      "epoch 68: loss 0.35400140285491943\n",
      "epoch 69: loss 0.41052156686782837\n",
      "epoch 70: loss 0.36945703625679016\n",
      "epoch 71: loss 0.3054903745651245\n",
      "epoch 72: loss 0.24759191274642944\n",
      "epoch 73: loss 0.31312912702560425\n",
      "epoch 74: loss 0.25418010354042053\n",
      "epoch 75: loss 0.34283512830734253\n",
      "epoch 76: loss 0.3054789900779724\n",
      "epoch 77: loss 0.39657101035118103\n",
      "epoch 78: loss 0.3095511198043823\n",
      "epoch 79: loss 0.31024837493896484\n",
      "epoch 80: loss 0.36661630868911743\n",
      "epoch 81: loss 0.2689642906188965\n",
      "epoch 82: loss 0.3140661418437958\n",
      "epoch 83: loss 0.36400294303894043\n",
      "epoch 84: loss 0.27726882696151733\n",
      "epoch 85: loss 0.29175865650177\n",
      "epoch 86: loss 0.3357113003730774\n",
      "epoch 87: loss 0.3321065902709961\n",
      "epoch 88: loss 0.28232505917549133\n",
      "epoch 89: loss 0.3634384870529175\n",
      "epoch 90: loss 0.3250737488269806\n",
      "epoch 91: loss 0.42890071868896484\n",
      "epoch 92: loss 0.38649892807006836\n",
      "epoch 93: loss 0.30412203073501587\n",
      "epoch 94: loss 0.2968823313713074\n",
      "epoch 95: loss 0.18099546432495117\n",
      "epoch 96: loss 0.22675931453704834\n",
      "epoch 97: loss 0.3148179054260254\n",
      "epoch 98: loss 0.3363901376724243\n",
      "epoch 99: loss 0.3214702606201172\n",
      "epoch 100: loss 0.46785637736320496\n",
      "epoch 101: loss 0.2891143262386322\n",
      "epoch 102: loss 0.24136167764663696\n",
      "epoch 103: loss 0.2987731397151947\n",
      "epoch 104: loss 0.2476864755153656\n",
      "epoch 105: loss 0.36064594984054565\n",
      "epoch 106: loss 0.3103142976760864\n",
      "epoch 107: loss 0.26606348156929016\n",
      "epoch 108: loss 0.3378996253013611\n",
      "epoch 109: loss 0.3466048836708069\n",
      "epoch 110: loss 0.33947792649269104\n",
      "epoch 111: loss 0.3500107526779175\n",
      "epoch 112: loss 0.37671783566474915\n",
      "epoch 113: loss 0.2742064893245697\n",
      "epoch 114: loss 0.15257273614406586\n",
      "epoch 115: loss 0.4110935926437378\n",
      "epoch 116: loss 0.28991156816482544\n",
      "epoch 117: loss 0.2529977858066559\n",
      "epoch 118: loss 0.31406262516975403\n",
      "epoch 119: loss 0.38987547159194946\n",
      "epoch 120: loss 0.2676205039024353\n",
      "epoch 121: loss 0.3643532395362854\n",
      "epoch 122: loss 0.23204179108142853\n",
      "epoch 123: loss 0.3695836663246155\n",
      "epoch 124: loss 0.35377705097198486\n",
      "epoch 125: loss 0.33538058400154114\n",
      "epoch 126: loss 0.3757031559944153\n",
      "epoch 127: loss 0.4214041829109192\n",
      "epoch 128: loss 0.3142920732498169\n",
      "epoch 129: loss 0.3584253787994385\n",
      "epoch 130: loss 0.4503098130226135\n",
      "epoch 131: loss 0.37032973766326904\n",
      "epoch 132: loss 0.38746631145477295\n",
      "epoch 133: loss 0.4091302752494812\n",
      "epoch 134: loss 0.3405560851097107\n",
      "epoch 135: loss 0.2992055416107178\n",
      "epoch 136: loss 0.27564018964767456\n",
      "epoch 0: loss 0.37426427006721497\n",
      "epoch 1: loss 0.3271675109863281\n",
      "epoch 2: loss 0.39464107155799866\n",
      "epoch 3: loss 0.3578847348690033\n",
      "epoch 4: loss 0.3506605327129364\n",
      "epoch 5: loss 0.26785141229629517\n",
      "epoch 6: loss 0.2690805494785309\n",
      "epoch 7: loss 0.2158569097518921\n",
      "epoch 8: loss 0.43329477310180664\n",
      "epoch 9: loss 0.45814836025238037\n",
      "epoch 10: loss 0.3685981035232544\n",
      "epoch 11: loss 0.2945297360420227\n",
      "epoch 12: loss 0.3199194073677063\n",
      "epoch 13: loss 0.4158290922641754\n",
      "epoch 14: loss 0.395175576210022\n",
      "epoch 15: loss 0.28452301025390625\n",
      "epoch 16: loss 0.3530128002166748\n",
      "epoch 17: loss 0.3355409801006317\n",
      "epoch 18: loss 0.3967238664627075\n",
      "epoch 19: loss 0.26424193382263184\n",
      "epoch 20: loss 0.3064340353012085\n",
      "epoch 21: loss 0.34012123942375183\n",
      "epoch 22: loss 0.3149513602256775\n",
      "epoch 23: loss 0.3652072250843048\n",
      "epoch 24: loss 0.33563730120658875\n",
      "epoch 25: loss 0.40231478214263916\n",
      "epoch 26: loss 0.3472667634487152\n",
      "epoch 27: loss 0.3333587348461151\n",
      "epoch 28: loss 0.40260982513427734\n",
      "epoch 29: loss 0.310124933719635\n",
      "epoch 30: loss 0.28600025177001953\n",
      "epoch 31: loss 0.3303826153278351\n",
      "epoch 32: loss 0.3449455499649048\n",
      "epoch 33: loss 0.3344777822494507\n",
      "epoch 34: loss 0.38179340958595276\n",
      "epoch 35: loss 0.26327401399612427\n",
      "epoch 36: loss 0.3288212716579437\n",
      "epoch 37: loss 0.32401424646377563\n",
      "epoch 38: loss 0.21939164400100708\n",
      "epoch 39: loss 0.30994564294815063\n",
      "epoch 40: loss 0.30343401432037354\n",
      "epoch 41: loss 0.2801184058189392\n",
      "epoch 42: loss 0.25527462363243103\n",
      "epoch 43: loss 0.32671111822128296\n",
      "epoch 44: loss 0.3332514762878418\n",
      "epoch 45: loss 0.280647873878479\n",
      "epoch 46: loss 0.28659921884536743\n",
      "epoch 47: loss 0.2870027422904968\n",
      "epoch 48: loss 0.2458932101726532\n",
      "epoch 49: loss 0.32087060809135437\n",
      "epoch 50: loss 0.28064799308776855\n",
      "epoch 51: loss 0.40073806047439575\n",
      "epoch 52: loss 0.3092733919620514\n",
      "epoch 53: loss 0.3525945842266083\n",
      "epoch 54: loss 0.3399273455142975\n",
      "epoch 55: loss 0.3015756905078888\n",
      "epoch 56: loss 0.3452787399291992\n",
      "epoch 57: loss 0.24622008204460144\n",
      "epoch 58: loss 0.3293783962726593\n",
      "epoch 59: loss 0.2564878761768341\n",
      "epoch 60: loss 0.32869401574134827\n",
      "epoch 61: loss 0.21300002932548523\n",
      "epoch 62: loss 0.23515045642852783\n",
      "epoch 63: loss 0.32747337222099304\n",
      "epoch 64: loss 0.2805846929550171\n",
      "epoch 65: loss 0.3847683072090149\n",
      "epoch 66: loss 0.3291209042072296\n",
      "epoch 67: loss 0.36876654624938965\n",
      "epoch 68: loss 0.35390397906303406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 69: loss 0.4086812138557434\n",
      "epoch 70: loss 0.3643783926963806\n",
      "epoch 71: loss 0.31585872173309326\n",
      "epoch 72: loss 0.2560110092163086\n",
      "epoch 73: loss 0.3149998188018799\n",
      "epoch 74: loss 0.2546192407608032\n",
      "epoch 75: loss 0.3431563973426819\n",
      "epoch 76: loss 0.3113650381565094\n",
      "epoch 77: loss 0.39898261427879333\n",
      "epoch 78: loss 0.3086600601673126\n",
      "epoch 79: loss 0.3231874704360962\n",
      "epoch 80: loss 0.37400349974632263\n",
      "epoch 81: loss 0.2689087688922882\n",
      "epoch 82: loss 0.3144272565841675\n",
      "epoch 83: loss 0.3693005442619324\n",
      "epoch 84: loss 0.3047403395175934\n",
      "epoch 85: loss 0.29434841871261597\n",
      "epoch 86: loss 0.3387612998485565\n",
      "epoch 87: loss 0.332959920167923\n",
      "epoch 88: loss 0.2825661599636078\n",
      "epoch 89: loss 0.3751550316810608\n",
      "epoch 90: loss 0.32447540760040283\n",
      "epoch 91: loss 0.4452253580093384\n",
      "epoch 92: loss 0.3778028190135956\n",
      "epoch 93: loss 0.3107646405696869\n",
      "epoch 94: loss 0.2983131408691406\n",
      "epoch 95: loss 0.21673265099525452\n",
      "epoch 96: loss 0.23514708876609802\n",
      "epoch 97: loss 0.30761948227882385\n",
      "epoch 98: loss 0.34002575278282166\n",
      "epoch 99: loss 0.3237024247646332\n",
      "epoch 100: loss 0.45177650451660156\n",
      "epoch 101: loss 0.30525311827659607\n",
      "epoch 102: loss 0.2533609867095947\n",
      "epoch 103: loss 0.29872602224349976\n",
      "epoch 104: loss 0.2514355182647705\n",
      "epoch 105: loss 0.37544405460357666\n",
      "epoch 106: loss 0.32497814297676086\n",
      "epoch 107: loss 0.27875182032585144\n",
      "epoch 108: loss 0.35193932056427\n",
      "epoch 109: loss 0.3474772572517395\n",
      "epoch 110: loss 0.35877305269241333\n",
      "epoch 111: loss 0.3503127992153168\n",
      "epoch 112: loss 0.3611207902431488\n",
      "epoch 113: loss 0.2875409722328186\n",
      "epoch 114: loss 0.15875345468521118\n",
      "epoch 115: loss 0.3837948143482208\n",
      "epoch 116: loss 0.2826414108276367\n",
      "epoch 117: loss 0.25345316529273987\n",
      "epoch 118: loss 0.31350383162498474\n",
      "epoch 119: loss 0.37955284118652344\n",
      "epoch 120: loss 0.26821786165237427\n",
      "epoch 121: loss 0.38014429807662964\n",
      "epoch 122: loss 0.2471877783536911\n",
      "epoch 123: loss 0.3875156342983246\n",
      "epoch 124: loss 0.2821473777294159\n",
      "epoch 125: loss 0.32831063866615295\n",
      "epoch 126: loss 0.33308592438697815\n",
      "epoch 127: loss 0.37018638849258423\n",
      "epoch 128: loss 0.26140862703323364\n",
      "epoch 129: loss 0.3316653370857239\n",
      "epoch 130: loss 0.3539765477180481\n",
      "epoch 131: loss 0.3213356137275696\n",
      "epoch 132: loss 0.3136214017868042\n",
      "epoch 133: loss 0.36022114753723145\n",
      "epoch 134: loss 0.3161611557006836\n",
      "epoch 135: loss 0.248149573802948\n",
      "epoch 136: loss 0.23263749480247498\n",
      "epoch 0: loss 0.3512723445892334\n",
      "epoch 1: loss 0.312883585691452\n",
      "epoch 2: loss 0.37300121784210205\n",
      "epoch 3: loss 0.3442530035972595\n",
      "epoch 4: loss 0.34805595874786377\n",
      "epoch 5: loss 0.25208449363708496\n",
      "epoch 6: loss 0.28542986512184143\n",
      "epoch 7: loss 0.24053356051445007\n",
      "epoch 8: loss 0.4488856792449951\n",
      "epoch 9: loss 0.45114248991012573\n",
      "epoch 10: loss 0.3940298855304718\n",
      "epoch 11: loss 0.32048481702804565\n",
      "epoch 12: loss 0.3051965534687042\n",
      "epoch 13: loss 0.44986632466316223\n",
      "epoch 14: loss 0.33129724860191345\n",
      "epoch 15: loss 0.27917906641960144\n",
      "epoch 16: loss 0.35962557792663574\n",
      "epoch 17: loss 0.3454904556274414\n",
      "epoch 18: loss 0.36210596561431885\n",
      "epoch 19: loss 0.283531129360199\n",
      "epoch 20: loss 0.30589941143989563\n",
      "epoch 21: loss 0.3249362111091614\n",
      "epoch 22: loss 0.31384629011154175\n",
      "epoch 23: loss 0.3643494248390198\n",
      "epoch 24: loss 0.3590990900993347\n",
      "epoch 25: loss 0.3794233500957489\n",
      "epoch 26: loss 0.29727989435195923\n",
      "epoch 27: loss 0.34497174620628357\n",
      "epoch 28: loss 0.3863707184791565\n",
      "epoch 29: loss 0.31537872552871704\n",
      "epoch 30: loss 0.2706625759601593\n",
      "epoch 31: loss 0.33615636825561523\n",
      "epoch 32: loss 0.331210196018219\n",
      "epoch 33: loss 0.3512943983078003\n",
      "epoch 34: loss 0.37358975410461426\n",
      "epoch 35: loss 0.26519232988357544\n",
      "epoch 36: loss 0.3379575312137604\n",
      "epoch 37: loss 0.3042452335357666\n",
      "epoch 38: loss 0.21329589188098907\n",
      "epoch 39: loss 0.2821080982685089\n",
      "epoch 40: loss 0.2902827858924866\n",
      "epoch 41: loss 0.2834540009498596\n",
      "epoch 42: loss 0.23133866488933563\n",
      "epoch 43: loss 0.3141428828239441\n",
      "epoch 44: loss 0.32585954666137695\n",
      "epoch 45: loss 0.2865307927131653\n",
      "epoch 46: loss 0.26925021409988403\n",
      "epoch 47: loss 0.27372485399246216\n",
      "epoch 48: loss 0.2607603073120117\n",
      "epoch 49: loss 0.3461851477622986\n",
      "epoch 50: loss 0.2757047116756439\n",
      "epoch 51: loss 0.3975992500782013\n",
      "epoch 52: loss 0.2766975164413452\n",
      "epoch 53: loss 0.3583856523036957\n",
      "epoch 54: loss 0.3373550772666931\n",
      "epoch 55: loss 0.30661332607269287\n",
      "epoch 56: loss 0.33667880296707153\n",
      "epoch 57: loss 0.237122043967247\n",
      "epoch 58: loss 0.31701570749282837\n",
      "epoch 59: loss 0.2636392116546631\n",
      "epoch 60: loss 0.31398388743400574\n",
      "epoch 61: loss 0.21288558840751648\n",
      "epoch 62: loss 0.23651748895645142\n",
      "epoch 63: loss 0.32758474349975586\n",
      "epoch 64: loss 0.27059829235076904\n",
      "epoch 65: loss 0.38868850469589233\n",
      "epoch 66: loss 0.32481086254119873\n",
      "epoch 67: loss 0.38119906187057495\n",
      "epoch 68: loss 0.3562731146812439\n",
      "epoch 69: loss 0.4343477487564087\n",
      "epoch 70: loss 0.38001346588134766\n",
      "epoch 71: loss 0.3005245327949524\n",
      "epoch 72: loss 0.24347837269306183\n",
      "epoch 73: loss 0.3066527843475342\n",
      "epoch 74: loss 0.26380661129951477\n",
      "epoch 75: loss 0.3283751606941223\n",
      "epoch 76: loss 0.31591787934303284\n",
      "epoch 77: loss 0.3806275725364685\n",
      "epoch 78: loss 0.3381887376308441\n",
      "epoch 79: loss 0.31734177470207214\n",
      "epoch 80: loss 0.36056020855903625\n",
      "epoch 81: loss 0.2703838348388672\n",
      "epoch 82: loss 0.3356219530105591\n",
      "epoch 83: loss 0.3848939538002014\n",
      "epoch 84: loss 0.23711687326431274\n",
      "epoch 85: loss 0.29694461822509766\n",
      "epoch 86: loss 0.33714592456817627\n",
      "epoch 87: loss 0.32684630155563354\n",
      "epoch 88: loss 0.2802368104457855\n",
      "epoch 89: loss 0.29306378960609436\n",
      "epoch 90: loss 0.2450067400932312\n",
      "epoch 91: loss 0.3039798140525818\n",
      "epoch 92: loss 0.24926692247390747\n",
      "epoch 93: loss 0.30123403668403625\n",
      "epoch 94: loss 0.29520177841186523\n",
      "epoch 95: loss 0.1770491600036621\n",
      "epoch 96: loss 0.19958476722240448\n",
      "epoch 97: loss 0.2820969223976135\n",
      "epoch 98: loss 0.32904547452926636\n",
      "epoch 99: loss 0.3514426350593567\n",
      "epoch 100: loss 0.48279064893722534\n",
      "epoch 101: loss 0.3059577941894531\n",
      "epoch 102: loss 0.2460513710975647\n",
      "epoch 103: loss 0.3018086552619934\n",
      "epoch 104: loss 0.24116596579551697\n",
      "epoch 105: loss 0.3578503429889679\n",
      "epoch 106: loss 0.30714425444602966\n",
      "epoch 107: loss 0.2734444737434387\n",
      "epoch 108: loss 0.3396245539188385\n",
      "epoch 109: loss 0.3447219729423523\n",
      "epoch 110: loss 0.33670762181282043\n",
      "epoch 111: loss 0.3457384705543518\n",
      "epoch 112: loss 0.3602128028869629\n",
      "epoch 113: loss 0.2676658034324646\n",
      "epoch 114: loss 0.15355587005615234\n",
      "epoch 115: loss 0.3864823281764984\n",
      "epoch 116: loss 0.26943954825401306\n",
      "epoch 117: loss 0.24816885590553284\n",
      "epoch 118: loss 0.312427818775177\n",
      "epoch 119: loss 0.39705920219421387\n",
      "epoch 120: loss 0.2560994029045105\n",
      "epoch 121: loss 0.3653775751590729\n",
      "epoch 122: loss 0.22861802577972412\n",
      "epoch 123: loss 0.3794175386428833\n",
      "epoch 124: loss 0.3322644829750061\n",
      "epoch 125: loss 0.3432224690914154\n",
      "epoch 126: loss 0.3797963857650757\n",
      "epoch 127: loss 0.41289669275283813\n",
      "epoch 128: loss 0.2909538447856903\n",
      "epoch 129: loss 0.34353965520858765\n",
      "epoch 130: loss 0.40827202796936035\n",
      "epoch 131: loss 0.34455615282058716\n",
      "epoch 132: loss 0.35513782501220703\n",
      "epoch 133: loss 0.3865712881088257\n",
      "epoch 134: loss 0.3232373595237732\n",
      "epoch 135: loss 0.26823318004608154\n",
      "epoch 136: loss 0.2533814311027527\n",
      "epoch 0: loss 0.35036641359329224\n",
      "epoch 1: loss 0.3228945732116699\n",
      "epoch 2: loss 0.3491249084472656\n",
      "epoch 3: loss 0.3631289601325989\n",
      "epoch 4: loss 0.3148071765899658\n",
      "epoch 5: loss 0.252857506275177\n",
      "epoch 6: loss 0.2699522078037262\n",
      "epoch 7: loss 0.2191578894853592\n",
      "epoch 8: loss 0.4120926856994629\n",
      "epoch 9: loss 0.42905953526496887\n",
      "epoch 10: loss 0.35680675506591797\n",
      "epoch 11: loss 0.28981122374534607\n",
      "epoch 12: loss 0.3199427127838135\n",
      "epoch 13: loss 0.42745405435562134\n",
      "epoch 14: loss 0.3519939184188843\n",
      "epoch 15: loss 0.2755926251411438\n",
      "epoch 16: loss 0.36362212896347046\n",
      "epoch 17: loss 0.33262258768081665\n",
      "epoch 18: loss 0.36302056908607483\n",
      "epoch 19: loss 0.2661161720752716\n",
      "epoch 20: loss 0.30236077308654785\n",
      "epoch 21: loss 0.3248257637023926\n",
      "epoch 22: loss 0.30247145891189575\n",
      "epoch 23: loss 0.35945355892181396\n",
      "epoch 24: loss 0.32682445645332336\n",
      "epoch 25: loss 0.38053828477859497\n",
      "epoch 26: loss 0.34411442279815674\n",
      "epoch 27: loss 0.32874250411987305\n",
      "epoch 28: loss 0.3894038498401642\n",
      "epoch 29: loss 0.30886775255203247\n",
      "epoch 30: loss 0.28237324953079224\n",
      "epoch 31: loss 0.3296548128128052\n",
      "epoch 32: loss 0.3364741802215576\n",
      "epoch 33: loss 0.34111207723617554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34: loss 0.38492077589035034\n",
      "epoch 35: loss 0.26243099570274353\n",
      "epoch 36: loss 0.32866546511650085\n",
      "epoch 37: loss 0.3331109881401062\n",
      "epoch 38: loss 0.22818753123283386\n",
      "epoch 39: loss 0.3223137855529785\n",
      "epoch 40: loss 0.31907662749290466\n",
      "epoch 41: loss 0.284574031829834\n",
      "epoch 42: loss 0.23504942655563354\n",
      "epoch 43: loss 0.3247062563896179\n",
      "epoch 44: loss 0.3435864746570587\n",
      "epoch 45: loss 0.2809355854988098\n",
      "epoch 46: loss 0.2754623293876648\n",
      "epoch 47: loss 0.28047072887420654\n",
      "epoch 48: loss 0.23318606615066528\n",
      "epoch 49: loss 0.31180983781814575\n",
      "epoch 50: loss 0.28284698724746704\n",
      "epoch 51: loss 0.4050951600074768\n",
      "epoch 52: loss 0.3133782148361206\n",
      "epoch 53: loss 0.3538948893547058\n",
      "epoch 54: loss 0.3308537006378174\n",
      "epoch 55: loss 0.30643337965011597\n",
      "epoch 56: loss 0.3375346064567566\n",
      "epoch 57: loss 0.2522299885749817\n",
      "epoch 58: loss 0.3274885416030884\n",
      "epoch 59: loss 0.25962603092193604\n",
      "epoch 60: loss 0.32644492387771606\n",
      "epoch 61: loss 0.21711033582687378\n",
      "epoch 62: loss 0.23627260327339172\n",
      "epoch 63: loss 0.3228414058685303\n",
      "epoch 64: loss 0.2763705849647522\n",
      "epoch 65: loss 0.3747811019420624\n",
      "epoch 66: loss 0.3339804410934448\n",
      "epoch 67: loss 0.38925832509994507\n",
      "epoch 68: loss 0.3517390191555023\n",
      "epoch 69: loss 0.4105679392814636\n",
      "epoch 70: loss 0.35531115531921387\n",
      "epoch 71: loss 0.30734384059906006\n",
      "epoch 72: loss 0.2491978108882904\n",
      "epoch 73: loss 0.3098990321159363\n",
      "epoch 74: loss 0.2435181438922882\n",
      "epoch 75: loss 0.3452533483505249\n",
      "epoch 76: loss 0.3233473300933838\n",
      "epoch 77: loss 0.383813738822937\n",
      "epoch 78: loss 0.319019615650177\n",
      "epoch 79: loss 0.30268317461013794\n",
      "epoch 80: loss 0.36969995498657227\n",
      "epoch 81: loss 0.27215367555618286\n",
      "epoch 82: loss 0.3293442726135254\n",
      "epoch 83: loss 0.3700832426548004\n",
      "epoch 84: loss 0.26493537425994873\n",
      "epoch 85: loss 0.29966652393341064\n",
      "epoch 86: loss 0.32392463088035583\n",
      "epoch 87: loss 0.326357901096344\n",
      "epoch 88: loss 0.27111881971359253\n",
      "epoch 89: loss 0.2999233603477478\n",
      "epoch 90: loss 0.2595253586769104\n",
      "epoch 91: loss 0.3115904927253723\n",
      "epoch 92: loss 0.26366177201271057\n",
      "epoch 93: loss 0.3089287281036377\n",
      "epoch 94: loss 0.3218134045600891\n",
      "epoch 95: loss 0.18224596977233887\n",
      "epoch 96: loss 0.20503541827201843\n",
      "epoch 97: loss 0.286752313375473\n",
      "epoch 98: loss 0.32322224974632263\n",
      "epoch 99: loss 0.3574811518192291\n",
      "epoch 100: loss 0.48685428500175476\n",
      "epoch 101: loss 0.30953311920166016\n",
      "epoch 102: loss 0.2583029270172119\n",
      "epoch 103: loss 0.314689040184021\n",
      "epoch 104: loss 0.24612319469451904\n",
      "epoch 105: loss 0.3610018193721771\n",
      "epoch 106: loss 0.3078608512878418\n",
      "epoch 107: loss 0.29929831624031067\n",
      "epoch 108: loss 0.3468206524848938\n",
      "epoch 109: loss 0.3430798053741455\n",
      "epoch 110: loss 0.3504714369773865\n",
      "epoch 111: loss 0.3486648499965668\n",
      "epoch 112: loss 0.36946597695350647\n",
      "epoch 113: loss 0.3003908395767212\n",
      "epoch 114: loss 0.17597344517707825\n",
      "epoch 115: loss 0.4142831861972809\n",
      "epoch 116: loss 0.282792329788208\n",
      "epoch 117: loss 0.2539030909538269\n",
      "epoch 118: loss 0.3137426972389221\n",
      "epoch 119: loss 0.38895106315612793\n",
      "epoch 120: loss 0.28370118141174316\n",
      "epoch 121: loss 0.3531606197357178\n",
      "epoch 122: loss 0.23801644146442413\n",
      "epoch 123: loss 0.3999212384223938\n",
      "epoch 124: loss 0.2889755964279175\n",
      "epoch 125: loss 0.33701592683792114\n",
      "epoch 126: loss 0.3655472993850708\n",
      "epoch 127: loss 0.37953075766563416\n",
      "epoch 128: loss 0.24473795294761658\n",
      "epoch 129: loss 0.33196768164634705\n",
      "epoch 130: loss 0.3566238582134247\n",
      "epoch 131: loss 0.32789164781570435\n",
      "epoch 132: loss 0.30774810910224915\n",
      "epoch 133: loss 0.3293192982673645\n",
      "epoch 134: loss 0.3081323206424713\n",
      "epoch 135: loss 0.2423824667930603\n",
      "epoch 136: loss 0.23014605045318604\n",
      "epoch 0: loss 0.33534687757492065\n",
      "epoch 1: loss 0.32061660289764404\n",
      "epoch 2: loss 0.36626678705215454\n",
      "epoch 3: loss 0.34096118807792664\n",
      "epoch 4: loss 0.32861900329589844\n",
      "epoch 5: loss 0.24011662602424622\n",
      "epoch 6: loss 0.26885688304901123\n",
      "epoch 7: loss 0.21663902699947357\n",
      "epoch 8: loss 0.41049671173095703\n",
      "epoch 9: loss 0.40453553199768066\n",
      "epoch 10: loss 0.3507353663444519\n",
      "epoch 11: loss 0.286508709192276\n",
      "epoch 12: loss 0.3036743402481079\n",
      "epoch 13: loss 0.42230159044265747\n",
      "epoch 14: loss 0.3491664528846741\n",
      "epoch 15: loss 0.2676538825035095\n",
      "epoch 16: loss 0.35222652554512024\n",
      "epoch 17: loss 0.33127278089523315\n",
      "epoch 18: loss 0.35878705978393555\n",
      "epoch 19: loss 0.2724614143371582\n",
      "epoch 20: loss 0.304160475730896\n",
      "epoch 21: loss 0.32697010040283203\n",
      "epoch 22: loss 0.3002423644065857\n",
      "epoch 23: loss 0.3553794026374817\n",
      "epoch 24: loss 0.3279987871646881\n",
      "epoch 25: loss 0.37254491448402405\n",
      "epoch 26: loss 0.36758846044540405\n",
      "epoch 27: loss 0.33146464824676514\n",
      "epoch 28: loss 0.3938347101211548\n",
      "epoch 29: loss 0.3123508393764496\n",
      "epoch 30: loss 0.30108362436294556\n",
      "epoch 31: loss 0.325104296207428\n",
      "epoch 32: loss 0.3478204309940338\n",
      "epoch 33: loss 0.3293425142765045\n",
      "epoch 34: loss 0.3754681944847107\n",
      "epoch 35: loss 0.26946118474006653\n",
      "epoch 36: loss 0.32162535190582275\n",
      "epoch 37: loss 0.32472604513168335\n",
      "epoch 38: loss 0.21634308993816376\n",
      "epoch 39: loss 0.30170267820358276\n",
      "epoch 40: loss 0.2824031114578247\n",
      "epoch 41: loss 0.27678826451301575\n",
      "epoch 42: loss 0.261529803276062\n",
      "epoch 43: loss 0.3368445634841919\n",
      "epoch 44: loss 0.32418128848075867\n",
      "epoch 45: loss 0.2849678099155426\n",
      "epoch 46: loss 0.2795124351978302\n",
      "epoch 47: loss 0.27980828285217285\n",
      "epoch 48: loss 0.23017466068267822\n",
      "epoch 49: loss 0.31682923436164856\n",
      "epoch 50: loss 0.27935367822647095\n",
      "epoch 51: loss 0.38982370495796204\n",
      "epoch 52: loss 0.2669861316680908\n",
      "epoch 53: loss 0.3485652208328247\n",
      "epoch 54: loss 0.3450142741203308\n",
      "epoch 55: loss 0.2968403697013855\n",
      "epoch 56: loss 0.33957403898239136\n",
      "epoch 57: loss 0.2389855831861496\n",
      "epoch 58: loss 0.3166586756706238\n",
      "epoch 59: loss 0.25990480184555054\n",
      "epoch 60: loss 0.3115232586860657\n",
      "epoch 61: loss 0.21306276321411133\n",
      "epoch 62: loss 0.23886798322200775\n",
      "epoch 63: loss 0.32536613941192627\n",
      "epoch 64: loss 0.2743884325027466\n",
      "epoch 65: loss 0.38269904255867004\n",
      "epoch 66: loss 0.3244820535182953\n",
      "epoch 67: loss 0.37777385115623474\n",
      "epoch 68: loss 0.349435031414032\n",
      "epoch 69: loss 0.40546727180480957\n",
      "epoch 70: loss 0.3590726852416992\n",
      "epoch 71: loss 0.31525811553001404\n",
      "epoch 72: loss 0.24049103260040283\n",
      "epoch 73: loss 0.3042565584182739\n",
      "epoch 74: loss 0.2478477656841278\n",
      "epoch 75: loss 0.3563188314437866\n",
      "epoch 76: loss 0.31405723094940186\n",
      "epoch 77: loss 0.4042094647884369\n",
      "epoch 78: loss 0.3042064905166626\n",
      "epoch 79: loss 0.3206111788749695\n",
      "epoch 80: loss 0.37630167603492737\n",
      "epoch 81: loss 0.27204978466033936\n",
      "epoch 82: loss 0.3210616409778595\n",
      "epoch 83: loss 0.3663564622402191\n",
      "epoch 84: loss 0.3078010678291321\n",
      "epoch 85: loss 0.2861917018890381\n",
      "epoch 86: loss 0.3406451642513275\n",
      "epoch 87: loss 0.3246026039123535\n",
      "epoch 88: loss 0.29464197158813477\n",
      "epoch 89: loss 0.3740399479866028\n",
      "epoch 90: loss 0.3208811283111572\n",
      "epoch 91: loss 0.4265004098415375\n",
      "epoch 92: loss 0.4025057554244995\n",
      "epoch 93: loss 0.30292579531669617\n",
      "epoch 94: loss 0.31904155015945435\n",
      "epoch 95: loss 0.16955061256885529\n",
      "epoch 96: loss 0.219517320394516\n",
      "epoch 97: loss 0.31332916021347046\n",
      "epoch 98: loss 0.3656517267227173\n",
      "epoch 99: loss 0.34645554423332214\n",
      "epoch 100: loss 0.5075240135192871\n",
      "epoch 101: loss 0.31130707263946533\n",
      "epoch 102: loss 0.26049748063087463\n",
      "epoch 103: loss 0.28551316261291504\n",
      "epoch 104: loss 0.24830281734466553\n",
      "epoch 105: loss 0.36562108993530273\n",
      "epoch 106: loss 0.3118049204349518\n",
      "epoch 107: loss 0.26631075143814087\n",
      "epoch 108: loss 0.3367115259170532\n",
      "epoch 109: loss 0.3483824133872986\n",
      "epoch 110: loss 0.33785372972488403\n",
      "epoch 111: loss 0.3570496141910553\n",
      "epoch 112: loss 0.3749545216560364\n",
      "epoch 113: loss 0.2853526771068573\n",
      "epoch 114: loss 0.17044630646705627\n",
      "epoch 115: loss 0.4057011306285858\n",
      "epoch 116: loss 0.29723697900772095\n",
      "epoch 117: loss 0.25439658761024475\n",
      "epoch 118: loss 0.3160704970359802\n",
      "epoch 119: loss 0.3780367374420166\n",
      "epoch 120: loss 0.2590208649635315\n",
      "epoch 121: loss 0.3723211884498596\n",
      "epoch 122: loss 0.24267154932022095\n",
      "epoch 123: loss 0.3677777349948883\n",
      "epoch 124: loss 0.3334972560405731\n",
      "epoch 125: loss 0.3405730724334717\n",
      "epoch 126: loss 0.3767406642436981\n",
      "epoch 127: loss 0.42551204562187195\n",
      "epoch 128: loss 0.28741464018821716\n",
      "epoch 129: loss 0.3426315188407898\n",
      "epoch 130: loss 0.43487298488616943\n",
      "epoch 131: loss 0.3491353988647461\n",
      "epoch 132: loss 0.3702620267868042\n",
      "epoch 133: loss 0.3865869343280792\n",
      "epoch 134: loss 0.3214275538921356\n",
      "epoch 135: loss 0.26551786065101624\n",
      "epoch 136: loss 0.24608910083770752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 600).to(device)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu').to(device)\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(600, 500).to(device)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu').to(device)\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(500, 300).to(device)\n",
    "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu').to(device)\n",
    "        self.act3 = ReLU()\n",
    "        #\n",
    "        self.hidden4 = Linear(300, 150).to(device)\n",
    "        kaiming_uniform_(self.hidden4.weight, nonlinearity='relu').to(device)\n",
    "        self.act4 = ReLU()\n",
    "        #\n",
    "        self.hidden5 = Linear(150, 1).to(device)\n",
    "        xavier_uniform_(self.hidden5.weight).to(device)\n",
    "        self.act5 = Sigmoid()\n",
    "        \n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        #\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        #\n",
    "        X = self.hidden5(X)\n",
    "        X = self.act5(X)\n",
    "        return X\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in tqdm(range(100)):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs.to(device))\n",
    "            # calculate loss\n",
    "            #print(yhat.shape)\n",
    "            loss = criterion(yhat, targets.reshape(-1,1).to(device))\n",
    "            print(f\"epoch {i}: loss {loss}\")\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs.to(device)).cpu()\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.cpu().detach().numpy()\n",
    "    return yhat\n",
    " \n",
    "# define the network\n",
    "model = MLP(784)\n",
    "# train the model\n",
    "train_model(train_iterator, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(val_iterator, model)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d6c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd660a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b26f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6cbe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b745d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d900241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa209f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a9658b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a505e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65220392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31e1c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8a447925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, features, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(h)\n",
    "        return x, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d632afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_config = [6, 'M', 12, 'M', 24, 24, 'M']#[64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "vgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512,\n",
    "                512, 'M']\n",
    "\n",
    "vgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512,\n",
    "                'M', 512, 512, 512, 'M']\n",
    "\n",
    "vgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512,\n",
    "                512, 512, 'M', 512, 512, 512, 512, 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87e9f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vgg_layers(config, batch_norm):\n",
    "\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "\n",
    "    for c in config:\n",
    "        assert c == 'M' or isinstance(c, int)\n",
    "        if c == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = c\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4c2bcf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "788ae877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=7)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIM = 2\n",
    "\n",
    "model = VGG(vgg11_layers, OUTPUT_DIM)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c2e0f",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b79553ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_LR = 1e-7\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=START_LR)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88219a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRFinder:\n",
    "    def __init__(self, model, optimizer, criterion, device):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "\n",
    "        torch.save(model.state_dict(), 'init_params.pt')\n",
    "\n",
    "    def range_test(self, iterator, end_lr=10, num_iter=100,\n",
    "                   smooth_f=0.05, diverge_th=5):\n",
    "\n",
    "        lrs = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "\n",
    "        iterator = IteratorWrapper(iterator)\n",
    "\n",
    "        for iteration in range(num_iter):\n",
    "\n",
    "            loss = self._train_batch(iterator)\n",
    "\n",
    "            lrs.append(lr_scheduler.get_last_lr()[0])\n",
    "\n",
    "            # update lr\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if iteration > 0:\n",
    "                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n",
    "\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "\n",
    "            losses.append(loss)\n",
    "\n",
    "            if loss > diverge_th * best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        # reset model to initial parameters\n",
    "        model.load_state_dict(torch.load('init_params.pt'))\n",
    "\n",
    "        return lrs, losses\n",
    "\n",
    "    def _train_batch(self, iterator):\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        x, y = iterator.get_batch()\n",
    "\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "\n",
    "        y_pred, _ = self.model(x)\n",
    "\n",
    "        loss = self.criterion(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in\n",
    "                self.base_lrs]\n",
    "\n",
    "\n",
    "class IteratorWrapper:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self._iterator = iter(iterator)\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            inputs, labels = next(self._iterator)\n",
    "        except StopIteration:\n",
    "            self._iterator = iter(self.iterator)\n",
    "            inputs, labels, *_ = next(self._iterator)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def get_batch(self):\n",
    "        return next(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "60e84616",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (12x1x1). Calculated output size: (12x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20757/2487619677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEND_LR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_ITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_20757/3766690282.py\u001b[0m in \u001b[0;36mrange_test\u001b[0;34m(self, iterator, end_lr, num_iter, smooth_f, diverge_th)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20757/3766690282.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20757/1629760926.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    163\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                             self.return_indices)\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ai/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (12x1x1). Calculated output size: (12x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "END_LR = 10\n",
    "NUM_ITER = 100\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device)\n",
    "lrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a2950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd3b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
